{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "bpe_classification_setup_simple.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "13oTRH-4uMXH1tnw4jDZVnj92BojLfFHP",
      "authorship_tag": "ABX9TyPpIJkrjf5zbmciNAkiMFOU",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EternalSorrrow/transformer-feedforward-kernel/blob/main/bpe_classification_setup_simple.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g0_XMgEPOh7c",
        "outputId": "7b9c1072-de11-4c6c-dfcc-741329a44e11"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Nov 10 08:08:20 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 495.44       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  A100-SXM4-40GB      Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   32C    P0    44W / 400W |      0MiB / 40536MiB |      0%      Default |\n",
            "|                               |                      |             Disabled |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KGGnygXCDXLf",
        "outputId": "12127114-4469-47d7-a704-84b68b18aaea"
      },
      "source": [
        "!git clone https://github.com/google-research/long-range-arena.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'long-range-arena'...\n",
            "remote: Enumerating objects: 474, done.\u001b[K\n",
            "remote: Counting objects: 100% (474/474), done.\u001b[K\n",
            "remote: Compressing objects: 100% (195/195), done.\u001b[K\n",
            "remote: Total 474 (delta 330), reused 418 (delta 278), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (474/474), 153.25 KiB | 6.13 MiB/s, done.\n",
            "Resolving deltas: 100% (330/330), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SrMqZQQmd-vh",
        "outputId": "897d3e5f-c107-4f1a-a632-44a55749574c"
      },
      "source": [
        "#Execute if A100 is the current GPU\n",
        "\n",
        "!pip3 install torch==1.9.1+cu111 torchvision==0.10.1+cu111 torchaudio==0.9.1 -f https://download.pytorch.org/whl/torch_stable.html"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Collecting torch==1.9.1+cu111\n",
            "  Downloading https://download.pytorch.org/whl/cu111/torch-1.9.1%2Bcu111-cp37-cp37m-linux_x86_64.whl (2041.3 MB)\n",
            "\u001b[K     |█████████████                   | 834.1 MB 1.3 MB/s eta 0:15:54tcmalloc: large alloc 1147494400 bytes == 0x55c1d0fcc000 @  0x7f81a534d615 0x55c1981ef4cc 0x55c1982cf47a 0x55c1981f22ed 0x55c1982e3e1d 0x55c198265e99 0x55c1982609ee 0x55c1981f3bda 0x55c198265d00 0x55c1982609ee 0x55c1981f3bda 0x55c198262737 0x55c1982e4c66 0x55c198261daf 0x55c1982e4c66 0x55c198261daf 0x55c1982e4c66 0x55c198261daf 0x55c1981f4039 0x55c198237409 0x55c1981f2c52 0x55c198265c25 0x55c1982609ee 0x55c1981f3bda 0x55c198262737 0x55c1982609ee 0x55c1981f3bda 0x55c198261915 0x55c1981f3afa 0x55c198261c0d 0x55c1982609ee\n",
            "\u001b[K     |████████████████▌               | 1055.7 MB 1.2 MB/s eta 0:13:53tcmalloc: large alloc 1434370048 bytes == 0x55c215622000 @  0x7f81a534d615 0x55c1981ef4cc 0x55c1982cf47a 0x55c1981f22ed 0x55c1982e3e1d 0x55c198265e99 0x55c1982609ee 0x55c1981f3bda 0x55c198265d00 0x55c1982609ee 0x55c1981f3bda 0x55c198262737 0x55c1982e4c66 0x55c198261daf 0x55c1982e4c66 0x55c198261daf 0x55c1982e4c66 0x55c198261daf 0x55c1981f4039 0x55c198237409 0x55c1981f2c52 0x55c198265c25 0x55c1982609ee 0x55c1981f3bda 0x55c198262737 0x55c1982609ee 0x55c1981f3bda 0x55c198261915 0x55c1981f3afa 0x55c198261c0d 0x55c1982609ee\n",
            "\u001b[K     |█████████████████████           | 1336.2 MB 1.2 MB/s eta 0:09:44tcmalloc: large alloc 1792966656 bytes == 0x55c19a454000 @  0x7f81a534d615 0x55c1981ef4cc 0x55c1982cf47a 0x55c1981f22ed 0x55c1982e3e1d 0x55c198265e99 0x55c1982609ee 0x55c1981f3bda 0x55c198265d00 0x55c1982609ee 0x55c1981f3bda 0x55c198262737 0x55c1982e4c66 0x55c198261daf 0x55c1982e4c66 0x55c198261daf 0x55c1982e4c66 0x55c198261daf 0x55c1981f4039 0x55c198237409 0x55c1981f2c52 0x55c198265c25 0x55c1982609ee 0x55c1981f3bda 0x55c198262737 0x55c1982609ee 0x55c1981f3bda 0x55c198261915 0x55c1981f3afa 0x55c198261c0d 0x55c1982609ee\n",
            "\u001b[K     |██████████████████████████▌     | 1691.1 MB 1.2 MB/s eta 0:04:58tcmalloc: large alloc 2241208320 bytes == 0x55c20523c000 @  0x7f81a534d615 0x55c1981ef4cc 0x55c1982cf47a 0x55c1981f22ed 0x55c1982e3e1d 0x55c198265e99 0x55c1982609ee 0x55c1981f3bda 0x55c198265d00 0x55c1982609ee 0x55c1981f3bda 0x55c198262737 0x55c1982e4c66 0x55c198261daf 0x55c1982e4c66 0x55c198261daf 0x55c1982e4c66 0x55c198261daf 0x55c1981f4039 0x55c198237409 0x55c1981f2c52 0x55c198265c25 0x55c1982609ee 0x55c1981f3bda 0x55c198262737 0x55c1982609ee 0x55c1981f3bda 0x55c198261915 0x55c1981f3afa 0x55c198261c0d 0x55c1982609ee\n",
            "\u001b[K     |████████████████████████████████| 2041.3 MB 1.1 MB/s eta 0:00:01tcmalloc: large alloc 2041315328 bytes == 0x55c28ab9e000 @  0x7f81a534c1e7 0x55c198225067 0x55c1981ef4cc 0x55c1982cf47a 0x55c1981f22ed 0x55c1982e3e1d 0x55c198265e99 0x55c1982609ee 0x55c1981f3bda 0x55c198261c0d 0x55c1982609ee 0x55c1981f3bda 0x55c198261c0d 0x55c1982609ee 0x55c1981f3bda 0x55c198261c0d 0x55c1982609ee 0x55c1981f3bda 0x55c198261c0d 0x55c1982609ee 0x55c1981f3bda 0x55c198261c0d 0x55c1981f3afa 0x55c198261c0d 0x55c1982609ee 0x55c1981f3bda 0x55c198262737 0x55c1982609ee 0x55c1981f3bda 0x55c198262737 0x55c1982609ee\n",
            "tcmalloc: large alloc 2551644160 bytes == 0x55c30465e000 @  0x7f81a534d615 0x55c1981ef4cc 0x55c1982cf47a 0x55c1981f22ed 0x55c1982e3e1d 0x55c198265e99 0x55c1982609ee 0x55c1981f3bda 0x55c198261c0d 0x55c1982609ee 0x55c1981f3bda 0x55c198261c0d 0x55c1982609ee 0x55c1981f3bda 0x55c198261c0d 0x55c1982609ee 0x55c1981f3bda 0x55c198261c0d 0x55c1982609ee 0x55c1981f3bda 0x55c198261c0d 0x55c1981f3afa 0x55c198261c0d 0x55c1982609ee 0x55c1981f3bda 0x55c198262737 0x55c1982609ee 0x55c1981f3bda 0x55c198262737 0x55c1982609ee 0x55c1981f4271\n",
            "\u001b[K     |████████████████████████████████| 2041.3 MB 5.6 kB/s \n",
            "\u001b[?25hCollecting torchvision==0.10.1+cu111\n",
            "  Downloading https://download.pytorch.org/whl/cu111/torchvision-0.10.1%2Bcu111-cp37-cp37m-linux_x86_64.whl (20.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 20.6 MB 1.2 MB/s \n",
            "\u001b[?25hCollecting torchaudio==0.9.1\n",
            "  Downloading torchaudio-0.9.1-cp37-cp37m-manylinux1_x86_64.whl (1.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9 MB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.9.1+cu111) (3.10.0.2)\n",
            "Requirement already satisfied: pillow>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.10.1+cu111) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision==0.10.1+cu111) (1.19.5)\n",
            "Installing collected packages: torch, torchvision, torchaudio\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.9.0+cu111\n",
            "    Uninstalling torch-1.9.0+cu111:\n",
            "      Successfully uninstalled torch-1.9.0+cu111\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.10.0+cu111\n",
            "    Uninstalling torchvision-0.10.0+cu111:\n",
            "      Successfully uninstalled torchvision-0.10.0+cu111\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchtext 0.10.0 requires torch==1.9.0, but you have torch 1.9.1+cu111 which is incompatible.\u001b[0m\n",
            "Successfully installed torch-1.9.1+cu111 torchaudio-0.9.1 torchvision-0.10.1+cu111\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ATXw-aRr0sg-",
        "outputId": "2447f5ab-cc35-4f33-e9f7-518068640f5b"
      },
      "source": [
        "%cd /content/long-range-arena\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "from lra_benchmarks.text_classification.input_pipeline import get_tc_datasets\n",
        "\n",
        "batch_size=32\n",
        "accumulation_steps=32 // batch_size\n",
        "max_length=4000\n",
        "\n",
        "train_dataset, valid_dataset, test_dataset, encoder = get_tc_datasets(1, 'imdb_reviews', batch_size=batch_size, max_length=max_length)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/long-range-arena\n",
            "INFO:tensorflow:Finished preprocessing\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Finished preprocessing\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:<MapDataset shapes: {Source: (), Target: ()}, types: {Source: tf.string, Target: tf.int64}>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:<MapDataset shapes: {Source: (), Target: ()}, types: {Source: tf.string, Target: tf.int64}>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GV8HdFQJN8Pj",
        "outputId": "846f5a72-9724-437d-bd46-cbaf9f4d9ef9"
      },
      "source": [
        "sample = next(iter(train_dataset))['inputs']\n",
        "\n",
        "for i in range(min(4, batch_size)):\n",
        "  print(encoder.decode(sample[i]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Obviously, there wasn't a huge budget for this film which definitely hindered the production. But the story and ending were so brutal that they made up for a lot. I mean brutal on the level of Ju Dou and other (great) Chinese films. I first saw this when I was 14 years old, I ran home and begged God to forgive me for everything...\n",
            "Tara Reid as an intellectual, Christian Slater(usually great) as a dollar store Constantine and Stephen Dorff as...well it's STEPHEN DORFF FOR Christ SAKE!!!! I personally just want to thank those brilliant casting directors for the hard work and effort. You guys are on. Heres an idea, just my humble lowly opinion as the movie going public but it follows directly with your previous choices,a movie about the most brilliant neuro-physicist in history invent one pill to cure all diseases ever known to man and get this, heres the clincher they have to be played by Jessica Simpson and Paris Hilton. I knew you guys would love that. Seriously though you owe me $7.50.\n",
            "Still a sucker for Pyun's esthetic sense, I liked this movie, though the \"unfinished\" ending was a let-down. As usual, Pyun develops a warped sense of humour and Kathy Long's fights are extremely impressive. Beautifully photographed, this has the feel it was done for the big screen.\n",
            "I really wish i could give this a negative vote, because i think i just wasted 83 minutes of my life watching the worst horror movie ever put to film. the acting was just god awful, i mean REALLLYYYY bad, the dialog was worse, the script sounded like it was written by.... i can't think of anything horrible enough to say. And the day \"outside\" and the night \"inside\" shots make you think the events took over several days. Terribly acted, directed, written, etc etc. all the way down to the gofer how gets lunch for everyone. STAY AWAY FROM THIS ONE AT ALL COSTS. If my only saving grace to stay out of hell is by doing one good deed, it is to tell the world to not watch this crap. This movie is the exact reason why horror movies are never taken seriously.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c4ZHtbE300dO"
      },
      "source": [
        "import torch.nn as nn\n",
        "import math\n",
        "\n",
        "class TEmbedding(nn.Module):\n",
        "  def __init__(self, num_embeddings, hidden_dim, seq_length=1024, padding_idx=0):\n",
        "    super(TEmbedding, self).__init__()\n",
        "    \n",
        "    self.num_embeddings = num_embeddings\n",
        "    self.hidden_dim=hidden_dim\n",
        "    self.seq_length = seq_length\n",
        "    self.padding_idx = padding_idx\n",
        "\n",
        "    self.embedding = nn.Embedding(num_embeddings, hidden_dim, padding_idx)\n",
        "    self.pos_embeds  = nn.Parameter(torch.zeros(1, self.seq_length, self.hidden_dim))\n",
        "\n",
        "    self.cls = nn.Parameter(torch.zeros(1, 1, self.hidden_dim)) #!!!!!!! INIT WITH ANOTHER VALUE IF REQUIRED\n",
        "\n",
        "  def forward(self, input):\n",
        "    batch_size, seq_len = input.shape\n",
        "    \n",
        "    embed = self.embedding(input)\n",
        "    embed = embed + self.pos_embeds\n",
        "    embed = torch.cat([ self.cls.expand(batch_size, 1, -1), embed ], axis=1)\n",
        "\n",
        "    return embed\n",
        "    \n",
        "\n",
        "class TAttention(nn.Module):\n",
        "  def __init__(self, hidden_dim, num_heads, dropout_rate):\n",
        "    super(TAttention, self).__init__()\n",
        "    self.hidden_dim=hidden_dim\n",
        "    self.num_heads =num_heads\n",
        "    \n",
        "    assert not hidden_dim % num_heads\n",
        "    \n",
        "    self.head_dim =hidden_dim // num_heads\n",
        "\n",
        "    self.q = nn.Linear(self.hidden_dim, self.hidden_dim)\n",
        "    self.k = nn.Linear(self.hidden_dim, self.hidden_dim)\n",
        "    self.v = nn.Linear(self.hidden_dim, self.hidden_dim)\n",
        "\n",
        "    self.lin = nn.Linear(self.hidden_dim, self.hidden_dim)\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "  def split_heads(self, x):\n",
        "    new_shape = x.shape[:-1] + (self.num_heads, self.head_dim)\n",
        "    x = x.view(* new_shape)\n",
        "    return x.permute(0, 2, 1, 3)\n",
        "\n",
        "  def forward(self, x, losses=[]):\n",
        "    q = self.q(x)\n",
        "    k = self.k(x)\n",
        "    v = self.v(x)\n",
        "\n",
        "    q, k, v = self.split_heads(q), self.split_heads(k), self.split_heads(v)\n",
        "    q = torch.mul(q, 1. / torch.sqrt(torch.tensor(self.hidden_dim)))\n",
        "\n",
        "    qk = torch.matmul(q, k.transpose(-1, -2))\n",
        "    qk = nn.Softmax(dim=-1)(qk)\n",
        "\n",
        "    def assertion_function(tsr):\n",
        "      tsr = torch.sum(tsr, axis=-1)\n",
        "      tsr = tsr - torch.ones_like(tsr)\n",
        "      return torch.max(torch.abs(tsr)) < 1e-5\n",
        "\n",
        "    assert assertion_function(qk)\n",
        "\n",
        "    qk = self.dropout(qk) #Like in TF implementation; could be done before Softmax by random -inf addition\n",
        "\n",
        "    out = torch.matmul(qk, v)\n",
        "    out = out.permute(0, 2, 1, 3)\n",
        "\n",
        "    new_shape = out.shape[:-2] + (self.hidden_dim,)\n",
        "\n",
        "    out = out.reshape(* new_shape)\n",
        "\n",
        "    out = self.lin(out)\n",
        "\n",
        "    return out\n",
        "\n",
        "class HWLinear(nn.Module):\n",
        "  def __init__(self, num_heads, input_dim, output_dim, use_bias):\n",
        "    super(HWLinear, self).__init__()\n",
        "    \n",
        "    self.use_bias = use_bias\n",
        "    if use_bias:\n",
        "      self.bias   = nn.Parameter(torch.zeros( (1, num_heads, 1, output_dim)))\n",
        "\n",
        "    self.weight = nn.Parameter(torch.empty( (num_heads, input_dim, output_dim)))\n",
        "\n",
        "    def he_init(m):\n",
        "      s =  np.sqrt( 2. / input_dim )\n",
        "      m.data.normal_(0, s)\n",
        "\n",
        "    he_init(self.weight)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = torch.matmul(x, self.weight)\n",
        "    if self.use_bias:\n",
        "      x += self.bias\n",
        "    return x\n",
        "\n",
        "class Lambda(nn.Module):\n",
        "  def __init__(self, lambda_, objects=None):\n",
        "      super(Lambda, self).__init__()\n",
        "      self.lambda_ = lambda_\n",
        "      self.objects = objects\n",
        "\n",
        "  def forward(self, x):\n",
        "    if self.objects is not None:\n",
        "      return self.lambda_(self.objects, x)\n",
        "    return self.lambda_(x)\n",
        "\n",
        "class LKAAttention(nn.Module):\n",
        "  def __init__(self, hidden_dim, num_heads, dropout_rate):\n",
        "    super(LKAAttention, self).__init__()\n",
        "    self.hidden_dim=hidden_dim\n",
        "    self.num_heads =num_heads\n",
        "\n",
        "    assert not hidden_dim % num_heads\n",
        "    \n",
        "    self.head_dim =hidden_dim // num_heads\n",
        "    \n",
        "    self.q = nn.Linear(self.hidden_dim, self.hidden_dim)\n",
        "    self.k = nn.Linear(self.hidden_dim, self.hidden_dim)\n",
        "    self.v = nn.Linear(self.hidden_dim, self.hidden_dim)\n",
        "\n",
        "    #self.lka = nn.Sequential(\n",
        "    #  nn.Linear(self.head_dim, self.head_dim), nn.GELU(),\n",
        "    #  nn.Linear(self.head_dim, self.head_dim), nn.Softplus(beta=2.5),\n",
        "    #)\n",
        "\n",
        "    #256, 4, 16, 1024\n",
        "    #256, 64, 1, 1024\n",
        "    class AMGOLU(nn.Module):\n",
        "      def __init__(self, num_heads, hidden_dim, gate_rank, dropout_rate, gate_nonlinearity, kernel_nonlinearity, use_bias=False):\n",
        "        super(AMGOLU, self).__init__()\n",
        "\n",
        "        self.head_dim = hidden_dim // num_heads\n",
        "        self.num_heads= num_heads\n",
        "        \n",
        "        self.orth_weight = HWLinear(num_heads, self.head_dim, self.head_dim, use_bias)\n",
        "        self.orth_weight.weight = nn.Parameter(torch.stack([ nn.init.orthogonal_(torch.empty((self.head_dim, self.head_dim))) for _ in range(num_heads) ], dim=0))\n",
        "\n",
        "        self.gate_weight_a = HWLinear(num_heads, self.head_dim, gate_rank, use_bias)\n",
        "        self.gate_weight_b = HWLinear(num_heads, gate_rank, self.head_dim, use_bias)\n",
        "\n",
        "        self.kernel_nonlinearity = kernel_nonlinearity\n",
        "        self.gate_nonlinearity   = gate_nonlinearity\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "      def forward(self, x):\n",
        "        x, losses = x\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        forward_info = self.orth_weight(x)\n",
        "        forward_info = self.kernel_nonlinearity(forward_info)\n",
        "\n",
        "        gate_info = self.gate_weight_a(x)\n",
        "        gate_info = self.gate_weight_b(gate_info)\n",
        "        gate_info = self.gate_nonlinearity(gate_info)\n",
        "\n",
        "        x = forward_info * gate_info\n",
        "        \n",
        "        loss = torch.eye(self.head_dim, device=self.orth_weight.weight.device).unsqueeze(0).expand(self.num_heads, -1, -1)\n",
        "        loss = nn.MSELoss()(torch.matmul(self.orth_weight.weight, self.orth_weight.weight.transpose(-1, -2)), loss)\n",
        "        loss *= 0.001\n",
        "\n",
        "        losses.append(loss)\n",
        "\n",
        "        return x, losses\n",
        "\n",
        "\n",
        "\n",
        "    class GatedOrthoKernel(nn.Module):\n",
        "      def __init__(self, num_heads, hidden_dim, dropout_rate=0.1, gate_nonlinearity=nn.Sigmoid(), kernel_nonlinearity=nn.Identity(), use_bias=False):\n",
        "        super(GatedOrthoKernel, self).__init__()\n",
        "\n",
        "        self.head_dim = hidden_dim // num_heads\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "        self.orth_weight = HWLinear(num_heads, self.head_dim, self.head_dim, use_bias)\n",
        "        self.orth_weight.weight = nn.Parameter(torch.stack([ nn.init.orthogonal_(torch.empty((self.head_dim, self.head_dim))) for _ in range(num_heads) ], dim=0))\n",
        "        self.gate_weight = HWLinear(num_heads, self.head_dim, self.head_dim, use_bias)\n",
        "\n",
        "        self.kernel_nonlinearity = kernel_nonlinearity\n",
        "        self.gate_nonlinearity   = gate_nonlinearity\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "      def forward(self, x):\n",
        "        x, losses = x\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = self.kernel_nonlinearity(self.orth_weight(x)) * self.gate_nonlinearity(self.gate_weight(x))\n",
        "        \n",
        "        loss = torch.eye(self.head_dim, device=self.orth_weight.weight.device).unsqueeze(0).expand(self.num_heads, -1, -1)\n",
        "        loss = nn.MSELoss()(torch.matmul(self.orth_weight.weight, self.orth_weight.weight.transpose(-1, -2)), loss)\n",
        "        loss *= 0.01\n",
        "\n",
        "        losses.append(loss)\n",
        "\n",
        "        return x, losses\n",
        "\n",
        "\n",
        "    class HeadWiseFF(nn.Module):\n",
        "      def __init__(self, num_heads, hidden_dim, dropout_rate, nonlinearity=nn.Identity(), use_bias=False, residual=False):\n",
        "        super(HeadWiseFF, self).__init__()\n",
        "        \n",
        "        head_dim = hidden_dim // num_heads\n",
        "\n",
        "        self.bias   = nn.Parameter(torch.empty( (1, num_heads, 1, head_dim)))\n",
        "        self.dropout= nn.Dropout(dropout_rate)\n",
        "        self.use_bias = use_bias\n",
        "\n",
        "        self.weight = nn.Parameter(torch.empty( (num_heads, head_dim, head_dim)))\n",
        "        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
        "\n",
        "        #Orthogonal initialization\n",
        "        #Workaround with torch.stack, since Torch initializes a tensor as orthgonal by flattening its trailing dims and QR-factorizing the resulting 2d\n",
        "        \n",
        "        #self.weight = torch.stack([ nn.init.orthogonal_(torch.empty((head_dim, head_dim))) for _ in range(num_heads) ], dim=0)\n",
        "        #self.weight = nn.Parameter(self.weight)\n",
        "\n",
        "        bound = 1 / math.sqrt(head_dim)\n",
        "        nn.init.uniform_(self.bias, -bound, bound)\n",
        "\n",
        "        self.nonlinearity = nonlinearity\n",
        "        self.residual= residual\n",
        "\n",
        "      def forward(self, x):\n",
        "        \n",
        "        x, losses = x\n",
        "\n",
        "        bs, hd, seq, hdim = x.shape\n",
        "        y = self.dropout(x)\n",
        "        y = torch.matmul(y, self.weight) #BS, HD, SEQ, HDIM\n",
        "        if self.use_bias:\n",
        "          y += self.bias\n",
        "        y = self.nonlinearity(y)\n",
        "\n",
        "        #loss = torch.eye(hdim, device=self.weight.device).unsqueeze(0).expand(* self.weight.shape)\n",
        "        #loss = nn.MSELoss()(torch.matmul(self.weight, self.weight.transpose(-1, -2)), loss)\n",
        "        #loss *= 0.005\n",
        "\n",
        "        #losses.append(loss)\n",
        "\n",
        "        if self.residual:\n",
        "          return x + y, losses\n",
        "        return y, losses\n",
        "\n",
        "    class VampKernel(nn.Module):\n",
        "      def __init__(self, num_heads, hidden_dim, latent_dim, num_pseudoinputs, nonlinearity=nn.Softplus(), use_bias=True):\n",
        "        super(VampKernel, self).__init__()\n",
        "\n",
        "        head_dim = hidden_dim // num_heads\n",
        "        self.head_dim = head_dim\n",
        "        self.use_bias = use_bias\n",
        "        self.num_heads= num_heads\n",
        "        self.nonlinearity=nonlinearity\n",
        "\n",
        "        #Encoder q(z2 | x)\n",
        "        self.q_z2_x = nn.Sequential(\n",
        "            HWLinear(num_heads, head_dim, latent_dim, use_bias),\n",
        "            nn.GELU()\n",
        "        )\n",
        "\n",
        "        self.q_z2_mean    = HWLinear(num_heads, latent_dim, latent_dim, use_bias)\n",
        "        self.q_z2_log_var = nn.Sequential(\n",
        "            HWLinear(num_heads, latent_dim, latent_dim, use_bias),\n",
        "            Lambda(lambda x: torch.tanh(x) * 4.0 - 2.0)\n",
        "        )\n",
        "\n",
        "        #Encoder q(z1 | x, z2)\n",
        "        self.q_z1_x = nn.Sequential(\n",
        "            HWLinear(num_heads, head_dim, latent_dim, use_bias),\n",
        "            nn.GELU()\n",
        "        )\n",
        "\n",
        "        self.q_z1_z2 = nn.Sequential(\n",
        "            HWLinear(num_heads, latent_dim, latent_dim, use_bias),\n",
        "            nn.GELU()\n",
        "        )\n",
        "\n",
        "        #self.q_z1_joint = HWLinear(num_heads, latent_dim * 2, latent_dim, use_bias)\n",
        "\n",
        "        self.q_z1_mean   = HWLinear(num_heads, latent_dim * 2, latent_dim, use_bias)\n",
        "        self.q_z1_log_var= nn.Sequential(\n",
        "            HWLinear(num_heads, latent_dim * 2, latent_dim, use_bias),\n",
        "            Lambda(lambda x: torch.tanh(x) * 4.0 - 2.0)\n",
        "        )\n",
        "\n",
        "        #Decoder p(z1 | z2)\n",
        "        self.p_z1_z2_mean = HWLinear(num_heads, latent_dim, latent_dim, use_bias)\n",
        "        self.p_z1_z2_log_var = nn.Sequential(\n",
        "            HWLinear(num_heads, latent_dim, latent_dim, use_bias),\n",
        "            Lambda(lambda x: torch.tanh(x) * 4.0 - 2.0)\n",
        "        )\n",
        "\n",
        "        #Decoder p(x | z1, z2)\n",
        "        self.p_x_z1 = nn.Sequential(\n",
        "            HWLinear(num_heads, latent_dim, latent_dim, use_bias),\n",
        "            nn.GELU()\n",
        "        )\n",
        "\n",
        "        self.p_x_z2 = nn.Sequential(\n",
        "            HWLinear(num_heads, latent_dim, latent_dim, use_bias),\n",
        "            nn.GELU()\n",
        "        )\n",
        "\n",
        "        #self.p_x_joint = HWLinear(num_heads, latent_dim * 2, latent_dim, use_bias)\n",
        "        self.p_x_mean  = HWLinear(num_heads, latent_dim * 2, head_dim, use_bias)\n",
        "\n",
        "        #Pseudoinputs\n",
        "        self.means = nn.Sequential(\n",
        "            HWLinear(num_heads, num_pseudoinputs, head_dim, False),\n",
        "            nn.Tanh(),\n",
        "        )\n",
        "        self.dummy = nn.Parameter(torch.stack([ torch.eye(num_pseudoinputs) ] * num_heads, dim=0), requires_grad=False)\n",
        "\n",
        "      def reparameterize(self, mu, log_var):\n",
        "        if self.training:\n",
        "          return mu\n",
        "        \n",
        "        std = torch.exp(0.5 * log_var)\n",
        "        eps = torch.randn_like(std)\n",
        "\n",
        "        return mu + eps * std\n",
        "\n",
        "      def log_p_z2(self, z2):\n",
        "        #TODO\n",
        "        X = self.means(self.dummy)\n",
        "        HD, C, HDIM = X.shape\n",
        "        \n",
        "        pseudo_z2 = self.q_z2_x(X)\n",
        "        pseudo_z2_mean, pseudo_z2_log_var = self.q_z2_mean(pseudo_z2), self.q_z2_log_var(pseudo_z2)\n",
        "\n",
        "        #z2 = BS x HD x SEQ x HDIM\n",
        "        #pseudo = HD x C x HDIM\n",
        "        \n",
        "        #expand to BS x HD x SEQ x 1 x HDIM\n",
        "        #epxnad to  1 x HD x   1 x C x HDIM\n",
        "        z2 = z2.unsqueeze(-2)\n",
        "        pseudo_z2_mean = pseudo_z2_mean.unsqueeze(0).unsqueeze(2)\n",
        "        pseudo_z2_log_var = pseudo_z2_log_var.unsqueeze(0).unsqueeze(2)\n",
        "\n",
        "        return self.log_Normal_diag(z2, pseudo_z2_mean, pseudo_z2_log_var)\n",
        "        \n",
        "\n",
        "      def log_Normal_diag(self, x, mean, log_var, reduction=torch.mean):\n",
        "          log_normal = -0.5 * ( log_var + torch.pow(x - mean, 2) / torch.exp(log_var) )\n",
        "          return reduction( log_normal )\n",
        "\n",
        "      def add_losses(self, losses, z1, z2, z1_p_mean, z1_p_logvar, z1_q_mean, z1_q_logvar, z2_q_mean, z2_q_logvar):\n",
        "        #Reconstruction if needed\n",
        "        #...\n",
        "\n",
        "        #KLD\n",
        "        log_p_z1 = self.log_Normal_diag(z1, z1_p_mean, z1_p_logvar)\n",
        "        log_q_z1 = self.log_Normal_diag(z1, z1_q_mean, z1_q_logvar)\n",
        "        log_p_z2 = self.log_p_z2(z2)\n",
        "        log_q_z2 = self.log_Normal_diag(z2, z2_q_mean, z2_q_logvar)\n",
        "\n",
        "        kld = -(log_p_z1 + log_p_z2 - log_q_z1 - log_q_z2)\n",
        "        losses.append(kld)\n",
        "\n",
        "      def forward(self, x):\n",
        "        x, losses = x\n",
        "\n",
        "        z2 = self.q_z2_x(x)\n",
        "        z2_mean, z2_log_var = self.q_z2_mean(z2), self.q_z2_log_var(z2)\n",
        "        z2 = self.reparameterize(z2_mean, z2_log_var)\n",
        "\n",
        "        z1 = self.q_z1_x(x)\n",
        "        z1_z2 = self.q_z1_z2(z2)\n",
        "        z1 = torch.cat([ z1, z1_z2 ], dim=-1)\n",
        "        z1_mean, z1_log_var = self.q_z1_mean(z1), self.q_z1_log_var(z1)\n",
        "        z1 = self.reparameterize(z1_mean, z1_log_var)\n",
        "\n",
        "        z1_p_mean, z1_p_logvar = self.p_z1_z2_mean(z2), self.p_z1_z2_log_var(z2)\n",
        "\n",
        "        self.add_losses(losses, z1, z2, z1_p_mean, z1_p_logvar, z1_mean, z1_log_var, z2_mean, z2_log_var)\n",
        "        \n",
        "        x_z1 = self.p_x_z1(z1)\n",
        "        x_z2 = self.p_x_z2(z2)\n",
        "\n",
        "        output = torch.cat([ z_x1, x_z2 ], dim=-1)\n",
        "        output = self.p_x_mean(output)\n",
        "\n",
        "        self.add_losses(losses, z1, z2, z1_p_mean, z1_p_logvar, z1_mean, z1_log_var, z2_mean, z2_log_var)\n",
        "\n",
        "        return self.nonlinearity(output), losses\n",
        "\n",
        "    class VariationalKernel(nn.Module):\n",
        "      def __init__(self, num_heads, hidden_dim, latent_dim, nonlinearity=nn.Softplus(), use_bias=True):\n",
        "        super(VariationalKernel, self).__init__()\n",
        "\n",
        "        head_dim = hidden_dim // num_heads\n",
        "        \n",
        "        self.z_mean = HWLinear(num_heads, head_dim, latent_dim, use_bias)\n",
        "        self.z_log_var = nn.Sequential(\n",
        "            HWLinear(num_heads, head_dim, latent_dim, use_bias),\n",
        "            Lambda(lambda x: torch.tanh(x) * 4.0 - 2.0)\n",
        "        )\n",
        "\n",
        "        self.out = HWLinear(num_heads, latent_dim, head_dim, use_bias)\n",
        "        self.nonlinearity = nonlinearity\n",
        "\n",
        "      def reparameterize(self, mu, log_var):\n",
        "        if self.training:\n",
        "          return mu\n",
        "        \n",
        "        std = torch.exp(0.5 * log_var)\n",
        "        eps = torch.randn_like(std)\n",
        "\n",
        "        return mu + eps * std\n",
        "\n",
        "      def add_kld(self, z_mean, z_log_var, losses, reduction=torch.mean):\n",
        "        _, hd, _, _ = z_mean.shape\n",
        "\n",
        "        for i in range(hd):\n",
        "          kl_div = -0.5 * reduction(1 + z_log_var[:, i, :, :] - z_mean[:, i, :, :].pow(2) - z_log_var[:, i, :, :].exp())\n",
        "          losses.append(kl_div)\n",
        "\n",
        "      def forward(self, x):\n",
        "        x, losses = x\n",
        "        \n",
        "        z_mean   = self.z_mean(x)\n",
        "        z_log_var= self.z_log_var(x)\n",
        "\n",
        "        z = self.reparameterize(z_mean, z_log_var)\n",
        "        \n",
        "        out = self.out(z)\n",
        "\n",
        "        self.add_kld(z_mean, z_log_var, losses)\n",
        "\n",
        "        ###############################\n",
        "        return self.nonlinearity(out), losses\n",
        "        ###############################\n",
        "\n",
        "        ###############################\n",
        "        #rec_loss = nn.MSELoss()(out, x)\n",
        "        #losses.append(rec_loss)\n",
        "        \n",
        "        #return self.nonlinearity(z), losses\n",
        "        ################################\n",
        "\n",
        "    self.lka = nn.Sequential(\n",
        "        \n",
        "        #AMGOLU(self.num_heads, self.hidden_dim, self.head_dim // 4, dropout_rate, nn.Sigmoid(), nn.Identity(), False),\n",
        "        #AMGOLU(self.num_heads, self.hidden_dim, self.head_dim // 4, dropout_rate, nn.Sigmoid(), nn.Identity(), False),\n",
        "        #AMGOLU(self.num_heads, self.hidden_dim, self.head_dim // 4, dropout_rate, nn.Sigmoid(), nn.Softplus(), False),\n",
        "        \n",
        "        #HeadWiseFF(self.num_heads, self.hidden_dim, dropout_rate, nn.Softplus(), use_bias=False),\n",
        "        \n",
        "        Lambda(lambda o, x: (o['act'](x[0]), x[1]), { 'act' : nn.Identity() })\n",
        "        \n",
        "    )\n",
        "\n",
        "    self.lin = nn.Linear(self.hidden_dim, self.hidden_dim)\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "  def split_heads(self, x):\n",
        "    new_shape = x.shape[:-1] + (self.num_heads, self.head_dim)\n",
        "    x = x.view(* new_shape)\n",
        "    return x.permute(0, 2, 1, 3)\n",
        "\n",
        "  def forward(self, x, losses=[]):\n",
        "    q = self.q(x)\n",
        "    k = self.k(x)\n",
        "    v = self.v(x)\n",
        "\n",
        "    q, k, v = self.split_heads(q), self.split_heads(k), self.split_heads(v)\n",
        "    #BS x HEADS x SEQ x HEAD_DIM\n",
        "    \n",
        "    q, _ = self.lka((q, losses))\n",
        "    k, _ = self.lka((k, losses)) #Use this for var kernel\n",
        "\n",
        "    q = q / math.sqrt(self.head_dim)\n",
        "    k = k / math.sqrt(self.head_dim)\n",
        "\n",
        "    numerator = torch.matmul(k.unsqueeze(-1), v.unsqueeze(-2))\n",
        "    numerator = numerator.sum(axis=2)\n",
        "    numerator = torch.matmul(q, numerator)\n",
        "    \n",
        "    denominator = k.sum(axis=2).unsqueeze(-1)\n",
        "    denominator = q.matmul(denominator)\n",
        "\n",
        "    out = numerator / denominator\n",
        "    out = out.permute(0, 2, 1, 3)\n",
        "    \n",
        "    #TODO: INSERT DROPOUT\n",
        "    \n",
        "    new_shape = out.shape[:-2] + (self.hidden_dim,)\n",
        "    out = out.reshape(* new_shape)\n",
        "\n",
        "    out = self.lin(out)\n",
        "\n",
        "    return out\n",
        "\n",
        "class SimpleAttention(nn.Module):\n",
        "  def __init__(self, hidden_dim, num_heads, dropout_rate):\n",
        "    super(SimpleAttention, self).__init__()\n",
        "    self.hidden_dim=hidden_dim\n",
        "    self.num_heads =num_heads\n",
        "\n",
        "    assert not hidden_dim % num_heads\n",
        "    \n",
        "    self.head_dim =hidden_dim // num_heads\n",
        "    \n",
        "    self.q = nn.Linear(self.hidden_dim, self.hidden_dim)\n",
        "    self.k = nn.Linear(self.hidden_dim, self.hidden_dim)\n",
        "    self.v = nn.Linear(self.hidden_dim, self.hidden_dim)\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout_rate)\n",
        "    self.lin = nn.Linear(self.hidden_dim, self.hidden_dim)\n",
        "\n",
        "  def split_heads(self, x):\n",
        "    new_shape = x.shape[:-1] + (self.num_heads, self.head_dim)\n",
        "    x = x.view(* new_shape)\n",
        "    return x.permute(0, 2, 1, 3)\n",
        "\n",
        "  def forward(self, x, losses=[]):\n",
        "    q = self.q(x)\n",
        "    k = self.k(x)\n",
        "    v = self.v(x)\n",
        "\n",
        "    q, k, v = self.split_heads(q), self.split_heads(k), self.split_heads(v) #BS x HEADS x SEQ x HEAD_DIM\n",
        "\n",
        "    kv = torch.matmul(k.transpose(-1, -2), v)\n",
        "    kv = self.dropout(kv)\n",
        "\n",
        "    out = torch.matmul(q, kv)\n",
        "    out = out.permute(0, 2, 1, 3)\n",
        "    \n",
        "    new_shape = out.shape[:-2] + (self.hidden_dim,)\n",
        "    out = out.reshape(* new_shape)\n",
        "\n",
        "    return out\n",
        "\n",
        "class FtAttention(nn.Module):\n",
        "  def __init__(self, *args, **kwargs):\n",
        "    super(FtAttention, self).__init__()\n",
        "\n",
        "  def forward(self, x, losses=[]):\n",
        "    return torch.fft.fft(torch.fft.fft(x, dim=-1), dim=-2).real\n",
        "\n",
        "class TBlock(nn.Module):\n",
        "  def __init__(self, hidden_dim, inter_dim, num_heads, dropout_rate):\n",
        "    super(TBlock, self).__init__()\n",
        "\n",
        "    self.hidden_dim = hidden_dim\n",
        "    self.inter_dim  = inter_dim\n",
        "\n",
        "    self.layernorm_input = nn.LayerNorm(hidden_dim, eps=1e-6)\n",
        "    self.layernorm_inter = nn.LayerNorm(hidden_dim, eps=1e-6)\n",
        "\n",
        "    self.attention = TAttention(hidden_dim, num_heads, dropout_rate)\n",
        "\n",
        "    self.ffn       = nn.Sequential(\n",
        "        nn.Linear(hidden_dim, inter_dim), nn.GELU(), nn.Dropout(dropout_rate),\n",
        "        nn.Linear(inter_dim, hidden_dim), nn.GELU(), nn.Dropout(dropout_rate),\n",
        "    )\n",
        "\n",
        "\n",
        "  def forward(self, input, losses=[]):\n",
        "    x = self.layernorm_input(input)\n",
        "    x = self.attention(x, losses)\n",
        "\n",
        "    x = input + x\n",
        "\n",
        "    y = self.layernorm_inter(x)\n",
        "    x = self.ffn(y) + x\n",
        "\n",
        "    return x\n",
        "\n",
        "class TClassifier(nn.Module):\n",
        "  def __init__(self, classes, hidden_dim, inter_dim, dropout_rate):\n",
        "    super(TClassifier, self).__init__()\n",
        "\n",
        "    self.layernorm = nn.LayerNorm(hidden_dim, eps=1e-6)\n",
        "    self.dropout   = nn.Dropout(dropout_rate)\n",
        "\n",
        "    self.ffn       = nn.Sequential(\n",
        "        nn.Linear(hidden_dim, inter_dim), nn.GELU(),\n",
        "    )\n",
        "    self.output    = nn.Linear(inter_dim, classes)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.layernorm(x)\n",
        "    x = x[:, 0, :]\n",
        "    x = self.dropout(x)\n",
        "\n",
        "    x = self.ffn(x)\n",
        "    logits = self.output(x)\n",
        "\n",
        "    return logits\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "  def __init__(self, classes, num_embeddings, seq_len, hidden_dim=64, num_heads=4, num_blocks=8, output_units=512, internal_dropout_rate=0.1, output_dropout_rate=0.5):\n",
        "    super(Transformer, self).__init__()\n",
        "\n",
        "    inter_dim = 2 * hidden_dim\n",
        "    \n",
        "    self.embed_layer = TEmbedding(num_embeddings, hidden_dim, seq_len)\n",
        "    self.blocks      = nn.ModuleList([ TBlock(hidden_dim, inter_dim, num_heads, internal_dropout_rate) for _ in range(num_blocks) ])\n",
        "    self.classifier  = TClassifier(classes, hidden_dim, output_units, output_dropout_rate)\n",
        "\n",
        "  def forward(self, pixel_values):\n",
        "    additional_losses = []\n",
        "\n",
        "    x = self.embed_layer(pixel_values)\n",
        "    \n",
        "    for block in self.blocks:\n",
        "      x = block(x, additional_losses)\n",
        "    \n",
        "    x = self.classifier(x)\n",
        "\n",
        "    return x, additional_losses"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uKl9SMDoCJeK"
      },
      "source": [
        "def num_parameters(model):\n",
        "  return sum(list(map(\n",
        "      lambda x: np.prod(x[1].shape), model.named_parameters()\n",
        "  )))\n",
        "\n",
        "n_classes = 2\n",
        "\n",
        "def model_factory():\n",
        "  model = Transformer(\n",
        "    classes   =n_classes,\n",
        "    num_embeddings=encoder.vocab_size,\n",
        "    seq_len=max_length,\n",
        "    hidden_dim=256,\n",
        "    num_heads =4,\n",
        "    num_blocks=4,\n",
        "    output_units=1024,\n",
        "    internal_dropout_rate=0.1,\n",
        "    output_dropout_rate=0.0\n",
        "  ).cuda()\n",
        "  \n",
        "  orig_count = num_parameters(model)\n",
        "\n",
        "  for block in model.blocks:\n",
        "    #block.attention = FtAttention()\n",
        "    #block.attention = LKAAttention(256, 4, 0.1).cuda()\n",
        "    block.attention = SimpleAttention(256, 4, 0.1).cuda()\n",
        "    ...\n",
        "  \n",
        "  new_count = num_parameters(model)\n",
        "  print(f'Original model {orig_count} params, new model {new_count} params, ratio {new_count / orig_count:.3}')\n",
        "\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "id": "l0P77jbqTEqK",
        "outputId": "e6935734-cd67-428b-a7e1-fb0252bf5bf8"
      },
      "source": [
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "\n",
        "def get_schedule(warmup_steps):\n",
        "  def lr_schedule(step):\n",
        "    return 1.0 * np.minimum(1.0, step / warmup_steps) / np.sqrt(np.maximum(step, warmup_steps))\n",
        "\n",
        "  return lr_schedule\n",
        "\n",
        "lr=0.05\n",
        "weight_decay=0.1\n",
        "warmup=8000\n",
        "\n",
        "def training_setup():\n",
        "  model = model_factory()\n",
        "  criterion = nn.CrossEntropyLoss().cuda()\n",
        "  optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "  schedule_func = get_schedule(warmup)\n",
        "  scheduler = LambdaLR(optimizer, schedule_func)\n",
        "\n",
        "  return model, criterion, optimizer, schedule_func, scheduler\n",
        "\n",
        "_, _, _, schedule_func, _ = training_setup()\n",
        "\n",
        "plt.plot([ lr * schedule_func(i) for i in range(len(train_dataset) // accumulation_steps * 25) ])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original model 3464194 params, new model 3464194 params, ratio 1.0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fcd231208d0>]"
            ]
          },
          "metadata": {},
          "execution_count": 45
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_B6ohTx7wgH"
      },
      "source": [
        "import time\n",
        "\n",
        "def save_model(model, optimizer, name='/content/drive/MyDrive/Work/Misc/lka-mini-base.tar'):\n",
        "  torch.save({\n",
        "              'model_state_dict': model.state_dict(),\n",
        "              'optimizer_state_dict': optimizer.state_dict(),\n",
        "              }, name)\n",
        "\n",
        "def progress_bar(len, total, current):\n",
        "  current_scaled = int(round(len * current / total))\n",
        "\n",
        "  s = '[' + '=' * (current_scaled - 1)\n",
        "  s += '>' if current != total else '='\n",
        "  s += '-' * (len - current_scaled) + ']'\n",
        "\n",
        "  return s\n",
        "\n",
        "def accuracy(model_output, labels):\n",
        "  model_output = model_output.argmax(dim=-1)\n",
        "\n",
        "  return (labels == model_output).float().mean().cpu().numpy()\n",
        "\n",
        "def train_model(model, name, train_dataset, valid_dataset, optimizer, criterion, scheduler, accumulation_steps, epochs, epoch_len=None, eps = 1e-6, skip_eval=25):\n",
        "  \n",
        "  best_acc = 0.0\n",
        "\n",
        "  bnum = math.ceil(len(train_dataset) / accumulation_steps)\n",
        "  train_dataset = train_dataset.shuffle(len(train_dataset), reshuffle_each_iteration=True)\n",
        "\n",
        "  times_repeat = epochs if epoch_len is None else math.ceil(epochs * epoch_len / bnum)\n",
        "  train_dataset = train_dataset.repeat(times_repeat)\n",
        "  train_datagen = iter(train_dataset)\n",
        "  \n",
        "  if epoch_len is not None:\n",
        "    bnum = epoch_len\n",
        "  \n",
        "  for epoch in range(epochs):  # loop over the dataset multiple times\n",
        "\n",
        "      #epoch start timestamp\n",
        "      t = time.time()\n",
        "\n",
        "      running_loss = 0.0\n",
        "      running_reg  = 0.0\n",
        "      running_acc  = 0.0\n",
        "\n",
        "      running_momentum = 0.99\n",
        "\n",
        "      epoch_loss = [  ]\n",
        "      epoch_reg  = [  ]\n",
        "      epoch_acc  = [  ]\n",
        "\n",
        "      model.train()\n",
        "\n",
        "      print(f'Epoch {epoch}')\n",
        "\n",
        "      process_inputs = lambda x: torch.Tensor(x.numpy()).to(torch.int64)\n",
        "\n",
        "      for i in range(bnum):\n",
        "          # zero the parameter gradients\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "          #accumulate gradients for a certain amount of steps\n",
        "          for k in range(accumulation_steps):\n",
        "            # get the inputs; data is a list of [inputs, labels]\n",
        "\n",
        "            try:\n",
        "              data = next(train_datagen)\n",
        "            except:\n",
        "              break\n",
        "            inputs, labels = data['inputs'], data['targets']\n",
        "            inputs, labels = process_inputs(inputs), process_inputs(labels)\n",
        "            inputs, labels = inputs.cuda(), labels.cuda()\n",
        "\n",
        "            # forward + backward + optimize\n",
        "            outputs, additional_losses = model(inputs)\n",
        "            loss = criterion(outputs + eps, labels)\n",
        "\n",
        "            additional_losses = sum(additional_losses) if additional_losses else torch.Tensor([ 0.0 ]).cuda()\n",
        "            ((loss + additional_losses) / accumulation_steps).backward()\n",
        "\n",
        "            acc = accuracy(outputs, labels)\n",
        "\n",
        "            running_loss = running_loss * running_momentum + (1 - running_momentum) * loss.item()\n",
        "            running_loss_unb = running_loss / (1 - running_momentum ** (i * accumulation_steps + k + 1))\n",
        "\n",
        "            running_acc  = running_acc  * running_momentum + (1 - running_momentum) * acc\n",
        "            running_acc_unb = running_acc / (1 - running_momentum ** (i * accumulation_steps + k + 1))\n",
        "\n",
        "            running_reg  = running_reg  * running_momentum + (1 - running_momentum) * additional_losses.item()\n",
        "            running_reg_unb = running_reg / (1 - running_momentum ** (i * accumulation_steps + k + 1))\n",
        "\n",
        "            epoch_loss.append(loss.item())\n",
        "            epoch_acc.append(acc)\n",
        "            epoch_reg.append(additional_losses.item())\n",
        "\n",
        "          optimizer.step()\n",
        "\n",
        "          pbar = progress_bar(20, bnum, i + 1)\n",
        "\n",
        "          print(f'\\r{pbar} {i + 1}/{bnum}:', end='')\n",
        "          print(f' - running_loss: {running_loss_unb:.4f} - running_reg: {running_reg_unb:.6f} - running_acc: {running_acc_unb:.4f} - lr: {scheduler.get_last_lr()[0]:.5f}', end='')\n",
        "\n",
        "          scheduler.step()\n",
        "      \n",
        "      epoch_loss = np.mean(epoch_loss)\n",
        "      epoch_acc  = np.mean(epoch_acc)\n",
        "      epoch_reg  = np.mean(epoch_reg)\n",
        "      \n",
        "      print(f' - epoch_loss: {epoch_loss:.4f} - epoch_reg: {epoch_reg:.6f} - epoch_acc: {epoch_acc:.4f}', end='')\n",
        "\n",
        "      epoch_loss, epoch_acc, epoch_reg = [], [], []\n",
        "\n",
        "      \n",
        "      if epoch >= skip_eval:\n",
        "        model.eval()\n",
        "        valid_dataset.repeat()\n",
        "\n",
        "        with torch.no_grad():\n",
        "          for i, data in enumerate(iter(valid_dataset)):\n",
        "            inputs, labels = data['inputs'], data['targets']\n",
        "            inputs, labels = process_inputs(inputs), process_inputs(labels)\n",
        "            inputs, labels = inputs.cuda(), labels.cuda()\n",
        "\n",
        "            outputs, aux_losses = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            acc = accuracy(outputs, labels)\n",
        "            aux_losses = sum(aux_losses) if aux_losses else torch.Tensor([ 0.0 ]).cuda()\n",
        "\n",
        "            epoch_loss.append(loss.item())\n",
        "            epoch_acc.append(acc)\n",
        "            epoch_reg.append(aux_losses.item())\n",
        "\n",
        "        epoch_loss, epoch_acc, epoch_reg = np.mean(epoch_loss), np.mean(epoch_acc), np.mean(epoch_reg)\n",
        "\n",
        "        if epoch_acc > best_acc:\n",
        "          best_acc = epoch_acc\n",
        "          save_model(model, optimizer, name)\n",
        "      \n",
        "      else:\n",
        "        epoch_loss, epoch_acc, epoch_reg = 0.0, 0.0, 0.0\n",
        "\n",
        "      #epoch computing time\n",
        "      t = time.time() - t\n",
        "\n",
        "      print(f' - valid_loss: {epoch_loss:.4f} - valid_reg: {epoch_reg:.6f} - valid_acc: {epoch_acc:.4f} - epoch_time: {t:.4f} s')\n",
        " \n",
        "  checkpoint = torch.load(name)\n",
        "  return checkpoint"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "egh-IbW76AN4"
      },
      "source": [
        "def test(model, criterion, test_dataset):\n",
        "  epoch_loss, epoch_acc, epoch_reg = [], [], []\n",
        "\n",
        "  model.eval()\n",
        "  test_dataset.repeat()\n",
        "\n",
        "  process_inputs = lambda x: torch.Tensor(x.numpy()).to(torch.int64)\n",
        "\n",
        "  t = time.time()\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for i, data in enumerate(iter(test_dataset)):\n",
        "      inputs, labels = data['inputs'], data['targets']\n",
        "      inputs, labels = process_inputs(inputs), process_inputs(labels)\n",
        "      inputs, labels = inputs.cuda(), labels.cuda()\n",
        "\n",
        "      outputs, aux_losses = model(inputs)\n",
        "      loss = criterion(outputs, labels)\n",
        "      acc = accuracy(outputs, labels)\n",
        "      aux_losses = sum(aux_losses) if aux_losses else torch.Tensor([ 0.0 ]).cuda()\n",
        "\n",
        "      epoch_loss.append(loss.item())\n",
        "      epoch_acc.append(acc)\n",
        "      epoch_reg.append(aux_losses.item())\n",
        "\n",
        "  t = time.time() - t\n",
        "\n",
        "  epoch_loss, epoch_acc, epoch_reg = np.mean(epoch_loss), np.mean(epoch_acc), np.mean(epoch_reg)\n",
        "\n",
        "  print(f' - test_loss: {epoch_loss:.4f} - test_reg: {epoch_reg:.6f} - test_acc: {epoch_acc:.4f} - test_time: {t:.4f} s')\n",
        "  return epoch_loss, epoch_reg, epoch_acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J4d0DhgWDbm3",
        "outputId": "aa2bac7f-ebd5-4582-e790-266e6766d4dc"
      },
      "source": [
        "test_accuracy = [  ]\n",
        "\n",
        "for i in range(5): ####!!!!!!!!!!!!!!\n",
        "  path = 'model_to_test_' + str(i) + '.b'\n",
        "\n",
        "  model, criterion, optimizer, schedule_func, scheduler = training_setup()\n",
        "\n",
        "  checkpoint = train_model(model, path, train_dataset, valid_dataset, optimizer, criterion, scheduler, accumulation_steps, 100, 200, skip_eval=15)\n",
        "  model.load_state_dict(checkpoint['model_state_dict'])\n",
        "  \n",
        "  _, _, acc = test(model, criterion, test_dataset)\n",
        "  test_accuracy.append(acc)\n",
        "\n",
        "test_accuracy = np.mean(test_accuracy)\n",
        "\n",
        "print(f'\\nTotal accuracy: {test_accuracy:.4f}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original model 3464194 params, new model 3464194 params, ratio 1.0\n",
            "Epoch 0\n",
            "[====================] 200/200: - running_loss: 0.6928 - running_reg: 0.000000 - running_acc: 0.5250 - lr: 0.00001 - epoch_loss: 0.6944 - epoch_reg: 0.000000 - epoch_acc: 0.5214 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 39.3252 s\n",
            "Epoch 1\n",
            "[====================] 200/200: - running_loss: 0.6869 - running_reg: 0.000000 - running_acc: 0.5364 - lr: 0.00003 - epoch_loss: 0.6888 - epoch_reg: 0.000000 - epoch_acc: 0.5308 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 28.4367 s\n",
            "Epoch 2\n",
            "[====================] 200/200: - running_loss: 0.6898 - running_reg: 0.000000 - running_acc: 0.5401 - lr: 0.00004 - epoch_loss: 0.6901 - epoch_reg: 0.000000 - epoch_acc: 0.5402 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 28.5065 s\n",
            "Epoch 3\n",
            "[====================] 200/200: - running_loss: 0.6838 - running_reg: 0.000000 - running_acc: 0.5602 - lr: 0.00006 - epoch_loss: 0.6848 - epoch_reg: 0.000000 - epoch_acc: 0.5553 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 28.5146 s\n",
            "Epoch 4\n",
            "[====================] 200/200: - running_loss: 0.6857 - running_reg: 0.000000 - running_acc: 0.5507 - lr: 0.00007 - epoch_loss: 0.6838 - epoch_reg: 0.000000 - epoch_acc: 0.5556 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 28.4582 s\n",
            "Epoch 5\n",
            "[====================] 200/200: - running_loss: 0.6883 - running_reg: 0.000000 - running_acc: 0.5387 - lr: 0.00008 - epoch_loss: 0.6889 - epoch_reg: 0.000000 - epoch_acc: 0.5416 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 28.5669 s\n",
            "Epoch 6\n",
            "[====================] 200/200: - running_loss: 0.6878 - running_reg: 0.000000 - running_acc: 0.5467 - lr: 0.00010 - epoch_loss: 0.6862 - epoch_reg: 0.000000 - epoch_acc: 0.5530 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 28.6581 s\n",
            "Epoch 7\n",
            "[====================] 200/200: - running_loss: 0.6799 - running_reg: 0.000000 - running_acc: 0.5723 - lr: 0.00011 - epoch_loss: 0.6810 - epoch_reg: 0.000000 - epoch_acc: 0.5695 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 28.5590 s\n",
            "Epoch 8\n",
            "[====================] 200/200: - running_loss: 0.6908 - running_reg: 0.000000 - running_acc: 0.5361 - lr: 0.00013 - epoch_loss: 0.6901 - epoch_reg: 0.000000 - epoch_acc: 0.5384 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 28.5763 s\n",
            "Epoch 9\n",
            "[====================] 200/200: - running_loss: 0.6859 - running_reg: 0.000000 - running_acc: 0.5518 - lr: 0.00014 - epoch_loss: 0.6858 - epoch_reg: 0.000000 - epoch_acc: 0.5533 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 28.5800 s\n",
            "Epoch 10\n",
            "[====================] 200/200: - running_loss: 0.6848 - running_reg: 0.000000 - running_acc: 0.5579 - lr: 0.00015 - epoch_loss: 0.6820 - epoch_reg: 0.000000 - epoch_acc: 0.5647 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 28.6057 s\n",
            "Epoch 11\n",
            "[====================] 200/200: - running_loss: 0.6819 - running_reg: 0.000000 - running_acc: 0.5579 - lr: 0.00017 - epoch_loss: 0.6842 - epoch_reg: 0.000000 - epoch_acc: 0.5548 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 28.3987 s\n",
            "Epoch 12\n",
            "[====================] 200/200: - running_loss: 0.6865 - running_reg: 0.000000 - running_acc: 0.5482 - lr: 0.00018 - epoch_loss: 0.6848 - epoch_reg: 0.000000 - epoch_acc: 0.5534 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 28.6092 s\n",
            "Epoch 13\n",
            "[====================] 200/200: - running_loss: 0.6784 - running_reg: 0.000000 - running_acc: 0.5642 - lr: 0.00020 - epoch_loss: 0.6800 - epoch_reg: 0.000000 - epoch_acc: 0.5589 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 28.5900 s\n",
            "Epoch 14\n",
            "[====================] 200/200: - running_loss: 0.6806 - running_reg: 0.000000 - running_acc: 0.5625 - lr: 0.00021 - epoch_loss: 0.6777 - epoch_reg: 0.000000 - epoch_acc: 0.5709 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 28.6370 s\n",
            "Epoch 15\n",
            "[====================] 200/200: - running_loss: 0.6823 - running_reg: 0.000000 - running_acc: 0.5635 - lr: 0.00022 - epoch_loss: 0.6810 - epoch_reg: 0.000000 - epoch_acc: 0.5631 - valid_loss: 0.6866 - valid_reg: 0.000000 - valid_acc: 0.5352 - epoch_time: 62.8294 s\n",
            "Epoch 16\n",
            "[====================] 200/200: - running_loss: 0.6763 - running_reg: 0.000000 - running_acc: 0.5819 - lr: 0.00024 - epoch_loss: 0.6760 - epoch_reg: 0.000000 - epoch_acc: 0.5803 - valid_loss: 0.6932 - valid_reg: 0.000000 - valid_acc: 0.5101 - epoch_time: 62.7363 s\n",
            "Epoch 17\n",
            "[====================] 200/200: - running_loss: 0.6904 - running_reg: 0.000000 - running_acc: 0.5258 - lr: 0.00025 - epoch_loss: 0.6869 - epoch_reg: 0.000000 - epoch_acc: 0.5356 - valid_loss: 0.6960 - valid_reg: 0.000000 - valid_acc: 0.5070 - epoch_time: 62.8224 s\n",
            "Epoch 18\n",
            "[====================] 200/200: - running_loss: 0.6775 - running_reg: 0.000000 - running_acc: 0.5600 - lr: 0.00027 - epoch_loss: 0.6805 - epoch_reg: 0.000000 - epoch_acc: 0.5508 - valid_loss: 0.6698 - valid_reg: 0.000000 - valid_acc: 0.5977 - epoch_time: 62.7563 s\n",
            "Epoch 19\n",
            "[====================] 200/200: - running_loss: 0.6812 - running_reg: 0.000000 - running_acc: 0.5653 - lr: 0.00028 - epoch_loss: 0.6779 - epoch_reg: 0.000000 - epoch_acc: 0.5763 - valid_loss: 0.6795 - valid_reg: 0.000000 - valid_acc: 0.5600 - epoch_time: 62.6944 s\n",
            "Epoch 20\n",
            "[====================] 200/200: - running_loss: 0.6709 - running_reg: 0.000000 - running_acc: 0.5838 - lr: 0.00029 - epoch_loss: 0.6728 - epoch_reg: 0.000000 - epoch_acc: 0.5802 - valid_loss: 0.6732 - valid_reg: 0.000000 - valid_acc: 0.5898 - epoch_time: 63.0051 s\n",
            "Epoch 21\n",
            "[====================] 200/200: - running_loss: 0.6724 - running_reg: 0.000000 - running_acc: 0.5860 - lr: 0.00031 - epoch_loss: 0.6731 - epoch_reg: 0.000000 - epoch_acc: 0.5819 - valid_loss: 0.6646 - valid_reg: 0.000000 - valid_acc: 0.5930 - epoch_time: 62.7879 s\n",
            "Epoch 22\n",
            "[====================] 200/200: - running_loss: 0.6927 - running_reg: 0.000000 - running_acc: 0.5296 - lr: 0.00032 - epoch_loss: 0.6940 - epoch_reg: 0.000000 - epoch_acc: 0.5258 - valid_loss: 0.6780 - valid_reg: 0.000000 - valid_acc: 0.5790 - epoch_time: 62.9284 s\n",
            "Epoch 23\n",
            "[====================] 200/200: - running_loss: 0.6942 - running_reg: 0.000000 - running_acc: 0.5221 - lr: 0.00034 - epoch_loss: 0.6924 - epoch_reg: 0.000000 - epoch_acc: 0.5250 - valid_loss: 0.6930 - valid_reg: 0.000000 - valid_acc: 0.4998 - epoch_time: 62.7862 s\n",
            "Epoch 24\n",
            "[====================] 200/200: - running_loss: 0.6832 - running_reg: 0.000000 - running_acc: 0.5469 - lr: 0.00035 - epoch_loss: 0.6854 - epoch_reg: 0.000000 - epoch_acc: 0.5394 - valid_loss: 0.6862 - valid_reg: 0.000000 - valid_acc: 0.5222 - epoch_time: 62.9403 s\n",
            "Epoch 25\n",
            "[====================] 200/200: - running_loss: 0.6787 - running_reg: 0.000000 - running_acc: 0.5669 - lr: 0.00036 - epoch_loss: 0.6806 - epoch_reg: 0.000000 - epoch_acc: 0.5636 - valid_loss: 0.6663 - valid_reg: 0.000000 - valid_acc: 0.5914 - epoch_time: 62.9697 s\n",
            "Epoch 26\n",
            "[====================] 200/200: - running_loss: 0.6736 - running_reg: 0.000000 - running_acc: 0.5772 - lr: 0.00038 - epoch_loss: 0.6741 - epoch_reg: 0.000000 - epoch_acc: 0.5752 - valid_loss: 0.6644 - valid_reg: 0.000000 - valid_acc: 0.6007 - epoch_time: 63.0003 s\n",
            "Epoch 27\n",
            "[====================] 200/200: - running_loss: 0.6699 - running_reg: 0.000000 - running_acc: 0.5891 - lr: 0.00039 - epoch_loss: 0.6699 - epoch_reg: 0.000000 - epoch_acc: 0.5903 - valid_loss: 0.6941 - valid_reg: 0.000000 - valid_acc: 0.5252 - epoch_time: 62.8248 s\n",
            "Epoch 28\n",
            "[====================] 200/200: - running_loss: 0.6647 - running_reg: 0.000000 - running_acc: 0.6005 - lr: 0.00041 - epoch_loss: 0.6651 - epoch_reg: 0.000000 - epoch_acc: 0.5973 - valid_loss: 0.6690 - valid_reg: 0.000000 - valid_acc: 0.5878 - epoch_time: 62.7660 s\n",
            "Epoch 29\n",
            "[====================] 200/200: - running_loss: 0.6665 - running_reg: 0.000000 - running_acc: 0.5891 - lr: 0.00042 - epoch_loss: 0.6654 - epoch_reg: 0.000000 - epoch_acc: 0.5906 - valid_loss: 0.6696 - valid_reg: 0.000000 - valid_acc: 0.6049 - epoch_time: 62.9978 s\n",
            "Epoch 30\n",
            "[====================] 200/200: - running_loss: 0.6738 - running_reg: 0.000000 - running_acc: 0.5819 - lr: 0.00043 - epoch_loss: 0.6749 - epoch_reg: 0.000000 - epoch_acc: 0.5792 - valid_loss: 0.6586 - valid_reg: 0.000000 - valid_acc: 0.6088 - epoch_time: 62.9746 s\n",
            "Epoch 31\n",
            "[====================] 200/200: - running_loss: 0.6635 - running_reg: 0.000000 - running_acc: 0.6004 - lr: 0.00045 - epoch_loss: 0.6636 - epoch_reg: 0.000000 - epoch_acc: 0.6019 - valid_loss: 0.6597 - valid_reg: 0.000000 - valid_acc: 0.6047 - epoch_time: 62.8677 s\n",
            "Epoch 32\n",
            "[====================] 200/200: - running_loss: 0.6607 - running_reg: 0.000000 - running_acc: 0.6008 - lr: 0.00046 - epoch_loss: 0.6643 - epoch_reg: 0.000000 - epoch_acc: 0.5969 - valid_loss: 0.6839 - valid_reg: 0.000000 - valid_acc: 0.6025 - epoch_time: 62.8263 s\n",
            "Epoch 33\n",
            "[====================] 200/200: - running_loss: 0.6553 - running_reg: 0.000000 - running_acc: 0.6173 - lr: 0.00048 - epoch_loss: 0.6556 - epoch_reg: 0.000000 - epoch_acc: 0.6177 - valid_loss: 0.6560 - valid_reg: 0.000000 - valid_acc: 0.6062 - epoch_time: 62.9064 s\n",
            "Epoch 34\n",
            "[====================] 200/200: - running_loss: 0.6640 - running_reg: 0.000000 - running_acc: 0.6025 - lr: 0.00049 - epoch_loss: 0.6651 - epoch_reg: 0.000000 - epoch_acc: 0.5987 - valid_loss: 0.6583 - valid_reg: 0.000000 - valid_acc: 0.6108 - epoch_time: 63.0490 s\n",
            "Epoch 35\n",
            "[====================] 200/200: - running_loss: 0.6628 - running_reg: 0.000000 - running_acc: 0.5991 - lr: 0.00050 - epoch_loss: 0.6610 - epoch_reg: 0.000000 - epoch_acc: 0.6041 - valid_loss: 0.6671 - valid_reg: 0.000000 - valid_acc: 0.5876 - epoch_time: 62.8157 s\n",
            "Epoch 36\n",
            "[====================] 200/200: - running_loss: 0.6524 - running_reg: 0.000000 - running_acc: 0.6161 - lr: 0.00052 - epoch_loss: 0.6548 - epoch_reg: 0.000000 - epoch_acc: 0.6131 - valid_loss: 0.6612 - valid_reg: 0.000000 - valid_acc: 0.6055 - epoch_time: 62.6859 s\n",
            "Epoch 37\n",
            "[====================] 200/200: - running_loss: 0.6580 - running_reg: 0.000000 - running_acc: 0.6073 - lr: 0.00053 - epoch_loss: 0.6614 - epoch_reg: 0.000000 - epoch_acc: 0.6030 - valid_loss: 0.6587 - valid_reg: 0.000000 - valid_acc: 0.5997 - epoch_time: 62.8477 s\n",
            "Epoch 38\n",
            "[====================] 200/200: - running_loss: 0.6509 - running_reg: 0.000000 - running_acc: 0.6199 - lr: 0.00054 - epoch_loss: 0.6484 - epoch_reg: 0.000000 - epoch_acc: 0.6198 - valid_loss: 0.6501 - valid_reg: 0.000000 - valid_acc: 0.6197 - epoch_time: 62.8348 s\n",
            "Epoch 39\n",
            "[====================] 200/200: - running_loss: 0.6447 - running_reg: 0.000000 - running_acc: 0.6207 - lr: 0.00056 - epoch_loss: 0.6468 - epoch_reg: 0.000000 - epoch_acc: 0.6216 - valid_loss: 0.6649 - valid_reg: 0.000000 - valid_acc: 0.6031 - epoch_time: 62.8826 s\n",
            "Epoch 40\n",
            "[====================] 200/200: - running_loss: 0.6444 - running_reg: 0.000000 - running_acc: 0.6205 - lr: 0.00055 - epoch_loss: 0.6458 - epoch_reg: 0.000000 - epoch_acc: 0.6211 - valid_loss: 0.6491 - valid_reg: 0.000000 - valid_acc: 0.6217 - epoch_time: 62.8486 s\n",
            "Epoch 41\n",
            "[====================] 200/200: - running_loss: 0.6493 - running_reg: 0.000000 - running_acc: 0.6206 - lr: 0.00055 - epoch_loss: 0.6491 - epoch_reg: 0.000000 - epoch_acc: 0.6208 - valid_loss: 0.6421 - valid_reg: 0.000000 - valid_acc: 0.6296 - epoch_time: 62.7879 s\n",
            "Epoch 42\n",
            "[====================] 200/200: - running_loss: 0.6410 - running_reg: 0.000000 - running_acc: 0.6368 - lr: 0.00054 - epoch_loss: 0.6432 - epoch_reg: 0.000000 - epoch_acc: 0.6328 - valid_loss: 0.6421 - valid_reg: 0.000000 - valid_acc: 0.6381 - epoch_time: 62.7664 s\n",
            "Epoch 43\n",
            "[====================] 200/200: - running_loss: 0.6443 - running_reg: 0.000000 - running_acc: 0.6239 - lr: 0.00053 - epoch_loss: 0.6405 - epoch_reg: 0.000000 - epoch_acc: 0.6308 - valid_loss: 0.6446 - valid_reg: 0.000000 - valid_acc: 0.6327 - epoch_time: 62.7124 s\n",
            "Epoch 44\n",
            "[====================] 200/200: - running_loss: 0.6351 - running_reg: 0.000000 - running_acc: 0.6335 - lr: 0.00053 - epoch_loss: 0.6359 - epoch_reg: 0.000000 - epoch_acc: 0.6341 - valid_loss: 0.6369 - valid_reg: 0.000000 - valid_acc: 0.6354 - epoch_time: 62.7496 s\n",
            "Epoch 45\n",
            "[====================] 200/200: - running_loss: 0.6332 - running_reg: 0.000000 - running_acc: 0.6339 - lr: 0.00052 - epoch_loss: 0.6358 - epoch_reg: 0.000000 - epoch_acc: 0.6344 - valid_loss: 0.6482 - valid_reg: 0.000000 - valid_acc: 0.6138 - epoch_time: 62.7737 s\n",
            "Epoch 46\n",
            "[====================] 200/200: - running_loss: 0.6351 - running_reg: 0.000000 - running_acc: 0.6353 - lr: 0.00052 - epoch_loss: 0.6356 - epoch_reg: 0.000000 - epoch_acc: 0.6364 - valid_loss: 0.6370 - valid_reg: 0.000000 - valid_acc: 0.6350 - epoch_time: 62.6313 s\n",
            "Epoch 47\n",
            "[====================] 200/200: - running_loss: 0.6368 - running_reg: 0.000000 - running_acc: 0.6381 - lr: 0.00051 - epoch_loss: 0.6345 - epoch_reg: 0.000000 - epoch_acc: 0.6420 - valid_loss: 0.6493 - valid_reg: 0.000000 - valid_acc: 0.6156 - epoch_time: 62.8711 s\n",
            "Epoch 48\n",
            "[====================] 200/200: - running_loss: 0.6423 - running_reg: 0.000000 - running_acc: 0.6282 - lr: 0.00051 - epoch_loss: 0.6399 - epoch_reg: 0.000000 - epoch_acc: 0.6325 - valid_loss: 0.6350 - valid_reg: 0.000000 - valid_acc: 0.6413 - epoch_time: 62.7522 s\n",
            "Epoch 49\n",
            "[====================] 200/200: - running_loss: 0.6287 - running_reg: 0.000000 - running_acc: 0.6466 - lr: 0.00050 - epoch_loss: 0.6265 - epoch_reg: 0.000000 - epoch_acc: 0.6491 - valid_loss: 0.6329 - valid_reg: 0.000000 - valid_acc: 0.6445 - epoch_time: 62.8983 s\n",
            "Epoch 50\n",
            "[====================] 200/200: - running_loss: 0.6294 - running_reg: 0.000000 - running_acc: 0.6471 - lr: 0.00050 - epoch_loss: 0.6338 - epoch_reg: 0.000000 - epoch_acc: 0.6419 - valid_loss: 0.6325 - valid_reg: 0.000000 - valid_acc: 0.6447 - epoch_time: 62.8020 s\n",
            "Epoch 51\n",
            "[====================] 200/200: - running_loss: 0.6203 - running_reg: 0.000000 - running_acc: 0.6520 - lr: 0.00049 - epoch_loss: 0.6195 - epoch_reg: 0.000000 - epoch_acc: 0.6525 - valid_loss: 0.6284 - valid_reg: 0.000000 - valid_acc: 0.6488 - epoch_time: 62.9504 s\n",
            "Epoch 52\n",
            "[====================] 200/200: - running_loss: 0.6231 - running_reg: 0.000000 - running_acc: 0.6484 - lr: 0.00049 - epoch_loss: 0.6271 - epoch_reg: 0.000000 - epoch_acc: 0.6433 - valid_loss: 0.6285 - valid_reg: 0.000000 - valid_acc: 0.6473 - epoch_time: 62.7748 s\n",
            "Epoch 53\n",
            "[====================] 200/200: - running_loss: 0.6218 - running_reg: 0.000000 - running_acc: 0.6511 - lr: 0.00048 - epoch_loss: 0.6229 - epoch_reg: 0.000000 - epoch_acc: 0.6522 - valid_loss: 0.6268 - valid_reg: 0.000000 - valid_acc: 0.6533 - epoch_time: 62.8064 s\n",
            "Epoch 54\n",
            "[====================] 200/200: - running_loss: 0.6273 - running_reg: 0.000000 - running_acc: 0.6470 - lr: 0.00048 - epoch_loss: 0.6233 - epoch_reg: 0.000000 - epoch_acc: 0.6511 - valid_loss: 0.6292 - valid_reg: 0.000000 - valid_acc: 0.6517 - epoch_time: 62.6479 s\n",
            "Epoch 55\n",
            "[====================] 200/200: - running_loss: 0.6198 - running_reg: 0.000000 - running_acc: 0.6547 - lr: 0.00047 - epoch_loss: 0.6177 - epoch_reg: 0.000000 - epoch_acc: 0.6581 - valid_loss: 0.6252 - valid_reg: 0.000000 - valid_acc: 0.6537 - epoch_time: 62.7019 s\n",
            "Epoch 56\n",
            "[====================] 200/200: - running_loss: 0.6142 - running_reg: 0.000000 - running_acc: 0.6610 - lr: 0.00047 - epoch_loss: 0.6176 - epoch_reg: 0.000000 - epoch_acc: 0.6577 - valid_loss: 0.6260 - valid_reg: 0.000000 - valid_acc: 0.6506 - epoch_time: 62.7919 s\n",
            "Epoch 57\n",
            "[====================] 200/200: - running_loss: 0.6226 - running_reg: 0.000000 - running_acc: 0.6585 - lr: 0.00046 - epoch_loss: 0.6216 - epoch_reg: 0.000000 - epoch_acc: 0.6586 - valid_loss: 0.6267 - valid_reg: 0.000000 - valid_acc: 0.6496 - epoch_time: 62.9724 s\n",
            "Epoch 58\n",
            "[====================] 200/200: - running_loss: 0.6114 - running_reg: 0.000000 - running_acc: 0.6640 - lr: 0.00046 - epoch_loss: 0.6138 - epoch_reg: 0.000000 - epoch_acc: 0.6606 - valid_loss: 0.6277 - valid_reg: 0.000000 - valid_acc: 0.6510 - epoch_time: 63.0462 s\n",
            "Epoch 59\n",
            "[====================] 200/200: - running_loss: 0.6187 - running_reg: 0.000000 - running_acc: 0.6470 - lr: 0.00046 - epoch_loss: 0.6215 - epoch_reg: 0.000000 - epoch_acc: 0.6463 - valid_loss: 0.6555 - valid_reg: 0.000000 - valid_acc: 0.6319 - epoch_time: 63.0397 s\n",
            "Epoch 60\n",
            "[====================] 200/200: - running_loss: 0.6224 - running_reg: 0.000000 - running_acc: 0.6536 - lr: 0.00045 - epoch_loss: 0.6219 - epoch_reg: 0.000000 - epoch_acc: 0.6522 - valid_loss: 0.6461 - valid_reg: 0.000000 - valid_acc: 0.6293 - epoch_time: 63.0318 s\n",
            "Epoch 61\n",
            "[====================] 200/200: - running_loss: 0.6206 - running_reg: 0.000000 - running_acc: 0.6541 - lr: 0.00045 - epoch_loss: 0.6189 - epoch_reg: 0.000000 - epoch_acc: 0.6562 - valid_loss: 0.6314 - valid_reg: 0.000000 - valid_acc: 0.6413 - epoch_time: 63.4453 s\n",
            "Epoch 62\n",
            "[====================] 200/200: - running_loss: 0.6207 - running_reg: 0.000000 - running_acc: 0.6506 - lr: 0.00045 - epoch_loss: 0.6146 - epoch_reg: 0.000000 - epoch_acc: 0.6591 - valid_loss: 0.6261 - valid_reg: 0.000000 - valid_acc: 0.6545 - epoch_time: 63.1319 s\n",
            "Epoch 63\n",
            "[====================] 200/200: - running_loss: 0.6138 - running_reg: 0.000000 - running_acc: 0.6622 - lr: 0.00044 - epoch_loss: 0.6140 - epoch_reg: 0.000000 - epoch_acc: 0.6606 - valid_loss: 0.6212 - valid_reg: 0.000000 - valid_acc: 0.6591 - epoch_time: 62.9733 s\n",
            "Epoch 64\n",
            "[====================] 200/200: - running_loss: 0.6180 - running_reg: 0.000000 - running_acc: 0.6570 - lr: 0.00044 - epoch_loss: 0.6125 - epoch_reg: 0.000000 - epoch_acc: 0.6650 - valid_loss: 0.6308 - valid_reg: 0.000000 - valid_acc: 0.6441 - epoch_time: 63.0081 s\n",
            "Epoch 65\n",
            "[====================] 200/200: - running_loss: 0.6162 - running_reg: 0.000000 - running_acc: 0.6602 - lr: 0.00044 - epoch_loss: 0.6159 - epoch_reg: 0.000000 - epoch_acc: 0.6586 - valid_loss: 0.6240 - valid_reg: 0.000000 - valid_acc: 0.6560 - epoch_time: 62.9908 s\n",
            "Epoch 66\n",
            "[====================] 200/200: - running_loss: 0.6150 - running_reg: 0.000000 - running_acc: 0.6584 - lr: 0.00043 - epoch_loss: 0.6132 - epoch_reg: 0.000000 - epoch_acc: 0.6631 - valid_loss: 0.6270 - valid_reg: 0.000000 - valid_acc: 0.6549 - epoch_time: 62.9525 s\n",
            "Epoch 67\n",
            "[====================] 200/200: - running_loss: 0.6117 - running_reg: 0.000000 - running_acc: 0.6647 - lr: 0.00043 - epoch_loss: 0.6098 - epoch_reg: 0.000000 - epoch_acc: 0.6667 - valid_loss: 0.6236 - valid_reg: 0.000000 - valid_acc: 0.6581 - epoch_time: 62.9092 s\n",
            "Epoch 68\n",
            "[====================] 200/200: - running_loss: 0.6100 - running_reg: 0.000000 - running_acc: 0.6668 - lr: 0.00043 - epoch_loss: 0.6114 - epoch_reg: 0.000000 - epoch_acc: 0.6631 - valid_loss: 0.6333 - valid_reg: 0.000000 - valid_acc: 0.6442 - epoch_time: 62.9412 s\n",
            "Epoch 69\n",
            "[====================] 200/200: - running_loss: 0.6105 - running_reg: 0.000000 - running_acc: 0.6609 - lr: 0.00042 - epoch_loss: 0.6123 - epoch_reg: 0.000000 - epoch_acc: 0.6594 - valid_loss: 0.6310 - valid_reg: 0.000000 - valid_acc: 0.6354 - epoch_time: 62.8941 s\n",
            "Epoch 70\n",
            "[====================] 200/200: - running_loss: 0.6095 - running_reg: 0.000000 - running_acc: 0.6660 - lr: 0.00042 - epoch_loss: 0.6085 - epoch_reg: 0.000000 - epoch_acc: 0.6673 - valid_loss: 0.6199 - valid_reg: 0.000000 - valid_acc: 0.6593 - epoch_time: 62.9923 s\n",
            "Epoch 71\n",
            "[====================] 200/200: - running_loss: 0.6095 - running_reg: 0.000000 - running_acc: 0.6667 - lr: 0.00042 - epoch_loss: 0.6075 - epoch_reg: 0.000000 - epoch_acc: 0.6673 - valid_loss: 0.6354 - valid_reg: 0.000000 - valid_acc: 0.6425 - epoch_time: 62.9030 s\n",
            "Epoch 72\n",
            "[====================] 200/200: - running_loss: 0.6108 - running_reg: 0.000000 - running_acc: 0.6611 - lr: 0.00041 - epoch_loss: 0.6098 - epoch_reg: 0.000000 - epoch_acc: 0.6633 - valid_loss: 0.6264 - valid_reg: 0.000000 - valid_acc: 0.6477 - epoch_time: 62.8820 s\n",
            "Epoch 73\n",
            "[====================] 200/200: - running_loss: 0.6116 - running_reg: 0.000000 - running_acc: 0.6661 - lr: 0.00041 - epoch_loss: 0.6097 - epoch_reg: 0.000000 - epoch_acc: 0.6669 - valid_loss: 0.6189 - valid_reg: 0.000000 - valid_acc: 0.6602 - epoch_time: 63.0154 s\n",
            "Epoch 74\n",
            "[====================] 200/200: - running_loss: 0.6024 - running_reg: 0.000000 - running_acc: 0.6731 - lr: 0.00041 - epoch_loss: 0.5979 - epoch_reg: 0.000000 - epoch_acc: 0.6748 - valid_loss: 0.6274 - valid_reg: 0.000000 - valid_acc: 0.6552 - epoch_time: 62.9073 s\n",
            "Epoch 75\n",
            "[====================] 200/200: - running_loss: 0.6042 - running_reg: 0.000000 - running_acc: 0.6790 - lr: 0.00041 - epoch_loss: 0.6033 - epoch_reg: 0.000000 - epoch_acc: 0.6770 - valid_loss: 0.6199 - valid_reg: 0.000000 - valid_acc: 0.6582 - epoch_time: 62.9019 s\n",
            "Epoch 76\n",
            "[====================] 200/200: - running_loss: 0.6065 - running_reg: 0.000000 - running_acc: 0.6663 - lr: 0.00040 - epoch_loss: 0.6073 - epoch_reg: 0.000000 - epoch_acc: 0.6647 - valid_loss: 0.6536 - valid_reg: 0.000000 - valid_acc: 0.6327 - epoch_time: 62.8668 s\n",
            "Epoch 77\n",
            "[====================] 200/200: - running_loss: 0.6018 - running_reg: 0.000000 - running_acc: 0.6769 - lr: 0.00040 - epoch_loss: 0.6048 - epoch_reg: 0.000000 - epoch_acc: 0.6737 - valid_loss: 0.6282 - valid_reg: 0.000000 - valid_acc: 0.6539 - epoch_time: 62.7658 s\n",
            "Epoch 78\n",
            "[====================] 200/200: - running_loss: 0.5976 - running_reg: 0.000000 - running_acc: 0.6803 - lr: 0.00040 - epoch_loss: 0.5995 - epoch_reg: 0.000000 - epoch_acc: 0.6805 - valid_loss: 0.6198 - valid_reg: 0.000000 - valid_acc: 0.6598 - epoch_time: 62.5698 s\n",
            "Epoch 79\n",
            "[====================] 200/200: - running_loss: 0.6058 - running_reg: 0.000000 - running_acc: 0.6631 - lr: 0.00040 - epoch_loss: 0.6058 - epoch_reg: 0.000000 - epoch_acc: 0.6659 - valid_loss: 0.6241 - valid_reg: 0.000000 - valid_acc: 0.6583 - epoch_time: 62.4838 s\n",
            "Epoch 80\n",
            "[====================] 200/200: - running_loss: 0.6150 - running_reg: 0.000000 - running_acc: 0.6593 - lr: 0.00039 - epoch_loss: 0.6096 - epoch_reg: 0.000000 - epoch_acc: 0.6631 - valid_loss: 0.6208 - valid_reg: 0.000000 - valid_acc: 0.6617 - epoch_time: 62.6867 s\n",
            "Epoch 81\n",
            "[====================] 200/200: - running_loss: 0.5971 - running_reg: 0.000000 - running_acc: 0.6739 - lr: 0.00039 - epoch_loss: 0.5990 - epoch_reg: 0.000000 - epoch_acc: 0.6700 - valid_loss: 0.6312 - valid_reg: 0.000000 - valid_acc: 0.6599 - epoch_time: 62.6288 s\n",
            "Epoch 82\n",
            "[====================] 200/200: - running_loss: 0.5950 - running_reg: 0.000000 - running_acc: 0.6710 - lr: 0.00039 - epoch_loss: 0.5998 - epoch_reg: 0.000000 - epoch_acc: 0.6661 - valid_loss: 0.6181 - valid_reg: 0.000000 - valid_acc: 0.6648 - epoch_time: 62.5455 s\n",
            "Epoch 83\n",
            "[====================] 200/200: - running_loss: 0.5996 - running_reg: 0.000000 - running_acc: 0.6810 - lr: 0.00039 - epoch_loss: 0.6001 - epoch_reg: 0.000000 - epoch_acc: 0.6787 - valid_loss: 0.6204 - valid_reg: 0.000000 - valid_acc: 0.6608 - epoch_time: 62.5574 s\n",
            "Epoch 84\n",
            "[====================] 200/200: - running_loss: 0.6006 - running_reg: 0.000000 - running_acc: 0.6762 - lr: 0.00038 - epoch_loss: 0.5991 - epoch_reg: 0.000000 - epoch_acc: 0.6787 - valid_loss: 0.6182 - valid_reg: 0.000000 - valid_acc: 0.6612 - epoch_time: 62.5842 s\n",
            "Epoch 85\n",
            "[====================] 200/200: - running_loss: 0.6022 - running_reg: 0.000000 - running_acc: 0.6687 - lr: 0.00038 - epoch_loss: 0.6008 - epoch_reg: 0.000000 - epoch_acc: 0.6703 - valid_loss: 0.6187 - valid_reg: 0.000000 - valid_acc: 0.6639 - epoch_time: 62.6376 s\n",
            "Epoch 86\n",
            "[====================] 200/200: - running_loss: 0.5965 - running_reg: 0.000000 - running_acc: 0.6805 - lr: 0.00038 - epoch_loss: 0.5956 - epoch_reg: 0.000000 - epoch_acc: 0.6819 - valid_loss: 0.6220 - valid_reg: 0.000000 - valid_acc: 0.6630 - epoch_time: 62.7650 s\n",
            "Epoch 87\n",
            "[====================] 200/200: - running_loss: 0.5946 - running_reg: 0.000000 - running_acc: 0.6753 - lr: 0.00038 - epoch_loss: 0.5916 - epoch_reg: 0.000000 - epoch_acc: 0.6772 - valid_loss: 0.6181 - valid_reg: 0.000000 - valid_acc: 0.6620 - epoch_time: 62.5838 s\n",
            "Epoch 88\n",
            "[====================] 200/200: - running_loss: 0.5874 - running_reg: 0.000000 - running_acc: 0.6839 - lr: 0.00037 - epoch_loss: 0.5957 - epoch_reg: 0.000000 - epoch_acc: 0.6766 - valid_loss: 0.6157 - valid_reg: 0.000000 - valid_acc: 0.6656 - epoch_time: 62.7407 s\n",
            "Epoch 89\n",
            "[====================] 200/200: - running_loss: 0.6042 - running_reg: 0.000000 - running_acc: 0.6736 - lr: 0.00037 - epoch_loss: 0.5979 - epoch_reg: 0.000000 - epoch_acc: 0.6812 - valid_loss: 0.6163 - valid_reg: 0.000000 - valid_acc: 0.6627 - epoch_time: 62.5240 s\n",
            "Epoch 90\n",
            "[====================] 200/200: - running_loss: 0.5923 - running_reg: 0.000000 - running_acc: 0.6787 - lr: 0.00037 - epoch_loss: 0.5954 - epoch_reg: 0.000000 - epoch_acc: 0.6759 - valid_loss: 0.6200 - valid_reg: 0.000000 - valid_acc: 0.6597 - epoch_time: 62.6210 s\n",
            "Epoch 91\n",
            "[====================] 200/200: - running_loss: 0.5945 - running_reg: 0.000000 - running_acc: 0.6813 - lr: 0.00037 - epoch_loss: 0.5899 - epoch_reg: 0.000000 - epoch_acc: 0.6870 - valid_loss: 0.6347 - valid_reg: 0.000000 - valid_acc: 0.6355 - epoch_time: 62.8247 s\n",
            "Epoch 92\n",
            "[====================] 200/200: - running_loss: 0.6014 - running_reg: 0.000000 - running_acc: 0.6809 - lr: 0.00037 - epoch_loss: 0.5951 - epoch_reg: 0.000000 - epoch_acc: 0.6816 - valid_loss: 0.6161 - valid_reg: 0.000000 - valid_acc: 0.6638 - epoch_time: 63.0346 s\n",
            "Epoch 93\n",
            "[====================] 200/200: - running_loss: 0.6024 - running_reg: 0.000000 - running_acc: 0.6712 - lr: 0.00036 - epoch_loss: 0.5982 - epoch_reg: 0.000000 - epoch_acc: 0.6784 - valid_loss: 0.6184 - valid_reg: 0.000000 - valid_acc: 0.6608 - epoch_time: 62.7115 s\n",
            "Epoch 94\n",
            "[====================] 200/200: - running_loss: 0.5881 - running_reg: 0.000000 - running_acc: 0.6899 - lr: 0.00036 - epoch_loss: 0.5871 - epoch_reg: 0.000000 - epoch_acc: 0.6902 - valid_loss: 0.6323 - valid_reg: 0.000000 - valid_acc: 0.6479 - epoch_time: 62.6313 s\n",
            "Epoch 95\n",
            "[====================] 200/200: - running_loss: 0.5879 - running_reg: 0.000000 - running_acc: 0.6818 - lr: 0.00036 - epoch_loss: 0.5884 - epoch_reg: 0.000000 - epoch_acc: 0.6819 - valid_loss: 0.6330 - valid_reg: 0.000000 - valid_acc: 0.6612 - epoch_time: 62.6933 s\n",
            "Epoch 96\n",
            "[====================] 200/200: - running_loss: 0.5888 - running_reg: 0.000000 - running_acc: 0.6895 - lr: 0.00036 - epoch_loss: 0.5922 - epoch_reg: 0.000000 - epoch_acc: 0.6812 - valid_loss: 0.6304 - valid_reg: 0.000000 - valid_acc: 0.6470 - epoch_time: 62.7670 s\n",
            "Epoch 97\n",
            "[====================] 200/200: - running_loss: 0.5901 - running_reg: 0.000000 - running_acc: 0.6844 - lr: 0.00036 - epoch_loss: 0.5932 - epoch_reg: 0.000000 - epoch_acc: 0.6805 - valid_loss: 0.6160 - valid_reg: 0.000000 - valid_acc: 0.6632 - epoch_time: 61.9630 s\n",
            "Epoch 98\n",
            "[====================] 200/200: - running_loss: 0.5760 - running_reg: 0.000000 - running_acc: 0.7002 - lr: 0.00036 - epoch_loss: 0.5778 - epoch_reg: 0.000000 - epoch_acc: 0.6967 - valid_loss: 0.6178 - valid_reg: 0.000000 - valid_acc: 0.6583 - epoch_time: 59.6121 s\n",
            "Epoch 99\n",
            "[====================] 200/200: - running_loss: 0.5839 - running_reg: 0.000000 - running_acc: 0.6916 - lr: 0.00035 - epoch_loss: 0.5874 - epoch_reg: 0.000000 - epoch_acc: 0.6869 - valid_loss: 0.6310 - valid_reg: 0.000000 - valid_acc: 0.6579 - epoch_time: 59.7548 s\n",
            " - test_loss: 0.6157 - test_reg: 0.000000 - test_acc: 0.6656 - test_time: 34.1004 s\n",
            "Original model 3464194 params, new model 3464194 params, ratio 1.0\n",
            "Epoch 0\n",
            "[====================] 200/200: - running_loss: 0.6940 - running_reg: 0.000000 - running_acc: 0.5278 - lr: 0.00001 - epoch_loss: 0.6943 - epoch_reg: 0.000000 - epoch_acc: 0.5269 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 39.2756 s\n",
            "Epoch 1\n",
            "[====================] 200/200: - running_loss: 0.6930 - running_reg: 0.000000 - running_acc: 0.5234 - lr: 0.00003 - epoch_loss: 0.6927 - epoch_reg: 0.000000 - epoch_acc: 0.5216 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 28.5010 s\n",
            "Epoch 2\n",
            "[====================] 200/200: - running_loss: 0.6910 - running_reg: 0.000000 - running_acc: 0.5238 - lr: 0.00004 - epoch_loss: 0.6919 - epoch_reg: 0.000000 - epoch_acc: 0.5208 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 28.5372 s\n",
            "Epoch 3\n",
            "[====================] 200/200: - running_loss: 0.6879 - running_reg: 0.000000 - running_acc: 0.5431 - lr: 0.00006 - epoch_loss: 0.6893 - epoch_reg: 0.000000 - epoch_acc: 0.5383 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 28.4187 s\n",
            "Epoch 4\n",
            "[====================] 200/200: - running_loss: 0.6967 - running_reg: 0.000000 - running_acc: 0.5257 - lr: 0.00007 - epoch_loss: 0.6945 - epoch_reg: 0.000000 - epoch_acc: 0.5295 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 28.5382 s\n",
            "Epoch 5\n",
            "[====================] 200/200: - running_loss: 0.6869 - running_reg: 0.000000 - running_acc: 0.5553 - lr: 0.00008 - epoch_loss: 0.6879 - epoch_reg: 0.000000 - epoch_acc: 0.5505 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 28.4539 s\n",
            "Epoch 6\n",
            "[====================] 200/200: - running_loss: 0.6879 - running_reg: 0.000000 - running_acc: 0.5467 - lr: 0.00010 - epoch_loss: 0.6894 - epoch_reg: 0.000000 - epoch_acc: 0.5394 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 28.5466 s\n",
            "Epoch 7\n",
            "[====================] 200/200: - running_loss: 0.6968 - running_reg: 0.000000 - running_acc: 0.5180 - lr: 0.00011 - epoch_loss: 0.6920 - epoch_reg: 0.000000 - epoch_acc: 0.5333 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 28.5267 s\n",
            "Epoch 8\n",
            "[====================] 200/200: - running_loss: 0.6914 - running_reg: 0.000000 - running_acc: 0.5243 - lr: 0.00013 - epoch_loss: 0.6920 - epoch_reg: 0.000000 - epoch_acc: 0.5217 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 28.5355 s\n",
            "Epoch 9\n",
            "[====================] 200/200: - running_loss: 0.6872 - running_reg: 0.000000 - running_acc: 0.5432 - lr: 0.00014 - epoch_loss: 0.6856 - epoch_reg: 0.000000 - epoch_acc: 0.5503 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 28.5350 s\n",
            "Epoch 10\n",
            "[====================] 200/200: - running_loss: 0.6865 - running_reg: 0.000000 - running_acc: 0.5570 - lr: 0.00015 - epoch_loss: 0.6864 - epoch_reg: 0.000000 - epoch_acc: 0.5548 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 28.5510 s\n",
            "Epoch 11\n",
            "[====================] 200/200: - running_loss: 0.6898 - running_reg: 0.000000 - running_acc: 0.5341 - lr: 0.00017 - epoch_loss: 0.6906 - epoch_reg: 0.000000 - epoch_acc: 0.5327 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 28.4337 s\n",
            "Epoch 12\n",
            "[====================] 200/200: - running_loss: 0.6874 - running_reg: 0.000000 - running_acc: 0.5462 - lr: 0.00018 - epoch_loss: 0.6861 - epoch_reg: 0.000000 - epoch_acc: 0.5508 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 28.5821 s\n",
            "Epoch 13\n",
            "[====================] 200/200: - running_loss: 0.6856 - running_reg: 0.000000 - running_acc: 0.5458 - lr: 0.00020 - epoch_loss: 0.6861 - epoch_reg: 0.000000 - epoch_acc: 0.5416 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 28.5999 s\n",
            "Epoch 14\n",
            "[====================] 200/200: - running_loss: 0.6908 - running_reg: 0.000000 - running_acc: 0.5372 - lr: 0.00021 - epoch_loss: 0.6875 - epoch_reg: 0.000000 - epoch_acc: 0.5453 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 28.5667 s\n",
            "Epoch 15\n",
            "[====================] 200/200: - running_loss: 0.6814 - running_reg: 0.000000 - running_acc: 0.5638 - lr: 0.00022 - epoch_loss: 0.6835 - epoch_reg: 0.000000 - epoch_acc: 0.5578 - valid_loss: 0.6946 - valid_reg: 0.000000 - valid_acc: 0.5161 - epoch_time: 62.5496 s\n",
            "Epoch 16\n",
            "[====================] 200/200: - running_loss: 0.6919 - running_reg: 0.000000 - running_acc: 0.5222 - lr: 0.00024 - epoch_loss: 0.6907 - epoch_reg: 0.000000 - epoch_acc: 0.5253 - valid_loss: 0.6864 - valid_reg: 0.000000 - valid_acc: 0.5758 - epoch_time: 62.6772 s\n",
            "Epoch 17\n",
            "[====================] 200/200: - running_loss: 0.6867 - running_reg: 0.000000 - running_acc: 0.5466 - lr: 0.00025 - epoch_loss: 0.6875 - epoch_reg: 0.000000 - epoch_acc: 0.5434 - valid_loss: 0.6723 - valid_reg: 0.000000 - valid_acc: 0.5884 - epoch_time: 62.5966 s\n",
            "Epoch 18\n",
            "[====================] 200/200: - running_loss: 0.6869 - running_reg: 0.000000 - running_acc: 0.5429 - lr: 0.00027 - epoch_loss: 0.6861 - epoch_reg: 0.000000 - epoch_acc: 0.5445 - valid_loss: 0.6977 - valid_reg: 0.000000 - valid_acc: 0.4998 - epoch_time: 62.5933 s\n",
            "Epoch 19\n",
            "[====================] 200/200: - running_loss: 0.6856 - running_reg: 0.000000 - running_acc: 0.5566 - lr: 0.00028 - epoch_loss: 0.6866 - epoch_reg: 0.000000 - epoch_acc: 0.5506 - valid_loss: 0.6922 - valid_reg: 0.000000 - valid_acc: 0.5099 - epoch_time: 62.6445 s\n",
            "Epoch 20\n",
            "[====================] 200/200: - running_loss: 0.6882 - running_reg: 0.000000 - running_acc: 0.5369 - lr: 0.00029 - epoch_loss: 0.6877 - epoch_reg: 0.000000 - epoch_acc: 0.5398 - valid_loss: 0.7450 - valid_reg: 0.000000 - valid_acc: 0.5033 - epoch_time: 62.8325 s\n",
            "Epoch 21\n",
            "[====================] 200/200: - running_loss: 0.6888 - running_reg: 0.000000 - running_acc: 0.5430 - lr: 0.00031 - epoch_loss: 0.6929 - epoch_reg: 0.000000 - epoch_acc: 0.5314 - valid_loss: 0.6886 - valid_reg: 0.000000 - valid_acc: 0.5232 - epoch_time: 62.8098 s\n",
            "Epoch 22\n",
            "[====================] 200/200: - running_loss: 0.6821 - running_reg: 0.000000 - running_acc: 0.5566 - lr: 0.00032 - epoch_loss: 0.6840 - epoch_reg: 0.000000 - epoch_acc: 0.5473 - valid_loss: 0.6870 - valid_reg: 0.000000 - valid_acc: 0.5556 - epoch_time: 62.6676 s\n",
            "Epoch 23\n",
            "[====================] 200/200: - running_loss: 0.6873 - running_reg: 0.000000 - running_acc: 0.5397 - lr: 0.00034 - epoch_loss: 0.6865 - epoch_reg: 0.000000 - epoch_acc: 0.5423 - valid_loss: 0.6837 - valid_reg: 0.000000 - valid_acc: 0.5760 - epoch_time: 62.7310 s\n",
            "Epoch 24\n",
            "[====================] 200/200: - running_loss: 0.6752 - running_reg: 0.000000 - running_acc: 0.5824 - lr: 0.00035 - epoch_loss: 0.6747 - epoch_reg: 0.000000 - epoch_acc: 0.5834 - valid_loss: 0.6911 - valid_reg: 0.000000 - valid_acc: 0.5205 - epoch_time: 62.7935 s\n",
            "Epoch 25\n",
            "[====================] 200/200: - running_loss: 0.6770 - running_reg: 0.000000 - running_acc: 0.5681 - lr: 0.00036 - epoch_loss: 0.6792 - epoch_reg: 0.000000 - epoch_acc: 0.5627 - valid_loss: 0.6775 - valid_reg: 0.000000 - valid_acc: 0.5665 - epoch_time: 62.6889 s\n",
            "Epoch 26\n",
            "[====================] 200/200: - running_loss: 0.6716 - running_reg: 0.000000 - running_acc: 0.5902 - lr: 0.00038 - epoch_loss: 0.6768 - epoch_reg: 0.000000 - epoch_acc: 0.5759 - valid_loss: 0.6666 - valid_reg: 0.000000 - valid_acc: 0.5885 - epoch_time: 62.8904 s\n",
            "Epoch 27\n",
            "[====================] 200/200: - running_loss: 0.6717 - running_reg: 0.000000 - running_acc: 0.5833 - lr: 0.00039 - epoch_loss: 0.6713 - epoch_reg: 0.000000 - epoch_acc: 0.5817 - valid_loss: 0.6645 - valid_reg: 0.000000 - valid_acc: 0.6027 - epoch_time: 62.8931 s\n",
            "Epoch 28\n",
            "[====================] 200/200: - running_loss: 0.6673 - running_reg: 0.000000 - running_acc: 0.5856 - lr: 0.00041 - epoch_loss: 0.6682 - epoch_reg: 0.000000 - epoch_acc: 0.5813 - valid_loss: 0.6593 - valid_reg: 0.000000 - valid_acc: 0.6059 - epoch_time: 62.7818 s\n",
            "Epoch 29\n",
            "[====================] 200/200: - running_loss: 0.6661 - running_reg: 0.000000 - running_acc: 0.5953 - lr: 0.00042 - epoch_loss: 0.6656 - epoch_reg: 0.000000 - epoch_acc: 0.5962 - valid_loss: 0.6656 - valid_reg: 0.000000 - valid_acc: 0.5868 - epoch_time: 63.0501 s\n",
            "Epoch 30\n",
            "[====================] 200/200: - running_loss: 0.6681 - running_reg: 0.000000 - running_acc: 0.5888 - lr: 0.00043 - epoch_loss: 0.6700 - epoch_reg: 0.000000 - epoch_acc: 0.5858 - valid_loss: 0.6845 - valid_reg: 0.000000 - valid_acc: 0.5619 - epoch_time: 63.0012 s\n",
            "Epoch 31\n",
            "[====================] 200/200: - running_loss: 0.6684 - running_reg: 0.000000 - running_acc: 0.5923 - lr: 0.00045 - epoch_loss: 0.6689 - epoch_reg: 0.000000 - epoch_acc: 0.5872 - valid_loss: 0.6670 - valid_reg: 0.000000 - valid_acc: 0.5864 - epoch_time: 63.0364 s\n",
            "Epoch 32\n",
            "[====================] 200/200: - running_loss: 0.6544 - running_reg: 0.000000 - running_acc: 0.6192 - lr: 0.00046 - epoch_loss: 0.6573 - epoch_reg: 0.000000 - epoch_acc: 0.6158 - valid_loss: 0.6601 - valid_reg: 0.000000 - valid_acc: 0.6126 - epoch_time: 63.0227 s\n",
            "Epoch 33\n",
            "[====================] 200/200: - running_loss: 0.6632 - running_reg: 0.000000 - running_acc: 0.6019 - lr: 0.00048 - epoch_loss: 0.6636 - epoch_reg: 0.000000 - epoch_acc: 0.6014 - valid_loss: 0.6599 - valid_reg: 0.000000 - valid_acc: 0.6032 - epoch_time: 62.8189 s\n",
            "Epoch 34\n",
            "[====================] 200/200: - running_loss: 0.6510 - running_reg: 0.000000 - running_acc: 0.6169 - lr: 0.00049 - epoch_loss: 0.6508 - epoch_reg: 0.000000 - epoch_acc: 0.6166 - valid_loss: 0.6527 - valid_reg: 0.000000 - valid_acc: 0.6207 - epoch_time: 63.2119 s\n",
            "Epoch 35\n",
            "[====================] 200/200: - running_loss: 0.6566 - running_reg: 0.000000 - running_acc: 0.6073 - lr: 0.00050 - epoch_loss: 0.6571 - epoch_reg: 0.000000 - epoch_acc: 0.6044 - valid_loss: 0.6528 - valid_reg: 0.000000 - valid_acc: 0.6108 - epoch_time: 62.8867 s\n",
            "Epoch 36\n",
            "[====================] 200/200: - running_loss: 0.6563 - running_reg: 0.000000 - running_acc: 0.6104 - lr: 0.00052 - epoch_loss: 0.6519 - epoch_reg: 0.000000 - epoch_acc: 0.6184 - valid_loss: 0.6642 - valid_reg: 0.000000 - valid_acc: 0.6099 - epoch_time: 63.0739 s\n",
            "Epoch 37\n",
            "[====================] 200/200: - running_loss: 0.6501 - running_reg: 0.000000 - running_acc: 0.6192 - lr: 0.00053 - epoch_loss: 0.6537 - epoch_reg: 0.000000 - epoch_acc: 0.6125 - valid_loss: 0.6679 - valid_reg: 0.000000 - valid_acc: 0.5830 - epoch_time: 63.0842 s\n",
            "Epoch 38\n",
            "[====================] 200/200: - running_loss: 0.6485 - running_reg: 0.000000 - running_acc: 0.6172 - lr: 0.00054 - epoch_loss: 0.6492 - epoch_reg: 0.000000 - epoch_acc: 0.6144 - valid_loss: 0.6702 - valid_reg: 0.000000 - valid_acc: 0.5909 - epoch_time: 62.8338 s\n",
            "Epoch 39\n",
            "[====================] 200/200: - running_loss: 0.6435 - running_reg: 0.000000 - running_acc: 0.6276 - lr: 0.00056 - epoch_loss: 0.6453 - epoch_reg: 0.000000 - epoch_acc: 0.6256 - valid_loss: 0.6631 - valid_reg: 0.000000 - valid_acc: 0.5984 - epoch_time: 62.8881 s\n",
            "Epoch 40\n",
            "[====================] 200/200: - running_loss: 0.6413 - running_reg: 0.000000 - running_acc: 0.6349 - lr: 0.00055 - epoch_loss: 0.6399 - epoch_reg: 0.000000 - epoch_acc: 0.6309 - valid_loss: 0.6607 - valid_reg: 0.000000 - valid_acc: 0.6110 - epoch_time: 62.7850 s\n",
            "Epoch 41\n",
            "[====================] 200/200: - running_loss: 0.6385 - running_reg: 0.000000 - running_acc: 0.6342 - lr: 0.00055 - epoch_loss: 0.6412 - epoch_reg: 0.000000 - epoch_acc: 0.6294 - valid_loss: 0.6470 - valid_reg: 0.000000 - valid_acc: 0.6336 - epoch_time: 62.9243 s\n",
            "Epoch 42\n",
            "[====================] 200/200: - running_loss: 0.6434 - running_reg: 0.000000 - running_acc: 0.6291 - lr: 0.00054 - epoch_loss: 0.6417 - epoch_reg: 0.000000 - epoch_acc: 0.6336 - valid_loss: 0.6603 - valid_reg: 0.000000 - valid_acc: 0.5995 - epoch_time: 62.6698 s\n",
            "Epoch 43\n",
            "[====================] 200/200: - running_loss: 0.6466 - running_reg: 0.000000 - running_acc: 0.6273 - lr: 0.00053 - epoch_loss: 0.6418 - epoch_reg: 0.000000 - epoch_acc: 0.6339 - valid_loss: 0.6396 - valid_reg: 0.000000 - valid_acc: 0.6389 - epoch_time: 63.0193 s\n",
            "Epoch 44\n",
            "[====================] 200/200: - running_loss: 0.6438 - running_reg: 0.000000 - running_acc: 0.6315 - lr: 0.00053 - epoch_loss: 0.6445 - epoch_reg: 0.000000 - epoch_acc: 0.6283 - valid_loss: 0.6439 - valid_reg: 0.000000 - valid_acc: 0.6346 - epoch_time: 62.8676 s\n",
            "Epoch 45\n",
            "[====================] 200/200: - running_loss: 0.6388 - running_reg: 0.000000 - running_acc: 0.6356 - lr: 0.00052 - epoch_loss: 0.6388 - epoch_reg: 0.000000 - epoch_acc: 0.6370 - valid_loss: 0.6624 - valid_reg: 0.000000 - valid_acc: 0.5856 - epoch_time: 62.7292 s\n",
            "Epoch 46\n",
            "[====================] 200/200: - running_loss: 0.6315 - running_reg: 0.000000 - running_acc: 0.6402 - lr: 0.00052 - epoch_loss: 0.6292 - epoch_reg: 0.000000 - epoch_acc: 0.6422 - valid_loss: 0.6373 - valid_reg: 0.000000 - valid_acc: 0.6428 - epoch_time: 62.8783 s\n",
            "Epoch 47\n",
            "[====================] 200/200: - running_loss: 0.6255 - running_reg: 0.000000 - running_acc: 0.6505 - lr: 0.00051 - epoch_loss: 0.6225 - epoch_reg: 0.000000 - epoch_acc: 0.6555 - valid_loss: 0.6411 - valid_reg: 0.000000 - valid_acc: 0.6282 - epoch_time: 62.7392 s\n",
            "Epoch 48\n",
            "[====================] 200/200: - running_loss: 0.6326 - running_reg: 0.000000 - running_acc: 0.6359 - lr: 0.00051 - epoch_loss: 0.6306 - epoch_reg: 0.000000 - epoch_acc: 0.6366 - valid_loss: 0.6308 - valid_reg: 0.000000 - valid_acc: 0.6467 - epoch_time: 62.9595 s\n",
            "Epoch 49\n",
            "[====================] 200/200: - running_loss: 0.6151 - running_reg: 0.000000 - running_acc: 0.6541 - lr: 0.00050 - epoch_loss: 0.6160 - epoch_reg: 0.000000 - epoch_acc: 0.6553 - valid_loss: 0.6313 - valid_reg: 0.000000 - valid_acc: 0.6474 - epoch_time: 62.7931 s\n",
            "Epoch 50\n",
            "[====================] 200/200: - running_loss: 0.6346 - running_reg: 0.000000 - running_acc: 0.6404 - lr: 0.00050 - epoch_loss: 0.6356 - epoch_reg: 0.000000 - epoch_acc: 0.6402 - valid_loss: 0.6330 - valid_reg: 0.000000 - valid_acc: 0.6432 - epoch_time: 62.7771 s\n",
            "Epoch 51\n",
            "[====================] 200/200: - running_loss: 0.6176 - running_reg: 0.000000 - running_acc: 0.6618 - lr: 0.00049 - epoch_loss: 0.6189 - epoch_reg: 0.000000 - epoch_acc: 0.6597 - valid_loss: 0.6419 - valid_reg: 0.000000 - valid_acc: 0.6467 - epoch_time: 62.7537 s\n",
            "Epoch 52\n",
            "[====================] 200/200: - running_loss: 0.6155 - running_reg: 0.000000 - running_acc: 0.6649 - lr: 0.00049 - epoch_loss: 0.6171 - epoch_reg: 0.000000 - epoch_acc: 0.6619 - valid_loss: 0.6337 - valid_reg: 0.000000 - valid_acc: 0.6448 - epoch_time: 62.8207 s\n",
            "Epoch 53\n",
            "[====================] 200/200: - running_loss: 0.6337 - running_reg: 0.000000 - running_acc: 0.6297 - lr: 0.00048 - epoch_loss: 0.6297 - epoch_reg: 0.000000 - epoch_acc: 0.6386 - valid_loss: 0.6290 - valid_reg: 0.000000 - valid_acc: 0.6453 - epoch_time: 62.8474 s\n",
            "Epoch 54\n",
            "[====================] 200/200: - running_loss: 0.6203 - running_reg: 0.000000 - running_acc: 0.6505 - lr: 0.00048 - epoch_loss: 0.6208 - epoch_reg: 0.000000 - epoch_acc: 0.6492 - valid_loss: 0.6276 - valid_reg: 0.000000 - valid_acc: 0.6524 - epoch_time: 62.9903 s\n",
            "Epoch 55\n",
            "[====================] 200/200: - running_loss: 0.6201 - running_reg: 0.000000 - running_acc: 0.6491 - lr: 0.00047 - epoch_loss: 0.6221 - epoch_reg: 0.000000 - epoch_acc: 0.6473 - valid_loss: 0.6276 - valid_reg: 0.000000 - valid_acc: 0.6558 - epoch_time: 62.9813 s\n",
            "Epoch 56\n",
            "[====================] 200/200: - running_loss: 0.6133 - running_reg: 0.000000 - running_acc: 0.6583 - lr: 0.00047 - epoch_loss: 0.6172 - epoch_reg: 0.000000 - epoch_acc: 0.6536 - valid_loss: 0.6245 - valid_reg: 0.000000 - valid_acc: 0.6564 - epoch_time: 62.9809 s\n",
            "Epoch 57\n",
            "[====================] 200/200: - running_loss: 0.6142 - running_reg: 0.000000 - running_acc: 0.6663 - lr: 0.00046 - epoch_loss: 0.6137 - epoch_reg: 0.000000 - epoch_acc: 0.6636 - valid_loss: 0.6485 - valid_reg: 0.000000 - valid_acc: 0.6344 - epoch_time: 62.8969 s\n",
            "Epoch 58\n",
            "[====================] 200/200: - running_loss: 0.6220 - running_reg: 0.000000 - running_acc: 0.6520 - lr: 0.00046 - epoch_loss: 0.6203 - epoch_reg: 0.000000 - epoch_acc: 0.6550 - valid_loss: 0.6296 - valid_reg: 0.000000 - valid_acc: 0.6492 - epoch_time: 62.8090 s\n",
            "Epoch 59\n",
            "[====================] 200/200: - running_loss: 0.6151 - running_reg: 0.000000 - running_acc: 0.6659 - lr: 0.00046 - epoch_loss: 0.6150 - epoch_reg: 0.000000 - epoch_acc: 0.6656 - valid_loss: 0.6294 - valid_reg: 0.000000 - valid_acc: 0.6512 - epoch_time: 62.8361 s\n",
            "Epoch 60\n",
            "[====================] 200/200: - running_loss: 0.6152 - running_reg: 0.000000 - running_acc: 0.6578 - lr: 0.00045 - epoch_loss: 0.6151 - epoch_reg: 0.000000 - epoch_acc: 0.6589 - valid_loss: 0.6451 - valid_reg: 0.000000 - valid_acc: 0.6266 - epoch_time: 62.8194 s\n",
            "Epoch 61\n",
            "[====================] 200/200: - running_loss: 0.6112 - running_reg: 0.000000 - running_acc: 0.6548 - lr: 0.00045 - epoch_loss: 0.6147 - epoch_reg: 0.000000 - epoch_acc: 0.6544 - valid_loss: 0.6245 - valid_reg: 0.000000 - valid_acc: 0.6568 - epoch_time: 62.8748 s\n",
            "Epoch 62\n",
            "[====================] 200/200: - running_loss: 0.6099 - running_reg: 0.000000 - running_acc: 0.6674 - lr: 0.00045 - epoch_loss: 0.6097 - epoch_reg: 0.000000 - epoch_acc: 0.6698 - valid_loss: 0.6289 - valid_reg: 0.000000 - valid_acc: 0.6573 - epoch_time: 62.9112 s\n",
            "Epoch 63\n",
            "[====================] 200/200: - running_loss: 0.6098 - running_reg: 0.000000 - running_acc: 0.6659 - lr: 0.00044 - epoch_loss: 0.6097 - epoch_reg: 0.000000 - epoch_acc: 0.6667 - valid_loss: 0.6223 - valid_reg: 0.000000 - valid_acc: 0.6583 - epoch_time: 62.9780 s\n",
            "Epoch 64\n",
            "[====================] 200/200: - running_loss: 0.6033 - running_reg: 0.000000 - running_acc: 0.6731 - lr: 0.00044 - epoch_loss: 0.6059 - epoch_reg: 0.000000 - epoch_acc: 0.6720 - valid_loss: 0.6234 - valid_reg: 0.000000 - valid_acc: 0.6561 - epoch_time: 62.8592 s\n",
            "Epoch 65\n",
            "[====================] 200/200: - running_loss: 0.6242 - running_reg: 0.000000 - running_acc: 0.6466 - lr: 0.00044 - epoch_loss: 0.6185 - epoch_reg: 0.000000 - epoch_acc: 0.6567 - valid_loss: 0.6408 - valid_reg: 0.000000 - valid_acc: 0.6304 - epoch_time: 62.8966 s\n",
            "Epoch 66\n",
            "[====================] 200/200: - running_loss: 0.6052 - running_reg: 0.000000 - running_acc: 0.6671 - lr: 0.00043 - epoch_loss: 0.6094 - epoch_reg: 0.000000 - epoch_acc: 0.6644 - valid_loss: 0.6535 - valid_reg: 0.000000 - valid_acc: 0.6407 - epoch_time: 62.7661 s\n",
            "Epoch 67\n",
            "[====================] 200/200: - running_loss: 0.6041 - running_reg: 0.000000 - running_acc: 0.6729 - lr: 0.00043 - epoch_loss: 0.6016 - epoch_reg: 0.000000 - epoch_acc: 0.6747 - valid_loss: 0.6222 - valid_reg: 0.000000 - valid_acc: 0.6586 - epoch_time: 62.8225 s\n",
            "Epoch 68\n",
            "[====================] 200/200: - running_loss: 0.6137 - running_reg: 0.000000 - running_acc: 0.6591 - lr: 0.00043 - epoch_loss: 0.6065 - epoch_reg: 0.000000 - epoch_acc: 0.6687 - valid_loss: 0.6378 - valid_reg: 0.000000 - valid_acc: 0.6319 - epoch_time: 62.9729 s\n",
            "Epoch 69\n",
            "[====================] 200/200: - running_loss: 0.6118 - running_reg: 0.000000 - running_acc: 0.6658 - lr: 0.00042 - epoch_loss: 0.6091 - epoch_reg: 0.000000 - epoch_acc: 0.6678 - valid_loss: 0.6249 - valid_reg: 0.000000 - valid_acc: 0.6523 - epoch_time: 62.8345 s\n",
            "Epoch 70\n",
            "[====================] 200/200: - running_loss: 0.6170 - running_reg: 0.000000 - running_acc: 0.6609 - lr: 0.00042 - epoch_loss: 0.6134 - epoch_reg: 0.000000 - epoch_acc: 0.6633 - valid_loss: 0.6376 - valid_reg: 0.000000 - valid_acc: 0.6363 - epoch_time: 62.8417 s\n",
            "Epoch 71\n",
            "[====================] 200/200: - running_loss: 0.6051 - running_reg: 0.000000 - running_acc: 0.6666 - lr: 0.00042 - epoch_loss: 0.6078 - epoch_reg: 0.000000 - epoch_acc: 0.6644 - valid_loss: 0.6469 - valid_reg: 0.000000 - valid_acc: 0.6299 - epoch_time: 62.8711 s\n",
            "Epoch 72\n",
            "[====================] 200/200: - running_loss: 0.5890 - running_reg: 0.000000 - running_acc: 0.6847 - lr: 0.00041 - epoch_loss: 0.5922 - epoch_reg: 0.000000 - epoch_acc: 0.6814 - valid_loss: 0.6238 - valid_reg: 0.000000 - valid_acc: 0.6614 - epoch_time: 62.9196 s\n",
            "Epoch 73\n",
            "[====================] 200/200: - running_loss: 0.6021 - running_reg: 0.000000 - running_acc: 0.6714 - lr: 0.00041 - epoch_loss: 0.6008 - epoch_reg: 0.000000 - epoch_acc: 0.6714 - valid_loss: 0.6230 - valid_reg: 0.000000 - valid_acc: 0.6569 - epoch_time: 62.8650 s\n",
            "Epoch 74\n",
            "[====================] 200/200: - running_loss: 0.5986 - running_reg: 0.000000 - running_acc: 0.6751 - lr: 0.00041 - epoch_loss: 0.6012 - epoch_reg: 0.000000 - epoch_acc: 0.6728 - valid_loss: 0.6230 - valid_reg: 0.000000 - valid_acc: 0.6562 - epoch_time: 62.8532 s\n",
            "Epoch 75\n",
            "[====================] 200/200: - running_loss: 0.5991 - running_reg: 0.000000 - running_acc: 0.6725 - lr: 0.00041 - epoch_loss: 0.5949 - epoch_reg: 0.000000 - epoch_acc: 0.6786 - valid_loss: 0.6259 - valid_reg: 0.000000 - valid_acc: 0.6532 - epoch_time: 62.8105 s\n",
            "Epoch 76\n",
            "[====================] 200/200: - running_loss: 0.6014 - running_reg: 0.000000 - running_acc: 0.6734 - lr: 0.00040 - epoch_loss: 0.6000 - epoch_reg: 0.000000 - epoch_acc: 0.6756 - valid_loss: 0.6207 - valid_reg: 0.000000 - valid_acc: 0.6589 - epoch_time: 62.8586 s\n",
            "Epoch 77\n",
            "[====================] 200/200: - running_loss: 0.6077 - running_reg: 0.000000 - running_acc: 0.6729 - lr: 0.00040 - epoch_loss: 0.6064 - epoch_reg: 0.000000 - epoch_acc: 0.6745 - valid_loss: 0.6254 - valid_reg: 0.000000 - valid_acc: 0.6582 - epoch_time: 62.7943 s\n",
            "Epoch 78\n",
            "[====================] 200/200: - running_loss: 0.5937 - running_reg: 0.000000 - running_acc: 0.6818 - lr: 0.00040 - epoch_loss: 0.5941 - epoch_reg: 0.000000 - epoch_acc: 0.6814 - valid_loss: 0.6248 - valid_reg: 0.000000 - valid_acc: 0.6518 - epoch_time: 62.9472 s\n",
            "Epoch 79\n",
            "[====================] 200/200: - running_loss: 0.6061 - running_reg: 0.000000 - running_acc: 0.6675 - lr: 0.00040 - epoch_loss: 0.6080 - epoch_reg: 0.000000 - epoch_acc: 0.6636 - valid_loss: 0.6282 - valid_reg: 0.000000 - valid_acc: 0.6512 - epoch_time: 62.8698 s\n",
            "Epoch 80\n",
            "[====================] 200/200: - running_loss: 0.5855 - running_reg: 0.000000 - running_acc: 0.6892 - lr: 0.00039 - epoch_loss: 0.5905 - epoch_reg: 0.000000 - epoch_acc: 0.6853 - valid_loss: 0.6345 - valid_reg: 0.000000 - valid_acc: 0.6580 - epoch_time: 62.8859 s\n",
            "Epoch 81\n",
            "[====================] 200/200: - running_loss: 0.5967 - running_reg: 0.000000 - running_acc: 0.6773 - lr: 0.00039 - epoch_loss: 0.5994 - epoch_reg: 0.000000 - epoch_acc: 0.6762 - valid_loss: 0.6293 - valid_reg: 0.000000 - valid_acc: 0.6612 - epoch_time: 62.8972 s\n",
            "Epoch 82\n",
            "[====================] 200/200: - running_loss: 0.5895 - running_reg: 0.000000 - running_acc: 0.6880 - lr: 0.00039 - epoch_loss: 0.5878 - epoch_reg: 0.000000 - epoch_acc: 0.6919 - valid_loss: 0.6312 - valid_reg: 0.000000 - valid_acc: 0.6389 - epoch_time: 62.7525 s\n",
            "Epoch 83\n",
            "[====================] 200/200: - running_loss: 0.5992 - running_reg: 0.000000 - running_acc: 0.6696 - lr: 0.00039 - epoch_loss: 0.5954 - epoch_reg: 0.000000 - epoch_acc: 0.6745 - valid_loss: 0.6182 - valid_reg: 0.000000 - valid_acc: 0.6629 - epoch_time: 62.9066 s\n",
            "Epoch 84\n",
            "[====================] 200/200: - running_loss: 0.5905 - running_reg: 0.000000 - running_acc: 0.6892 - lr: 0.00038 - epoch_loss: 0.5900 - epoch_reg: 0.000000 - epoch_acc: 0.6881 - valid_loss: 0.6383 - valid_reg: 0.000000 - valid_acc: 0.6478 - epoch_time: 62.9018 s\n",
            "Epoch 85\n",
            "[====================] 200/200: - running_loss: 0.5995 - running_reg: 0.000000 - running_acc: 0.6738 - lr: 0.00038 - epoch_loss: 0.6002 - epoch_reg: 0.000000 - epoch_acc: 0.6759 - valid_loss: 0.6349 - valid_reg: 0.000000 - valid_acc: 0.6625 - epoch_time: 62.8750 s\n",
            "Epoch 86\n",
            "[====================] 200/200: - running_loss: 0.5883 - running_reg: 0.000000 - running_acc: 0.6855 - lr: 0.00038 - epoch_loss: 0.5846 - epoch_reg: 0.000000 - epoch_acc: 0.6916 - valid_loss: 0.6176 - valid_reg: 0.000000 - valid_acc: 0.6644 - epoch_time: 62.9674 s\n",
            "Epoch 87\n",
            "[====================] 200/200: - running_loss: 0.5959 - running_reg: 0.000000 - running_acc: 0.6778 - lr: 0.00038 - epoch_loss: 0.5941 - epoch_reg: 0.000000 - epoch_acc: 0.6786 - valid_loss: 0.6239 - valid_reg: 0.000000 - valid_acc: 0.6641 - epoch_time: 62.8548 s\n",
            "Epoch 88\n",
            "[====================] 200/200: - running_loss: 0.5975 - running_reg: 0.000000 - running_acc: 0.6837 - lr: 0.00037 - epoch_loss: 0.5964 - epoch_reg: 0.000000 - epoch_acc: 0.6834 - valid_loss: 0.6173 - valid_reg: 0.000000 - valid_acc: 0.6647 - epoch_time: 63.0047 s\n",
            "Epoch 89\n",
            "[====================] 200/200: - running_loss: 0.5926 - running_reg: 0.000000 - running_acc: 0.6815 - lr: 0.00037 - epoch_loss: 0.5897 - epoch_reg: 0.000000 - epoch_acc: 0.6825 - valid_loss: 0.6207 - valid_reg: 0.000000 - valid_acc: 0.6596 - epoch_time: 62.7957 s\n",
            "Epoch 90\n",
            "[====================] 200/200: - running_loss: 0.5908 - running_reg: 0.000000 - running_acc: 0.6874 - lr: 0.00037 - epoch_loss: 0.5860 - epoch_reg: 0.000000 - epoch_acc: 0.6925 - valid_loss: 0.6236 - valid_reg: 0.000000 - valid_acc: 0.6583 - epoch_time: 62.8214 s\n",
            "Epoch 91\n",
            "[====================] 200/200: - running_loss: 0.5805 - running_reg: 0.000000 - running_acc: 0.6935 - lr: 0.00037 - epoch_loss: 0.5801 - epoch_reg: 0.000000 - epoch_acc: 0.6941 - valid_loss: 0.6511 - valid_reg: 0.000000 - valid_acc: 0.6456 - epoch_time: 62.6762 s\n",
            "Epoch 92\n",
            "[====================] 200/200: - running_loss: 0.5944 - running_reg: 0.000000 - running_acc: 0.6773 - lr: 0.00037 - epoch_loss: 0.5926 - epoch_reg: 0.000000 - epoch_acc: 0.6755 - valid_loss: 0.6175 - valid_reg: 0.000000 - valid_acc: 0.6637 - epoch_time: 62.8587 s\n",
            "Epoch 93\n",
            "[====================] 200/200: - running_loss: 0.5871 - running_reg: 0.000000 - running_acc: 0.6859 - lr: 0.00036 - epoch_loss: 0.5896 - epoch_reg: 0.000000 - epoch_acc: 0.6866 - valid_loss: 0.6229 - valid_reg: 0.000000 - valid_acc: 0.6570 - epoch_time: 63.1172 s\n",
            "Epoch 94\n",
            "[====================] 200/200: - running_loss: 0.5833 - running_reg: 0.000000 - running_acc: 0.6969 - lr: 0.00036 - epoch_loss: 0.5853 - epoch_reg: 0.000000 - epoch_acc: 0.6916 - valid_loss: 0.6307 - valid_reg: 0.000000 - valid_acc: 0.6614 - epoch_time: 62.8087 s\n",
            "Epoch 95\n",
            "[====================] 200/200: - running_loss: 0.5868 - running_reg: 0.000000 - running_acc: 0.6923 - lr: 0.00036 - epoch_loss: 0.5873 - epoch_reg: 0.000000 - epoch_acc: 0.6903 - valid_loss: 0.6160 - valid_reg: 0.000000 - valid_acc: 0.6654 - epoch_time: 62.8731 s\n",
            "Epoch 96\n",
            "[====================] 200/200: - running_loss: 0.5851 - running_reg: 0.000000 - running_acc: 0.6865 - lr: 0.00036 - epoch_loss: 0.5838 - epoch_reg: 0.000000 - epoch_acc: 0.6861 - valid_loss: 0.6169 - valid_reg: 0.000000 - valid_acc: 0.6637 - epoch_time: 62.7272 s\n",
            "Epoch 97\n",
            "[====================] 200/200: - running_loss: 0.5718 - running_reg: 0.000000 - running_acc: 0.6949 - lr: 0.00036 - epoch_loss: 0.5744 - epoch_reg: 0.000000 - epoch_acc: 0.6955 - valid_loss: 0.6230 - valid_reg: 0.000000 - valid_acc: 0.6638 - epoch_time: 61.9999 s\n",
            "Epoch 98\n",
            "[====================] 200/200: - running_loss: 0.5849 - running_reg: 0.000000 - running_acc: 0.6904 - lr: 0.00036 - epoch_loss: 0.5837 - epoch_reg: 0.000000 - epoch_acc: 0.6898 - valid_loss: 0.6193 - valid_reg: 0.000000 - valid_acc: 0.6655 - epoch_time: 59.7782 s\n",
            "Epoch 99\n",
            "[====================] 200/200: - running_loss: 0.5838 - running_reg: 0.000000 - running_acc: 0.6870 - lr: 0.00035 - epoch_loss: 0.5820 - epoch_reg: 0.000000 - epoch_acc: 0.6922 - valid_loss: 0.6179 - valid_reg: 0.000000 - valid_acc: 0.6643 - epoch_time: 59.5858 s\n",
            " - test_loss: 0.6193 - test_reg: 0.000000 - test_acc: 0.6655 - test_time: 33.9928 s\n",
            "Original model 3464194 params, new model 3464194 params, ratio 1.0\n",
            "Epoch 0\n",
            "[====================] 200/200: - running_loss: 0.6953 - running_reg: 0.000000 - running_acc: 0.5190 - lr: 0.00001 - epoch_loss: 0.6951 - epoch_reg: 0.000000 - epoch_acc: 0.5189 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 39.2033 s\n",
            "Epoch 1\n",
            "[====================] 200/200: - running_loss: 0.6902 - running_reg: 0.000000 - running_acc: 0.5379 - lr: 0.00003 - epoch_loss: 0.6909 - epoch_reg: 0.000000 - epoch_acc: 0.5327 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 28.3987 s\n",
            "Epoch 2\n",
            "[====================] 200/200: - running_loss: 0.6905 - running_reg: 0.000000 - running_acc: 0.5360 - lr: 0.00004 - epoch_loss: 0.6912 - epoch_reg: 0.000000 - epoch_acc: 0.5334 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 28.5341 s\n",
            "Epoch 3\n",
            "[====================] 200/200: - running_loss: 0.6833 - running_reg: 0.000000 - running_acc: 0.5669 - lr: 0.00006 - epoch_loss: 0.6831 - epoch_reg: 0.000000 - epoch_acc: 0.5659 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 28.4963 s\n",
            "Epoch 4\n",
            "[====================] 200/200: - running_loss: 0.6931 - running_reg: 0.000000 - running_acc: 0.5348 - lr: 0.00007 - epoch_loss: 0.6917 - epoch_reg: 0.000000 - epoch_acc: 0.5378 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 28.5365 s\n",
            "Epoch 5\n",
            "[====================] 200/200: - running_loss: 0.6817 - running_reg: 0.000000 - running_acc: 0.5637 - lr: 0.00008 - epoch_loss: 0.6839 - epoch_reg: 0.000000 - epoch_acc: 0.5595 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 28.5325 s\n",
            "Epoch 6\n",
            "[====================] 200/200: - running_loss: 0.6790 - running_reg: 0.000000 - running_acc: 0.5692 - lr: 0.00010 - epoch_loss: 0.6807 - epoch_reg: 0.000000 - epoch_acc: 0.5686 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 28.4810 s\n",
            "Epoch 7\n",
            "[====================] 200/200: - running_loss: 0.6864 - running_reg: 0.000000 - running_acc: 0.5483 - lr: 0.00011 - epoch_loss: 0.6868 - epoch_reg: 0.000000 - epoch_acc: 0.5481 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 28.5309 s\n",
            "Epoch 8\n",
            "[====================] 200/200: - running_loss: 0.6760 - running_reg: 0.000000 - running_acc: 0.5800 - lr: 0.00013 - epoch_loss: 0.6769 - epoch_reg: 0.000000 - epoch_acc: 0.5767 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 28.4881 s\n",
            "Epoch 9\n",
            "[====================] 200/200: - running_loss: 0.6831 - running_reg: 0.000000 - running_acc: 0.5509 - lr: 0.00014 - epoch_loss: 0.6797 - epoch_reg: 0.000000 - epoch_acc: 0.5644 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 28.5875 s\n",
            "Epoch 10\n",
            "[====================] 200/200: - running_loss: 0.6829 - running_reg: 0.000000 - running_acc: 0.5535 - lr: 0.00015 - epoch_loss: 0.6843 - epoch_reg: 0.000000 - epoch_acc: 0.5491 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 28.6423 s\n",
            "Epoch 11\n",
            "[====================] 200/200: - running_loss: 0.6855 - running_reg: 0.000000 - running_acc: 0.5466 - lr: 0.00017 - epoch_loss: 0.6854 - epoch_reg: 0.000000 - epoch_acc: 0.5464 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 28.5060 s\n",
            "Epoch 12\n",
            "[====================] 200/200: - running_loss: 0.6756 - running_reg: 0.000000 - running_acc: 0.5849 - lr: 0.00018 - epoch_loss: 0.6793 - epoch_reg: 0.000000 - epoch_acc: 0.5748 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 28.6358 s\n",
            "Epoch 13\n",
            "[====================] 200/200: - running_loss: 0.6951 - running_reg: 0.000000 - running_acc: 0.5035 - lr: 0.00020 - epoch_loss: 0.6961 - epoch_reg: 0.000000 - epoch_acc: 0.5030 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 28.6334 s\n",
            "Epoch 14\n",
            "[====================] 200/200: - running_loss: 0.6864 - running_reg: 0.000000 - running_acc: 0.5469 - lr: 0.00021 - epoch_loss: 0.6878 - epoch_reg: 0.000000 - epoch_acc: 0.5416 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 28.6138 s\n",
            "Epoch 15\n",
            "[====================] 200/200: - running_loss: 0.6806 - running_reg: 0.000000 - running_acc: 0.5559 - lr: 0.00022 - epoch_loss: 0.6822 - epoch_reg: 0.000000 - epoch_acc: 0.5542 - valid_loss: 0.7098 - valid_reg: 0.000000 - valid_acc: 0.5073 - epoch_time: 62.7860 s\n",
            "Epoch 16\n",
            "[====================] 200/200: - running_loss: 0.6829 - running_reg: 0.000000 - running_acc: 0.5472 - lr: 0.00024 - epoch_loss: 0.6869 - epoch_reg: 0.000000 - epoch_acc: 0.5336 - valid_loss: 0.6768 - valid_reg: 0.000000 - valid_acc: 0.5779 - epoch_time: 62.8299 s\n",
            "Epoch 17\n",
            "[====================] 200/200: - running_loss: 0.6768 - running_reg: 0.000000 - running_acc: 0.5696 - lr: 0.00025 - epoch_loss: 0.6798 - epoch_reg: 0.000000 - epoch_acc: 0.5605 - valid_loss: 0.6647 - valid_reg: 0.000000 - valid_acc: 0.5908 - epoch_time: 62.6823 s\n",
            "Epoch 18\n",
            "[====================] 200/200: - running_loss: 0.6839 - running_reg: 0.000000 - running_acc: 0.5525 - lr: 0.00027 - epoch_loss: 0.6818 - epoch_reg: 0.000000 - epoch_acc: 0.5595 - valid_loss: 0.6804 - valid_reg: 0.000000 - valid_acc: 0.5688 - epoch_time: 62.6990 s\n",
            "Epoch 19\n",
            "[====================] 200/200: - running_loss: 0.6795 - running_reg: 0.000000 - running_acc: 0.5637 - lr: 0.00028 - epoch_loss: 0.6784 - epoch_reg: 0.000000 - epoch_acc: 0.5652 - valid_loss: 0.6830 - valid_reg: 0.000000 - valid_acc: 0.5257 - epoch_time: 62.6475 s\n",
            "Epoch 20\n",
            "[====================] 200/200: - running_loss: 0.6805 - running_reg: 0.000000 - running_acc: 0.5685 - lr: 0.00029 - epoch_loss: 0.6782 - epoch_reg: 0.000000 - epoch_acc: 0.5734 - valid_loss: 0.6790 - valid_reg: 0.000000 - valid_acc: 0.5557 - epoch_time: 62.7151 s\n",
            "Epoch 21\n",
            "[====================] 200/200: - running_loss: 0.6758 - running_reg: 0.000000 - running_acc: 0.5756 - lr: 0.00031 - epoch_loss: 0.6734 - epoch_reg: 0.000000 - epoch_acc: 0.5786 - valid_loss: 0.7115 - valid_reg: 0.000000 - valid_acc: 0.5239 - epoch_time: 62.4801 s\n",
            "Epoch 22\n",
            "[====================] 200/200: - running_loss: 0.6700 - running_reg: 0.000000 - running_acc: 0.5853 - lr: 0.00032 - epoch_loss: 0.6717 - epoch_reg: 0.000000 - epoch_acc: 0.5823 - valid_loss: 0.6726 - valid_reg: 0.000000 - valid_acc: 0.5894 - epoch_time: 62.6391 s\n",
            "Epoch 23\n",
            "[====================] 200/200: - running_loss: 0.6700 - running_reg: 0.000000 - running_acc: 0.5908 - lr: 0.00034 - epoch_loss: 0.6693 - epoch_reg: 0.000000 - epoch_acc: 0.5939 - valid_loss: 0.7011 - valid_reg: 0.000000 - valid_acc: 0.5334 - epoch_time: 62.5389 s\n",
            "Epoch 24\n",
            "[====================] 200/200: - running_loss: 0.6633 - running_reg: 0.000000 - running_acc: 0.6023 - lr: 0.00035 - epoch_loss: 0.6653 - epoch_reg: 0.000000 - epoch_acc: 0.5939 - valid_loss: 0.6619 - valid_reg: 0.000000 - valid_acc: 0.6107 - epoch_time: 62.7173 s\n",
            "Epoch 25\n",
            "[====================] 200/200: - running_loss: 0.6677 - running_reg: 0.000000 - running_acc: 0.5863 - lr: 0.00036 - epoch_loss: 0.6661 - epoch_reg: 0.000000 - epoch_acc: 0.5941 - valid_loss: 0.6628 - valid_reg: 0.000000 - valid_acc: 0.6131 - epoch_time: 62.8312 s\n",
            "Epoch 26\n",
            "[====================] 200/200: - running_loss: 0.6677 - running_reg: 0.000000 - running_acc: 0.6039 - lr: 0.00038 - epoch_loss: 0.6689 - epoch_reg: 0.000000 - epoch_acc: 0.6005 - valid_loss: 0.6875 - valid_reg: 0.000000 - valid_acc: 0.5608 - epoch_time: 62.4818 s\n",
            "Epoch 27\n",
            "[====================] 200/200: - running_loss: 0.6851 - running_reg: 0.000000 - running_acc: 0.5584 - lr: 0.00039 - epoch_loss: 0.6831 - epoch_reg: 0.000000 - epoch_acc: 0.5625 - valid_loss: 0.6766 - valid_reg: 0.000000 - valid_acc: 0.5800 - epoch_time: 62.5473 s\n",
            "Epoch 28\n",
            "[====================] 200/200: - running_loss: 0.6681 - running_reg: 0.000000 - running_acc: 0.5995 - lr: 0.00041 - epoch_loss: 0.6689 - epoch_reg: 0.000000 - epoch_acc: 0.5994 - valid_loss: 0.6559 - valid_reg: 0.000000 - valid_acc: 0.6189 - epoch_time: 62.5965 s\n",
            "Epoch 29\n",
            "[====================] 200/200: - running_loss: 0.6570 - running_reg: 0.000000 - running_acc: 0.6169 - lr: 0.00042 - epoch_loss: 0.6588 - epoch_reg: 0.000000 - epoch_acc: 0.6137 - valid_loss: 0.6504 - valid_reg: 0.000000 - valid_acc: 0.6189 - epoch_time: 62.8432 s\n",
            "Epoch 30\n",
            "[====================] 200/200: - running_loss: 0.6560 - running_reg: 0.000000 - running_acc: 0.6169 - lr: 0.00043 - epoch_loss: 0.6564 - epoch_reg: 0.000000 - epoch_acc: 0.6164 - valid_loss: 0.6666 - valid_reg: 0.000000 - valid_acc: 0.5874 - epoch_time: 63.2285 s\n",
            "Epoch 31\n",
            "[====================] 200/200: - running_loss: 0.6582 - running_reg: 0.000000 - running_acc: 0.6080 - lr: 0.00045 - epoch_loss: 0.6553 - epoch_reg: 0.000000 - epoch_acc: 0.6133 - valid_loss: 0.6633 - valid_reg: 0.000000 - valid_acc: 0.5960 - epoch_time: 62.5780 s\n",
            "Epoch 32\n",
            "[====================] 200/200: - running_loss: 0.6516 - running_reg: 0.000000 - running_acc: 0.6122 - lr: 0.00046 - epoch_loss: 0.6536 - epoch_reg: 0.000000 - epoch_acc: 0.6098 - valid_loss: 0.6572 - valid_reg: 0.000000 - valid_acc: 0.6093 - epoch_time: 62.6987 s\n",
            "Epoch 33\n",
            "[====================] 200/200: - running_loss: 0.6625 - running_reg: 0.000000 - running_acc: 0.6009 - lr: 0.00048 - epoch_loss: 0.6582 - epoch_reg: 0.000000 - epoch_acc: 0.6075 - valid_loss: 0.6471 - valid_reg: 0.000000 - valid_acc: 0.6238 - epoch_time: 62.8013 s\n",
            "Epoch 34\n",
            "[====================] 200/200: - running_loss: 0.6572 - running_reg: 0.000000 - running_acc: 0.6102 - lr: 0.00049 - epoch_loss: 0.6518 - epoch_reg: 0.000000 - epoch_acc: 0.6162 - valid_loss: 0.6705 - valid_reg: 0.000000 - valid_acc: 0.5839 - epoch_time: 62.8140 s\n",
            "Epoch 35\n",
            "[====================] 200/200: - running_loss: 0.6503 - running_reg: 0.000000 - running_acc: 0.6239 - lr: 0.00050 - epoch_loss: 0.6486 - epoch_reg: 0.000000 - epoch_acc: 0.6253 - valid_loss: 0.6533 - valid_reg: 0.000000 - valid_acc: 0.6153 - epoch_time: 62.7500 s\n",
            "Epoch 36\n",
            "[====================] 200/200: - running_loss: 0.6568 - running_reg: 0.000000 - running_acc: 0.6138 - lr: 0.00052 - epoch_loss: 0.6519 - epoch_reg: 0.000000 - epoch_acc: 0.6158 - valid_loss: 0.6539 - valid_reg: 0.000000 - valid_acc: 0.6104 - epoch_time: 62.7813 s\n",
            "Epoch 37\n",
            "[====================] 200/200: - running_loss: 0.6518 - running_reg: 0.000000 - running_acc: 0.6220 - lr: 0.00053 - epoch_loss: 0.6497 - epoch_reg: 0.000000 - epoch_acc: 0.6219 - valid_loss: 0.6452 - valid_reg: 0.000000 - valid_acc: 0.6287 - epoch_time: 62.7915 s\n",
            "Epoch 38\n",
            "[====================] 200/200: - running_loss: 0.6614 - running_reg: 0.000000 - running_acc: 0.6087 - lr: 0.00054 - epoch_loss: 0.6636 - epoch_reg: 0.000000 - epoch_acc: 0.6033 - valid_loss: 0.6431 - valid_reg: 0.000000 - valid_acc: 0.6308 - epoch_time: 62.8772 s\n",
            "Epoch 39\n",
            "[====================] 200/200: - running_loss: 0.6476 - running_reg: 0.000000 - running_acc: 0.6197 - lr: 0.00056 - epoch_loss: 0.6444 - epoch_reg: 0.000000 - epoch_acc: 0.6277 - valid_loss: 0.6543 - valid_reg: 0.000000 - valid_acc: 0.6212 - epoch_time: 62.6846 s\n",
            "Epoch 40\n",
            "[====================] 200/200: - running_loss: 0.6441 - running_reg: 0.000000 - running_acc: 0.6281 - lr: 0.00055 - epoch_loss: 0.6490 - epoch_reg: 0.000000 - epoch_acc: 0.6219 - valid_loss: 0.6441 - valid_reg: 0.000000 - valid_acc: 0.6305 - epoch_time: 62.8658 s\n",
            "Epoch 41\n",
            "[====================] 200/200: - running_loss: 0.6395 - running_reg: 0.000000 - running_acc: 0.6333 - lr: 0.00055 - epoch_loss: 0.6410 - epoch_reg: 0.000000 - epoch_acc: 0.6327 - valid_loss: 0.6621 - valid_reg: 0.000000 - valid_acc: 0.6284 - epoch_time: 62.7733 s\n",
            "Epoch 42\n",
            "[====================] 200/200: - running_loss: 0.6342 - running_reg: 0.000000 - running_acc: 0.6448 - lr: 0.00054 - epoch_loss: 0.6324 - epoch_reg: 0.000000 - epoch_acc: 0.6458 - valid_loss: 0.6465 - valid_reg: 0.000000 - valid_acc: 0.6238 - epoch_time: 62.7207 s\n",
            "Epoch 43\n",
            "[====================] 200/200: - running_loss: 0.6465 - running_reg: 0.000000 - running_acc: 0.6164 - lr: 0.00053 - epoch_loss: 0.6436 - epoch_reg: 0.000000 - epoch_acc: 0.6241 - valid_loss: 0.6926 - valid_reg: 0.000000 - valid_acc: 0.5398 - epoch_time: 62.7353 s\n",
            "Epoch 44\n",
            "[====================] 200/200: - running_loss: 0.6380 - running_reg: 0.000000 - running_acc: 0.6383 - lr: 0.00053 - epoch_loss: 0.6400 - epoch_reg: 0.000000 - epoch_acc: 0.6344 - valid_loss: 0.6400 - valid_reg: 0.000000 - valid_acc: 0.6351 - epoch_time: 62.9896 s\n",
            "Epoch 45\n",
            "[====================] 200/200: - running_loss: 0.6266 - running_reg: 0.000000 - running_acc: 0.6445 - lr: 0.00052 - epoch_loss: 0.6292 - epoch_reg: 0.000000 - epoch_acc: 0.6436 - valid_loss: 0.6838 - valid_reg: 0.000000 - valid_acc: 0.5865 - epoch_time: 62.9195 s\n",
            "Epoch 46\n",
            "[====================] 200/200: - running_loss: 0.6300 - running_reg: 0.000000 - running_acc: 0.6486 - lr: 0.00052 - epoch_loss: 0.6269 - epoch_reg: 0.000000 - epoch_acc: 0.6506 - valid_loss: 0.6391 - valid_reg: 0.000000 - valid_acc: 0.6455 - epoch_time: 62.9130 s\n",
            "Epoch 47\n",
            "[====================] 200/200: - running_loss: 0.6363 - running_reg: 0.000000 - running_acc: 0.6360 - lr: 0.00051 - epoch_loss: 0.6384 - epoch_reg: 0.000000 - epoch_acc: 0.6328 - valid_loss: 0.6431 - valid_reg: 0.000000 - valid_acc: 0.6306 - epoch_time: 62.8130 s\n",
            "Epoch 48\n",
            "[====================] 200/200: - running_loss: 0.6298 - running_reg: 0.000000 - running_acc: 0.6474 - lr: 0.00051 - epoch_loss: 0.6308 - epoch_reg: 0.000000 - epoch_acc: 0.6467 - valid_loss: 0.6358 - valid_reg: 0.000000 - valid_acc: 0.6375 - epoch_time: 62.8421 s\n",
            "Epoch 49\n",
            "[====================] 200/200: - running_loss: 0.6296 - running_reg: 0.000000 - running_acc: 0.6503 - lr: 0.00050 - epoch_loss: 0.6290 - epoch_reg: 0.000000 - epoch_acc: 0.6463 - valid_loss: 0.6295 - valid_reg: 0.000000 - valid_acc: 0.6498 - epoch_time: 62.8762 s\n",
            "Epoch 50\n",
            "[====================] 200/200: - running_loss: 0.6212 - running_reg: 0.000000 - running_acc: 0.6528 - lr: 0.00050 - epoch_loss: 0.6233 - epoch_reg: 0.000000 - epoch_acc: 0.6514 - valid_loss: 0.6317 - valid_reg: 0.000000 - valid_acc: 0.6453 - epoch_time: 62.9242 s\n",
            "Epoch 51\n",
            "[====================] 200/200: - running_loss: 0.6290 - running_reg: 0.000000 - running_acc: 0.6503 - lr: 0.00049 - epoch_loss: 0.6280 - epoch_reg: 0.000000 - epoch_acc: 0.6520 - valid_loss: 0.6291 - valid_reg: 0.000000 - valid_acc: 0.6489 - epoch_time: 62.8143 s\n",
            "Epoch 52\n",
            "[====================] 200/200: - running_loss: 0.6238 - running_reg: 0.000000 - running_acc: 0.6494 - lr: 0.00049 - epoch_loss: 0.6228 - epoch_reg: 0.000000 - epoch_acc: 0.6505 - valid_loss: 0.6300 - valid_reg: 0.000000 - valid_acc: 0.6508 - epoch_time: 63.2223 s\n",
            "Epoch 53\n",
            "[====================] 200/200: - running_loss: 0.6305 - running_reg: 0.000000 - running_acc: 0.6396 - lr: 0.00048 - epoch_loss: 0.6243 - epoch_reg: 0.000000 - epoch_acc: 0.6464 - valid_loss: 0.6354 - valid_reg: 0.000000 - valid_acc: 0.6442 - epoch_time: 63.2757 s\n",
            "Epoch 54\n",
            "[====================] 200/200: - running_loss: 0.6168 - running_reg: 0.000000 - running_acc: 0.6559 - lr: 0.00048 - epoch_loss: 0.6142 - epoch_reg: 0.000000 - epoch_acc: 0.6603 - valid_loss: 0.6333 - valid_reg: 0.000000 - valid_acc: 0.6438 - epoch_time: 63.3724 s\n",
            "Epoch 55\n",
            "[====================] 200/200: - running_loss: 0.6283 - running_reg: 0.000000 - running_acc: 0.6426 - lr: 0.00047 - epoch_loss: 0.6260 - epoch_reg: 0.000000 - epoch_acc: 0.6459 - valid_loss: 0.6327 - valid_reg: 0.000000 - valid_acc: 0.6412 - epoch_time: 63.3158 s\n",
            "Epoch 56\n",
            "[====================] 200/200: - running_loss: 0.6140 - running_reg: 0.000000 - running_acc: 0.6625 - lr: 0.00047 - epoch_loss: 0.6159 - epoch_reg: 0.000000 - epoch_acc: 0.6581 - valid_loss: 0.6255 - valid_reg: 0.000000 - valid_acc: 0.6554 - epoch_time: 63.5667 s\n",
            "Epoch 57\n",
            "[====================] 200/200: - running_loss: 0.6147 - running_reg: 0.000000 - running_acc: 0.6591 - lr: 0.00046 - epoch_loss: 0.6165 - epoch_reg: 0.000000 - epoch_acc: 0.6591 - valid_loss: 0.6465 - valid_reg: 0.000000 - valid_acc: 0.6307 - epoch_time: 63.4071 s\n",
            "Epoch 58\n",
            "[====================] 200/200: - running_loss: 0.6136 - running_reg: 0.000000 - running_acc: 0.6590 - lr: 0.00046 - epoch_loss: 0.6165 - epoch_reg: 0.000000 - epoch_acc: 0.6580 - valid_loss: 0.6239 - valid_reg: 0.000000 - valid_acc: 0.6573 - epoch_time: 63.4995 s\n",
            "Epoch 59\n",
            "[====================] 200/200: - running_loss: 0.6103 - running_reg: 0.000000 - running_acc: 0.6618 - lr: 0.00046 - epoch_loss: 0.6116 - epoch_reg: 0.000000 - epoch_acc: 0.6597 - valid_loss: 0.6256 - valid_reg: 0.000000 - valid_acc: 0.6540 - epoch_time: 63.3871 s\n",
            "Epoch 60\n",
            "[====================] 200/200: - running_loss: 0.6183 - running_reg: 0.000000 - running_acc: 0.6556 - lr: 0.00045 - epoch_loss: 0.6197 - epoch_reg: 0.000000 - epoch_acc: 0.6539 - valid_loss: 0.6235 - valid_reg: 0.000000 - valid_acc: 0.6561 - epoch_time: 63.3721 s\n",
            "Epoch 61\n",
            "[====================] 200/200: - running_loss: 0.6148 - running_reg: 0.000000 - running_acc: 0.6612 - lr: 0.00045 - epoch_loss: 0.6130 - epoch_reg: 0.000000 - epoch_acc: 0.6617 - valid_loss: 0.6381 - valid_reg: 0.000000 - valid_acc: 0.6356 - epoch_time: 63.3215 s\n",
            "Epoch 62\n",
            "[====================] 200/200: - running_loss: 0.6143 - running_reg: 0.000000 - running_acc: 0.6570 - lr: 0.00045 - epoch_loss: 0.6130 - epoch_reg: 0.000000 - epoch_acc: 0.6594 - valid_loss: 0.6281 - valid_reg: 0.000000 - valid_acc: 0.6546 - epoch_time: 63.3792 s\n",
            "Epoch 63\n",
            "[====================] 200/200: - running_loss: 0.6111 - running_reg: 0.000000 - running_acc: 0.6681 - lr: 0.00044 - epoch_loss: 0.6113 - epoch_reg: 0.000000 - epoch_acc: 0.6648 - valid_loss: 0.6292 - valid_reg: 0.000000 - valid_acc: 0.6427 - epoch_time: 63.4076 s\n",
            "Epoch 64\n",
            "[====================] 200/200: - running_loss: 0.6132 - running_reg: 0.000000 - running_acc: 0.6533 - lr: 0.00044 - epoch_loss: 0.6137 - epoch_reg: 0.000000 - epoch_acc: 0.6545 - valid_loss: 0.6398 - valid_reg: 0.000000 - valid_acc: 0.6478 - epoch_time: 63.4180 s\n",
            "Epoch 65\n",
            "[====================] 200/200: - running_loss: 0.6059 - running_reg: 0.000000 - running_acc: 0.6689 - lr: 0.00044 - epoch_loss: 0.6076 - epoch_reg: 0.000000 - epoch_acc: 0.6686 - valid_loss: 0.6254 - valid_reg: 0.000000 - valid_acc: 0.6520 - epoch_time: 63.3882 s\n",
            "Epoch 66\n",
            "[====================] 200/200: - running_loss: 0.6028 - running_reg: 0.000000 - running_acc: 0.6749 - lr: 0.00043 - epoch_loss: 0.6015 - epoch_reg: 0.000000 - epoch_acc: 0.6780 - valid_loss: 0.6414 - valid_reg: 0.000000 - valid_acc: 0.6339 - epoch_time: 63.4116 s\n",
            "Epoch 67\n",
            "[====================] 200/200: - running_loss: 0.6168 - running_reg: 0.000000 - running_acc: 0.6588 - lr: 0.00043 - epoch_loss: 0.6168 - epoch_reg: 0.000000 - epoch_acc: 0.6562 - valid_loss: 0.6278 - valid_reg: 0.000000 - valid_acc: 0.6497 - epoch_time: 63.4111 s\n",
            "Epoch 68\n",
            "[====================] 200/200: - running_loss: 0.6076 - running_reg: 0.000000 - running_acc: 0.6613 - lr: 0.00043 - epoch_loss: 0.6085 - epoch_reg: 0.000000 - epoch_acc: 0.6636 - valid_loss: 0.6210 - valid_reg: 0.000000 - valid_acc: 0.6592 - epoch_time: 63.5330 s\n",
            "Epoch 69\n",
            "[====================] 200/200: - running_loss: 0.6159 - running_reg: 0.000000 - running_acc: 0.6599 - lr: 0.00042 - epoch_loss: 0.6088 - epoch_reg: 0.000000 - epoch_acc: 0.6662 - valid_loss: 0.6192 - valid_reg: 0.000000 - valid_acc: 0.6636 - epoch_time: 63.5163 s\n",
            "Epoch 70\n",
            "[====================] 200/200: - running_loss: 0.6048 - running_reg: 0.000000 - running_acc: 0.6727 - lr: 0.00042 - epoch_loss: 0.6099 - epoch_reg: 0.000000 - epoch_acc: 0.6681 - valid_loss: 0.6317 - valid_reg: 0.000000 - valid_acc: 0.6586 - epoch_time: 63.2924 s\n",
            "Epoch 71\n",
            "[====================] 200/200: - running_loss: 0.6110 - running_reg: 0.000000 - running_acc: 0.6656 - lr: 0.00042 - epoch_loss: 0.6133 - epoch_reg: 0.000000 - epoch_acc: 0.6659 - valid_loss: 0.6218 - valid_reg: 0.000000 - valid_acc: 0.6630 - epoch_time: 63.4118 s\n",
            "Epoch 72\n",
            "[====================] 200/200: - running_loss: 0.5983 - running_reg: 0.000000 - running_acc: 0.6781 - lr: 0.00041 - epoch_loss: 0.6035 - epoch_reg: 0.000000 - epoch_acc: 0.6744 - valid_loss: 0.6199 - valid_reg: 0.000000 - valid_acc: 0.6622 - epoch_time: 63.3850 s\n",
            "Epoch 73\n",
            "[====================] 200/200: - running_loss: 0.6050 - running_reg: 0.000000 - running_acc: 0.6736 - lr: 0.00041 - epoch_loss: 0.6076 - epoch_reg: 0.000000 - epoch_acc: 0.6687 - valid_loss: 0.6217 - valid_reg: 0.000000 - valid_acc: 0.6640 - epoch_time: 63.4672 s\n",
            "Epoch 74\n",
            "[====================] 200/200: - running_loss: 0.6055 - running_reg: 0.000000 - running_acc: 0.6682 - lr: 0.00041 - epoch_loss: 0.6113 - epoch_reg: 0.000000 - epoch_acc: 0.6606 - valid_loss: 0.6236 - valid_reg: 0.000000 - valid_acc: 0.6541 - epoch_time: 63.4326 s\n",
            "Epoch 75\n",
            "[====================] 200/200: - running_loss: 0.6108 - running_reg: 0.000000 - running_acc: 0.6626 - lr: 0.00041 - epoch_loss: 0.6072 - epoch_reg: 0.000000 - epoch_acc: 0.6664 - valid_loss: 0.6219 - valid_reg: 0.000000 - valid_acc: 0.6617 - epoch_time: 63.2949 s\n",
            "Epoch 76\n",
            "[====================] 200/200: - running_loss: 0.6008 - running_reg: 0.000000 - running_acc: 0.6735 - lr: 0.00040 - epoch_loss: 0.6012 - epoch_reg: 0.000000 - epoch_acc: 0.6744 - valid_loss: 0.6221 - valid_reg: 0.000000 - valid_acc: 0.6600 - epoch_time: 63.0715 s\n",
            "Epoch 77\n",
            "[====================] 200/200: - running_loss: 0.6045 - running_reg: 0.000000 - running_acc: 0.6666 - lr: 0.00040 - epoch_loss: 0.6041 - epoch_reg: 0.000000 - epoch_acc: 0.6706 - valid_loss: 0.6283 - valid_reg: 0.000000 - valid_acc: 0.6505 - epoch_time: 62.8479 s\n",
            "Epoch 78\n",
            "[====================] 200/200: - running_loss: 0.6002 - running_reg: 0.000000 - running_acc: 0.6782 - lr: 0.00040 - epoch_loss: 0.5973 - epoch_reg: 0.000000 - epoch_acc: 0.6781 - valid_loss: 0.6275 - valid_reg: 0.000000 - valid_acc: 0.6490 - epoch_time: 62.8296 s\n",
            "Epoch 79\n",
            "[====================] 200/200: - running_loss: 0.5991 - running_reg: 0.000000 - running_acc: 0.6729 - lr: 0.00040 - epoch_loss: 0.6001 - epoch_reg: 0.000000 - epoch_acc: 0.6747 - valid_loss: 0.6462 - valid_reg: 0.000000 - valid_acc: 0.6284 - epoch_time: 62.9448 s\n",
            "Epoch 80\n",
            "[====================] 200/200: - running_loss: 0.6023 - running_reg: 0.000000 - running_acc: 0.6769 - lr: 0.00039 - epoch_loss: 0.6009 - epoch_reg: 0.000000 - epoch_acc: 0.6767 - valid_loss: 0.6201 - valid_reg: 0.000000 - valid_acc: 0.6584 - epoch_time: 62.8169 s\n",
            "Epoch 81\n",
            "[====================] 200/200: - running_loss: 0.6013 - running_reg: 0.000000 - running_acc: 0.6772 - lr: 0.00039 - epoch_loss: 0.5996 - epoch_reg: 0.000000 - epoch_acc: 0.6742 - valid_loss: 0.6181 - valid_reg: 0.000000 - valid_acc: 0.6648 - epoch_time: 63.1850 s\n",
            "Epoch 82\n",
            "[====================] 200/200: - running_loss: 0.5951 - running_reg: 0.000000 - running_acc: 0.6804 - lr: 0.00039 - epoch_loss: 0.5935 - epoch_reg: 0.000000 - epoch_acc: 0.6798 - valid_loss: 0.6189 - valid_reg: 0.000000 - valid_acc: 0.6603 - epoch_time: 63.0853 s\n",
            "Epoch 83\n",
            "[====================] 200/200: - running_loss: 0.6017 - running_reg: 0.000000 - running_acc: 0.6734 - lr: 0.00039 - epoch_loss: 0.5997 - epoch_reg: 0.000000 - epoch_acc: 0.6767 - valid_loss: 0.6212 - valid_reg: 0.000000 - valid_acc: 0.6593 - epoch_time: 63.1808 s\n",
            "Epoch 84\n",
            "[====================] 200/200: - running_loss: 0.5976 - running_reg: 0.000000 - running_acc: 0.6837 - lr: 0.00038 - epoch_loss: 0.5951 - epoch_reg: 0.000000 - epoch_acc: 0.6836 - valid_loss: 0.6196 - valid_reg: 0.000000 - valid_acc: 0.6577 - epoch_time: 63.1119 s\n",
            "Epoch 85\n",
            "[====================] 200/200: - running_loss: 0.5964 - running_reg: 0.000000 - running_acc: 0.6844 - lr: 0.00038 - epoch_loss: 0.5965 - epoch_reg: 0.000000 - epoch_acc: 0.6839 - valid_loss: 0.6397 - valid_reg: 0.000000 - valid_acc: 0.6467 - epoch_time: 63.1134 s\n",
            "Epoch 86\n",
            "[====================] 200/200: - running_loss: 0.5931 - running_reg: 0.000000 - running_acc: 0.6814 - lr: 0.00038 - epoch_loss: 0.5931 - epoch_reg: 0.000000 - epoch_acc: 0.6795 - valid_loss: 0.6191 - valid_reg: 0.000000 - valid_acc: 0.6618 - epoch_time: 63.0547 s\n",
            "Epoch 87\n",
            "[====================] 200/200: - running_loss: 0.5881 - running_reg: 0.000000 - running_acc: 0.6878 - lr: 0.00038 - epoch_loss: 0.5925 - epoch_reg: 0.000000 - epoch_acc: 0.6819 - valid_loss: 0.6299 - valid_reg: 0.000000 - valid_acc: 0.6566 - epoch_time: 63.1094 s\n",
            "Epoch 88\n",
            "[====================] 200/200: - running_loss: 0.5921 - running_reg: 0.000000 - running_acc: 0.6812 - lr: 0.00037 - epoch_loss: 0.5949 - epoch_reg: 0.000000 - epoch_acc: 0.6791 - valid_loss: 0.6415 - valid_reg: 0.000000 - valid_acc: 0.6465 - epoch_time: 63.1286 s\n",
            "Epoch 89\n",
            "[====================] 200/200: - running_loss: 0.6004 - running_reg: 0.000000 - running_acc: 0.6705 - lr: 0.00037 - epoch_loss: 0.5984 - epoch_reg: 0.000000 - epoch_acc: 0.6748 - valid_loss: 0.6261 - valid_reg: 0.000000 - valid_acc: 0.6485 - epoch_time: 62.9623 s\n",
            "Epoch 90\n",
            "[====================] 200/200: - running_loss: 0.5915 - running_reg: 0.000000 - running_acc: 0.6747 - lr: 0.00037 - epoch_loss: 0.5877 - epoch_reg: 0.000000 - epoch_acc: 0.6825 - valid_loss: 0.6203 - valid_reg: 0.000000 - valid_acc: 0.6628 - epoch_time: 62.9345 s\n",
            "Epoch 91\n",
            "[====================] 200/200: - running_loss: 0.5905 - running_reg: 0.000000 - running_acc: 0.6842 - lr: 0.00037 - epoch_loss: 0.5886 - epoch_reg: 0.000000 - epoch_acc: 0.6881 - valid_loss: 0.6407 - valid_reg: 0.000000 - valid_acc: 0.6467 - epoch_time: 62.7915 s\n",
            "Epoch 92\n",
            "[====================] 200/200: - running_loss: 0.5886 - running_reg: 0.000000 - running_acc: 0.6850 - lr: 0.00037 - epoch_loss: 0.5920 - epoch_reg: 0.000000 - epoch_acc: 0.6825 - valid_loss: 0.6614 - valid_reg: 0.000000 - valid_acc: 0.6509 - epoch_time: 62.9414 s\n",
            "Epoch 93\n",
            "[====================] 200/200: - running_loss: 0.5869 - running_reg: 0.000000 - running_acc: 0.6880 - lr: 0.00036 - epoch_loss: 0.5906 - epoch_reg: 0.000000 - epoch_acc: 0.6861 - valid_loss: 0.6471 - valid_reg: 0.000000 - valid_acc: 0.6332 - epoch_time: 62.8280 s\n",
            "Epoch 94\n",
            "[====================] 200/200: - running_loss: 0.5897 - running_reg: 0.000000 - running_acc: 0.6929 - lr: 0.00036 - epoch_loss: 0.5896 - epoch_reg: 0.000000 - epoch_acc: 0.6872 - valid_loss: 0.6167 - valid_reg: 0.000000 - valid_acc: 0.6636 - epoch_time: 62.9231 s\n",
            "Epoch 95\n",
            "[====================] 200/200: - running_loss: 0.5916 - running_reg: 0.000000 - running_acc: 0.6851 - lr: 0.00036 - epoch_loss: 0.5875 - epoch_reg: 0.000000 - epoch_acc: 0.6886 - valid_loss: 0.6187 - valid_reg: 0.000000 - valid_acc: 0.6647 - epoch_time: 62.8431 s\n",
            "Epoch 96\n",
            "[====================] 200/200: - running_loss: 0.5897 - running_reg: 0.000000 - running_acc: 0.6854 - lr: 0.00036 - epoch_loss: 0.5872 - epoch_reg: 0.000000 - epoch_acc: 0.6867 - valid_loss: 0.6206 - valid_reg: 0.000000 - valid_acc: 0.6656 - epoch_time: 62.9016 s\n",
            "Epoch 97\n",
            "[====================] 200/200: - running_loss: 0.5908 - running_reg: 0.000000 - running_acc: 0.6867 - lr: 0.00036 - epoch_loss: 0.5870 - epoch_reg: 0.000000 - epoch_acc: 0.6906 - valid_loss: 0.6193 - valid_reg: 0.000000 - valid_acc: 0.6618 - epoch_time: 62.0748 s\n",
            "Epoch 98\n",
            "[====================] 200/200: - running_loss: 0.5856 - running_reg: 0.000000 - running_acc: 0.6901 - lr: 0.00036 - epoch_loss: 0.5846 - epoch_reg: 0.000000 - epoch_acc: 0.6925 - valid_loss: 0.6203 - valid_reg: 0.000000 - valid_acc: 0.6611 - epoch_time: 59.6114 s\n",
            "Epoch 99\n",
            "[====================] 200/200: - running_loss: 0.5849 - running_reg: 0.000000 - running_acc: 0.6910 - lr: 0.00035 - epoch_loss: 0.5863 - epoch_reg: 0.000000 - epoch_acc: 0.6897 - valid_loss: 0.6207 - valid_reg: 0.000000 - valid_acc: 0.6600 - epoch_time: 59.8682 s\n",
            " - test_loss: 0.6206 - test_reg: 0.000000 - test_acc: 0.6656 - test_time: 34.1265 s\n",
            "Original model 3464194 params, new model 3464194 params, ratio 1.0\n",
            "Epoch 0\n",
            "[====================] 200/200: - running_loss: 0.6941 - running_reg: 0.000000 - running_acc: 0.5196 - lr: 0.00001 - epoch_loss: 0.6974 - epoch_reg: 0.000000 - epoch_acc: 0.5067 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 39.2484 s\n",
            "Epoch 1\n",
            "[====================] 200/200: - running_loss: 0.6949 - running_reg: 0.000000 - running_acc: 0.5184 - lr: 0.00003 - epoch_loss: 0.6938 - epoch_reg: 0.000000 - epoch_acc: 0.5194 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 28.5300 s\n",
            "Epoch 2\n",
            "[====================] 200/200: - running_loss: 0.6893 - running_reg: 0.000000 - running_acc: 0.5342 - lr: 0.00004 - epoch_loss: 0.6913 - epoch_reg: 0.000000 - epoch_acc: 0.5297 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 28.4774 s\n",
            "Epoch 3\n",
            "[====================] 200/200: - running_loss: 0.6879 - running_reg: 0.000000 - running_acc: 0.5448 - lr: 0.00006 - epoch_loss: 0.6876 - epoch_reg: 0.000000 - epoch_acc: 0.5456 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 28.5041 s\n",
            "Epoch 4\n",
            "[====================] 200/200: - running_loss: 0.6863 - running_reg: 0.000000 - running_acc: 0.5533 - lr: 0.00007 - epoch_loss: 0.6873 - epoch_reg: 0.000000 - epoch_acc: 0.5489 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 28.5777 s\n",
            "Epoch 5\n",
            "[====================] 200/200: - running_loss: 0.6804 - running_reg: 0.000000 - running_acc: 0.5705 - lr: 0.00008 - epoch_loss: 0.6820 - epoch_reg: 0.000000 - epoch_acc: 0.5633 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 28.5700 s\n",
            "Epoch 6\n",
            "[====================] 200/200: - running_loss: 0.6812 - running_reg: 0.000000 - running_acc: 0.5566 - lr: 0.00010 - epoch_loss: 0.6808 - epoch_reg: 0.000000 - epoch_acc: 0.5586 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 28.5665 s\n",
            "Epoch 7\n",
            "[====================] 200/200: - running_loss: 0.6826 - running_reg: 0.000000 - running_acc: 0.5588 - lr: 0.00011 - epoch_loss: 0.6816 - epoch_reg: 0.000000 - epoch_acc: 0.5633 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 28.4749 s\n",
            "Epoch 8\n",
            "[====================] 200/200: - running_loss: 0.6768 - running_reg: 0.000000 - running_acc: 0.5778 - lr: 0.00013 - epoch_loss: 0.6773 - epoch_reg: 0.000000 - epoch_acc: 0.5759 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 28.5549 s\n",
            "Epoch 9\n",
            "[====================] 200/200: - running_loss: 0.6800 - running_reg: 0.000000 - running_acc: 0.5553 - lr: 0.00014 - epoch_loss: 0.6804 - epoch_reg: 0.000000 - epoch_acc: 0.5573 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 28.6290 s\n",
            "Epoch 10\n",
            "[====================] 200/200: - running_loss: 0.6830 - running_reg: 0.000000 - running_acc: 0.5614 - lr: 0.00015 - epoch_loss: 0.6821 - epoch_reg: 0.000000 - epoch_acc: 0.5619 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 28.7028 s\n",
            "Epoch 11\n",
            "[====================] 200/200: - running_loss: 0.6804 - running_reg: 0.000000 - running_acc: 0.5669 - lr: 0.00017 - epoch_loss: 0.6799 - epoch_reg: 0.000000 - epoch_acc: 0.5673 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 28.5639 s\n",
            "Epoch 12\n",
            "[====================] 200/200: - running_loss: 0.6841 - running_reg: 0.000000 - running_acc: 0.5606 - lr: 0.00018 - epoch_loss: 0.6857 - epoch_reg: 0.000000 - epoch_acc: 0.5580 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 28.7272 s\n",
            "Epoch 13\n",
            "[====================] 200/200: - running_loss: 0.6842 - running_reg: 0.000000 - running_acc: 0.5550 - lr: 0.00020 - epoch_loss: 0.6829 - epoch_reg: 0.000000 - epoch_acc: 0.5617 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 28.7689 s\n",
            "Epoch 14\n",
            "[====================] 200/200: - running_loss: 0.6776 - running_reg: 0.000000 - running_acc: 0.5750 - lr: 0.00021 - epoch_loss: 0.6804 - epoch_reg: 0.000000 - epoch_acc: 0.5697 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 28.6916 s\n",
            "Epoch 15\n",
            "[====================] 200/200: - running_loss: 0.6781 - running_reg: 0.000000 - running_acc: 0.5737 - lr: 0.00022 - epoch_loss: 0.6815 - epoch_reg: 0.000000 - epoch_acc: 0.5653 - valid_loss: 0.7111 - valid_reg: 0.000000 - valid_acc: 0.5233 - epoch_time: 63.2576 s\n",
            "Epoch 16\n",
            "[====================] 200/200: - running_loss: 0.6738 - running_reg: 0.000000 - running_acc: 0.5798 - lr: 0.00024 - epoch_loss: 0.6753 - epoch_reg: 0.000000 - epoch_acc: 0.5800 - valid_loss: 0.6684 - valid_reg: 0.000000 - valid_acc: 0.6010 - epoch_time: 63.2212 s\n",
            "Epoch 17\n",
            "[====================] 200/200: - running_loss: 0.6721 - running_reg: 0.000000 - running_acc: 0.5779 - lr: 0.00025 - epoch_loss: 0.6752 - epoch_reg: 0.000000 - epoch_acc: 0.5739 - valid_loss: 0.6812 - valid_reg: 0.000000 - valid_acc: 0.5878 - epoch_time: 62.6852 s\n",
            "Epoch 18\n",
            "[====================] 200/200: - running_loss: 0.6812 - running_reg: 0.000000 - running_acc: 0.5574 - lr: 0.00027 - epoch_loss: 0.6822 - epoch_reg: 0.000000 - epoch_acc: 0.5569 - valid_loss: 0.6754 - valid_reg: 0.000000 - valid_acc: 0.5789 - epoch_time: 62.6544 s\n",
            "Epoch 19\n",
            "[====================] 200/200: - running_loss: 0.6736 - running_reg: 0.000000 - running_acc: 0.5718 - lr: 0.00028 - epoch_loss: 0.6724 - epoch_reg: 0.000000 - epoch_acc: 0.5738 - valid_loss: 0.6828 - valid_reg: 0.000000 - valid_acc: 0.5636 - epoch_time: 62.7767 s\n",
            "Epoch 20\n",
            "[====================] 200/200: - running_loss: 0.6766 - running_reg: 0.000000 - running_acc: 0.5717 - lr: 0.00029 - epoch_loss: 0.6732 - epoch_reg: 0.000000 - epoch_acc: 0.5802 - valid_loss: 0.6701 - valid_reg: 0.000000 - valid_acc: 0.5830 - epoch_time: 62.6748 s\n",
            "Epoch 21\n",
            "[====================] 200/200: - running_loss: 0.6817 - running_reg: 0.000000 - running_acc: 0.5628 - lr: 0.00031 - epoch_loss: 0.6756 - epoch_reg: 0.000000 - epoch_acc: 0.5766 - valid_loss: 0.6749 - valid_reg: 0.000000 - valid_acc: 0.5742 - epoch_time: 63.0283 s\n",
            "Epoch 22\n",
            "[====================] 200/200: - running_loss: 0.6773 - running_reg: 0.000000 - running_acc: 0.5803 - lr: 0.00032 - epoch_loss: 0.6789 - epoch_reg: 0.000000 - epoch_acc: 0.5770 - valid_loss: 0.6751 - valid_reg: 0.000000 - valid_acc: 0.5739 - epoch_time: 62.9245 s\n",
            "Epoch 23\n",
            "[====================] 200/200: - running_loss: 0.6699 - running_reg: 0.000000 - running_acc: 0.5908 - lr: 0.00034 - epoch_loss: 0.6688 - epoch_reg: 0.000000 - epoch_acc: 0.5908 - valid_loss: 0.6808 - valid_reg: 0.000000 - valid_acc: 0.5627 - epoch_time: 63.0470 s\n",
            "Epoch 24\n",
            "[====================] 200/200: - running_loss: 0.6668 - running_reg: 0.000000 - running_acc: 0.5920 - lr: 0.00035 - epoch_loss: 0.6675 - epoch_reg: 0.000000 - epoch_acc: 0.5913 - valid_loss: 0.6711 - valid_reg: 0.000000 - valid_acc: 0.5841 - epoch_time: 62.9810 s\n",
            "Epoch 25\n",
            "[====================] 200/200: - running_loss: 0.6658 - running_reg: 0.000000 - running_acc: 0.5949 - lr: 0.00036 - epoch_loss: 0.6630 - epoch_reg: 0.000000 - epoch_acc: 0.6011 - valid_loss: 0.6671 - valid_reg: 0.000000 - valid_acc: 0.6067 - epoch_time: 63.0702 s\n",
            "Epoch 26\n",
            "[====================] 200/200: - running_loss: 0.6594 - running_reg: 0.000000 - running_acc: 0.6096 - lr: 0.00038 - epoch_loss: 0.6638 - epoch_reg: 0.000000 - epoch_acc: 0.6022 - valid_loss: 0.6561 - valid_reg: 0.000000 - valid_acc: 0.6073 - epoch_time: 63.0689 s\n",
            "Epoch 27\n",
            "[====================] 200/200: - running_loss: 0.6586 - running_reg: 0.000000 - running_acc: 0.6085 - lr: 0.00039 - epoch_loss: 0.6597 - epoch_reg: 0.000000 - epoch_acc: 0.6091 - valid_loss: 0.6763 - valid_reg: 0.000000 - valid_acc: 0.5730 - epoch_time: 63.0004 s\n",
            "Epoch 28\n",
            "[====================] 200/200: - running_loss: 0.6545 - running_reg: 0.000000 - running_acc: 0.6151 - lr: 0.00041 - epoch_loss: 0.6567 - epoch_reg: 0.000000 - epoch_acc: 0.6100 - valid_loss: 0.6976 - valid_reg: 0.000000 - valid_acc: 0.5800 - epoch_time: 63.0744 s\n",
            "Epoch 29\n",
            "[====================] 200/200: - running_loss: 0.6598 - running_reg: 0.000000 - running_acc: 0.6081 - lr: 0.00042 - epoch_loss: 0.6567 - epoch_reg: 0.000000 - epoch_acc: 0.6109 - valid_loss: 0.6595 - valid_reg: 0.000000 - valid_acc: 0.6063 - epoch_time: 63.0261 s\n",
            "Epoch 30\n",
            "[====================] 200/200: - running_loss: 0.6764 - running_reg: 0.000000 - running_acc: 0.5810 - lr: 0.00043 - epoch_loss: 0.6747 - epoch_reg: 0.000000 - epoch_acc: 0.5850 - valid_loss: 0.6624 - valid_reg: 0.000000 - valid_acc: 0.6074 - epoch_time: 63.0240 s\n",
            "Epoch 31\n",
            "[====================] 200/200: - running_loss: 0.6552 - running_reg: 0.000000 - running_acc: 0.6131 - lr: 0.00045 - epoch_loss: 0.6576 - epoch_reg: 0.000000 - epoch_acc: 0.6089 - valid_loss: 0.6559 - valid_reg: 0.000000 - valid_acc: 0.6109 - epoch_time: 63.1606 s\n",
            "Epoch 32\n",
            "[====================] 200/200: - running_loss: 0.6605 - running_reg: 0.000000 - running_acc: 0.6012 - lr: 0.00046 - epoch_loss: 0.6578 - epoch_reg: 0.000000 - epoch_acc: 0.6036 - valid_loss: 0.6611 - valid_reg: 0.000000 - valid_acc: 0.6018 - epoch_time: 63.0136 s\n",
            "Epoch 33\n",
            "[====================] 200/200: - running_loss: 0.6562 - running_reg: 0.000000 - running_acc: 0.6220 - lr: 0.00048 - epoch_loss: 0.6565 - epoch_reg: 0.000000 - epoch_acc: 0.6172 - valid_loss: 0.6566 - valid_reg: 0.000000 - valid_acc: 0.6078 - epoch_time: 63.0105 s\n",
            "Epoch 34\n",
            "[====================] 200/200: - running_loss: 0.6561 - running_reg: 0.000000 - running_acc: 0.6068 - lr: 0.00049 - epoch_loss: 0.6577 - epoch_reg: 0.000000 - epoch_acc: 0.6050 - valid_loss: 0.6530 - valid_reg: 0.000000 - valid_acc: 0.6146 - epoch_time: 63.1189 s\n",
            "Epoch 35\n",
            "[====================] 200/200: - running_loss: 0.6538 - running_reg: 0.000000 - running_acc: 0.6139 - lr: 0.00050 - epoch_loss: 0.6565 - epoch_reg: 0.000000 - epoch_acc: 0.6072 - valid_loss: 0.6923 - valid_reg: 0.000000 - valid_acc: 0.5774 - epoch_time: 62.7991 s\n",
            "Epoch 36\n",
            "[====================] 200/200: - running_loss: 0.6578 - running_reg: 0.000000 - running_acc: 0.6080 - lr: 0.00052 - epoch_loss: 0.6652 - epoch_reg: 0.000000 - epoch_acc: 0.5997 - valid_loss: 0.6534 - valid_reg: 0.000000 - valid_acc: 0.6158 - epoch_time: 63.0237 s\n",
            "Epoch 37\n",
            "[====================] 200/200: - running_loss: 0.6528 - running_reg: 0.000000 - running_acc: 0.6071 - lr: 0.00053 - epoch_loss: 0.6489 - epoch_reg: 0.000000 - epoch_acc: 0.6177 - valid_loss: 0.6602 - valid_reg: 0.000000 - valid_acc: 0.6052 - epoch_time: 62.7659 s\n",
            "Epoch 38\n",
            "[====================] 200/200: - running_loss: 0.6549 - running_reg: 0.000000 - running_acc: 0.6144 - lr: 0.00054 - epoch_loss: 0.6524 - epoch_reg: 0.000000 - epoch_acc: 0.6164 - valid_loss: 0.7094 - valid_reg: 0.000000 - valid_acc: 0.5353 - epoch_time: 62.9960 s\n",
            "Epoch 39\n",
            "[====================] 200/200: - running_loss: 0.6561 - running_reg: 0.000000 - running_acc: 0.6110 - lr: 0.00056 - epoch_loss: 0.6530 - epoch_reg: 0.000000 - epoch_acc: 0.6148 - valid_loss: 0.6834 - valid_reg: 0.000000 - valid_acc: 0.5751 - epoch_time: 63.0189 s\n",
            "Epoch 40\n",
            "[====================] 200/200: - running_loss: 0.6510 - running_reg: 0.000000 - running_acc: 0.6167 - lr: 0.00055 - epoch_loss: 0.6499 - epoch_reg: 0.000000 - epoch_acc: 0.6231 - valid_loss: 0.6513 - valid_reg: 0.000000 - valid_acc: 0.6195 - epoch_time: 63.1081 s\n",
            "Epoch 41\n",
            "[====================] 200/200: - running_loss: 0.6410 - running_reg: 0.000000 - running_acc: 0.6353 - lr: 0.00055 - epoch_loss: 0.6391 - epoch_reg: 0.000000 - epoch_acc: 0.6355 - valid_loss: 0.6499 - valid_reg: 0.000000 - valid_acc: 0.6220 - epoch_time: 63.2203 s\n",
            "Epoch 42\n",
            "[====================] 200/200: - running_loss: 0.6430 - running_reg: 0.000000 - running_acc: 0.6232 - lr: 0.00054 - epoch_loss: 0.6413 - epoch_reg: 0.000000 - epoch_acc: 0.6273 - valid_loss: 0.6421 - valid_reg: 0.000000 - valid_acc: 0.6329 - epoch_time: 63.1413 s\n",
            "Epoch 43\n",
            "[====================] 200/200: - running_loss: 0.6403 - running_reg: 0.000000 - running_acc: 0.6325 - lr: 0.00053 - epoch_loss: 0.6403 - epoch_reg: 0.000000 - epoch_acc: 0.6311 - valid_loss: 0.6551 - valid_reg: 0.000000 - valid_acc: 0.6172 - epoch_time: 63.0724 s\n",
            "Epoch 44\n",
            "[====================] 200/200: - running_loss: 0.6343 - running_reg: 0.000000 - running_acc: 0.6421 - lr: 0.00053 - epoch_loss: 0.6360 - epoch_reg: 0.000000 - epoch_acc: 0.6398 - valid_loss: 0.6473 - valid_reg: 0.000000 - valid_acc: 0.6302 - epoch_time: 63.0031 s\n",
            "Epoch 45\n",
            "[====================] 200/200: - running_loss: 0.6376 - running_reg: 0.000000 - running_acc: 0.6402 - lr: 0.00052 - epoch_loss: 0.6405 - epoch_reg: 0.000000 - epoch_acc: 0.6361 - valid_loss: 0.6635 - valid_reg: 0.000000 - valid_acc: 0.6153 - epoch_time: 63.1016 s\n",
            "Epoch 46\n",
            "[====================] 200/200: - running_loss: 0.6341 - running_reg: 0.000000 - running_acc: 0.6421 - lr: 0.00052 - epoch_loss: 0.6320 - epoch_reg: 0.000000 - epoch_acc: 0.6455 - valid_loss: 0.6434 - valid_reg: 0.000000 - valid_acc: 0.6239 - epoch_time: 62.9459 s\n",
            "Epoch 47\n",
            "[====================] 200/200: - running_loss: 0.6295 - running_reg: 0.000000 - running_acc: 0.6396 - lr: 0.00051 - epoch_loss: 0.6341 - epoch_reg: 0.000000 - epoch_acc: 0.6336 - valid_loss: 0.6550 - valid_reg: 0.000000 - valid_acc: 0.6099 - epoch_time: 63.0301 s\n",
            "Epoch 48\n",
            "[====================] 200/200: - running_loss: 0.6257 - running_reg: 0.000000 - running_acc: 0.6476 - lr: 0.00051 - epoch_loss: 0.6291 - epoch_reg: 0.000000 - epoch_acc: 0.6406 - valid_loss: 0.6377 - valid_reg: 0.000000 - valid_acc: 0.6318 - epoch_time: 63.0541 s\n",
            "Epoch 49\n",
            "[====================] 200/200: - running_loss: 0.6192 - running_reg: 0.000000 - running_acc: 0.6601 - lr: 0.00050 - epoch_loss: 0.6244 - epoch_reg: 0.000000 - epoch_acc: 0.6525 - valid_loss: 0.6698 - valid_reg: 0.000000 - valid_acc: 0.6122 - epoch_time: 63.0158 s\n",
            "Epoch 50\n",
            "[====================] 200/200: - running_loss: 0.6354 - running_reg: 0.000000 - running_acc: 0.6356 - lr: 0.00050 - epoch_loss: 0.6384 - epoch_reg: 0.000000 - epoch_acc: 0.6323 - valid_loss: 0.6643 - valid_reg: 0.000000 - valid_acc: 0.5929 - epoch_time: 63.0579 s\n",
            "Epoch 51\n",
            "[====================] 200/200: - running_loss: 0.6283 - running_reg: 0.000000 - running_acc: 0.6461 - lr: 0.00049 - epoch_loss: 0.6280 - epoch_reg: 0.000000 - epoch_acc: 0.6447 - valid_loss: 0.6521 - valid_reg: 0.000000 - valid_acc: 0.6091 - epoch_time: 63.1160 s\n",
            "Epoch 52\n",
            "[====================] 200/200: - running_loss: 0.6253 - running_reg: 0.000000 - running_acc: 0.6475 - lr: 0.00049 - epoch_loss: 0.6198 - epoch_reg: 0.000000 - epoch_acc: 0.6516 - valid_loss: 0.6337 - valid_reg: 0.000000 - valid_acc: 0.6439 - epoch_time: 63.2283 s\n",
            "Epoch 53\n",
            "[====================] 200/200: - running_loss: 0.6215 - running_reg: 0.000000 - running_acc: 0.6541 - lr: 0.00048 - epoch_loss: 0.6213 - epoch_reg: 0.000000 - epoch_acc: 0.6531 - valid_loss: 0.6371 - valid_reg: 0.000000 - valid_acc: 0.6330 - epoch_time: 62.8523 s\n",
            "Epoch 54\n",
            "[====================] 200/200: - running_loss: 0.6173 - running_reg: 0.000000 - running_acc: 0.6539 - lr: 0.00048 - epoch_loss: 0.6190 - epoch_reg: 0.000000 - epoch_acc: 0.6544 - valid_loss: 0.6256 - valid_reg: 0.000000 - valid_acc: 0.6526 - epoch_time: 62.8978 s\n",
            "Epoch 55\n",
            "[====================] 200/200: - running_loss: 0.6250 - running_reg: 0.000000 - running_acc: 0.6503 - lr: 0.00047 - epoch_loss: 0.6222 - epoch_reg: 0.000000 - epoch_acc: 0.6522 - valid_loss: 0.6307 - valid_reg: 0.000000 - valid_acc: 0.6471 - epoch_time: 62.8319 s\n",
            "Epoch 56\n",
            "[====================] 200/200: - running_loss: 0.6165 - running_reg: 0.000000 - running_acc: 0.6588 - lr: 0.00047 - epoch_loss: 0.6165 - epoch_reg: 0.000000 - epoch_acc: 0.6609 - valid_loss: 0.6247 - valid_reg: 0.000000 - valid_acc: 0.6542 - epoch_time: 62.8462 s\n",
            "Epoch 57\n",
            "[====================] 200/200: - running_loss: 0.6273 - running_reg: 0.000000 - running_acc: 0.6510 - lr: 0.00046 - epoch_loss: 0.6260 - epoch_reg: 0.000000 - epoch_acc: 0.6531 - valid_loss: 0.6253 - valid_reg: 0.000000 - valid_acc: 0.6527 - epoch_time: 62.8209 s\n",
            "Epoch 58\n",
            "[====================] 200/200: - running_loss: 0.6157 - running_reg: 0.000000 - running_acc: 0.6585 - lr: 0.00046 - epoch_loss: 0.6161 - epoch_reg: 0.000000 - epoch_acc: 0.6586 - valid_loss: 0.6526 - valid_reg: 0.000000 - valid_acc: 0.6119 - epoch_time: 62.8256 s\n",
            "Epoch 59\n",
            "[====================] 200/200: - running_loss: 0.6172 - running_reg: 0.000000 - running_acc: 0.6599 - lr: 0.00046 - epoch_loss: 0.6138 - epoch_reg: 0.000000 - epoch_acc: 0.6637 - valid_loss: 0.6274 - valid_reg: 0.000000 - valid_acc: 0.6488 - epoch_time: 62.8581 s\n",
            "Epoch 60\n",
            "[====================] 200/200: - running_loss: 0.6124 - running_reg: 0.000000 - running_acc: 0.6658 - lr: 0.00045 - epoch_loss: 0.6134 - epoch_reg: 0.000000 - epoch_acc: 0.6662 - valid_loss: 0.6325 - valid_reg: 0.000000 - valid_acc: 0.6547 - epoch_time: 63.0002 s\n",
            "Epoch 61\n",
            "[====================] 200/200: - running_loss: 0.6135 - running_reg: 0.000000 - running_acc: 0.6627 - lr: 0.00045 - epoch_loss: 0.6114 - epoch_reg: 0.000000 - epoch_acc: 0.6619 - valid_loss: 0.6230 - valid_reg: 0.000000 - valid_acc: 0.6572 - epoch_time: 62.9768 s\n",
            "Epoch 62\n",
            "[====================] 200/200: - running_loss: 0.6205 - running_reg: 0.000000 - running_acc: 0.6545 - lr: 0.00045 - epoch_loss: 0.6192 - epoch_reg: 0.000000 - epoch_acc: 0.6523 - valid_loss: 0.6230 - valid_reg: 0.000000 - valid_acc: 0.6562 - epoch_time: 62.7449 s\n",
            "Epoch 63\n",
            "[====================] 200/200: - running_loss: 0.6076 - running_reg: 0.000000 - running_acc: 0.6708 - lr: 0.00044 - epoch_loss: 0.6086 - epoch_reg: 0.000000 - epoch_acc: 0.6673 - valid_loss: 0.6340 - valid_reg: 0.000000 - valid_acc: 0.6482 - epoch_time: 62.8879 s\n",
            "Epoch 64\n",
            "[====================] 200/200: - running_loss: 0.6148 - running_reg: 0.000000 - running_acc: 0.6607 - lr: 0.00044 - epoch_loss: 0.6138 - epoch_reg: 0.000000 - epoch_acc: 0.6606 - valid_loss: 0.6343 - valid_reg: 0.000000 - valid_acc: 0.6557 - epoch_time: 62.8013 s\n",
            "Epoch 65\n",
            "[====================] 200/200: - running_loss: 0.6093 - running_reg: 0.000000 - running_acc: 0.6611 - lr: 0.00044 - epoch_loss: 0.6070 - epoch_reg: 0.000000 - epoch_acc: 0.6648 - valid_loss: 0.6274 - valid_reg: 0.000000 - valid_acc: 0.6550 - epoch_time: 62.7770 s\n",
            "Epoch 66\n",
            "[====================] 200/200: - running_loss: 0.6092 - running_reg: 0.000000 - running_acc: 0.6688 - lr: 0.00043 - epoch_loss: 0.6112 - epoch_reg: 0.000000 - epoch_acc: 0.6670 - valid_loss: 0.6226 - valid_reg: 0.000000 - valid_acc: 0.6577 - epoch_time: 62.8764 s\n",
            "Epoch 67\n",
            "[====================] 200/200: - running_loss: 0.6153 - running_reg: 0.000000 - running_acc: 0.6587 - lr: 0.00043 - epoch_loss: 0.6119 - epoch_reg: 0.000000 - epoch_acc: 0.6656 - valid_loss: 0.6308 - valid_reg: 0.000000 - valid_acc: 0.6566 - epoch_time: 62.7651 s\n",
            "Epoch 68\n",
            "[====================] 200/200: - running_loss: 0.6124 - running_reg: 0.000000 - running_acc: 0.6599 - lr: 0.00043 - epoch_loss: 0.6143 - epoch_reg: 0.000000 - epoch_acc: 0.6612 - valid_loss: 0.6229 - valid_reg: 0.000000 - valid_acc: 0.6562 - epoch_time: 62.7374 s\n",
            "Epoch 69\n",
            "[====================] 200/200: - running_loss: 0.6101 - running_reg: 0.000000 - running_acc: 0.6644 - lr: 0.00042 - epoch_loss: 0.6072 - epoch_reg: 0.000000 - epoch_acc: 0.6683 - valid_loss: 0.6246 - valid_reg: 0.000000 - valid_acc: 0.6556 - epoch_time: 63.0054 s\n",
            "Epoch 70\n",
            "[====================] 200/200: - running_loss: 0.6059 - running_reg: 0.000000 - running_acc: 0.6677 - lr: 0.00042 - epoch_loss: 0.6041 - epoch_reg: 0.000000 - epoch_acc: 0.6700 - valid_loss: 0.6239 - valid_reg: 0.000000 - valid_acc: 0.6559 - epoch_time: 62.9061 s\n",
            "Epoch 71\n",
            "[====================] 200/200: - running_loss: 0.6084 - running_reg: 0.000000 - running_acc: 0.6709 - lr: 0.00042 - epoch_loss: 0.6096 - epoch_reg: 0.000000 - epoch_acc: 0.6692 - valid_loss: 0.6254 - valid_reg: 0.000000 - valid_acc: 0.6537 - epoch_time: 62.8946 s\n",
            "Epoch 72\n",
            "[====================] 200/200: - running_loss: 0.6017 - running_reg: 0.000000 - running_acc: 0.6737 - lr: 0.00041 - epoch_loss: 0.6023 - epoch_reg: 0.000000 - epoch_acc: 0.6750 - valid_loss: 0.6215 - valid_reg: 0.000000 - valid_acc: 0.6574 - epoch_time: 62.7689 s\n",
            "Epoch 73\n",
            "[====================] 200/200: - running_loss: 0.6067 - running_reg: 0.000000 - running_acc: 0.6743 - lr: 0.00041 - epoch_loss: 0.6072 - epoch_reg: 0.000000 - epoch_acc: 0.6712 - valid_loss: 0.6174 - valid_reg: 0.000000 - valid_acc: 0.6626 - epoch_time: 62.9102 s\n",
            "Epoch 74\n",
            "[====================] 200/200: - running_loss: 0.5959 - running_reg: 0.000000 - running_acc: 0.6802 - lr: 0.00041 - epoch_loss: 0.5993 - epoch_reg: 0.000000 - epoch_acc: 0.6773 - valid_loss: 0.6222 - valid_reg: 0.000000 - valid_acc: 0.6578 - epoch_time: 62.8305 s\n",
            "Epoch 75\n",
            "[====================] 200/200: - running_loss: 0.6105 - running_reg: 0.000000 - running_acc: 0.6601 - lr: 0.00041 - epoch_loss: 0.6067 - epoch_reg: 0.000000 - epoch_acc: 0.6655 - valid_loss: 0.6177 - valid_reg: 0.000000 - valid_acc: 0.6618 - epoch_time: 62.9087 s\n",
            "Epoch 76\n",
            "[====================] 200/200: - running_loss: 0.6056 - running_reg: 0.000000 - running_acc: 0.6641 - lr: 0.00040 - epoch_loss: 0.6044 - epoch_reg: 0.000000 - epoch_acc: 0.6662 - valid_loss: 0.6209 - valid_reg: 0.000000 - valid_acc: 0.6605 - epoch_time: 62.8810 s\n",
            "Epoch 77\n",
            "[====================] 200/200: - running_loss: 0.6093 - running_reg: 0.000000 - running_acc: 0.6618 - lr: 0.00040 - epoch_loss: 0.6073 - epoch_reg: 0.000000 - epoch_acc: 0.6641 - valid_loss: 0.6267 - valid_reg: 0.000000 - valid_acc: 0.6557 - epoch_time: 62.7686 s\n",
            "Epoch 78\n",
            "[====================] 200/200: - running_loss: 0.5900 - running_reg: 0.000000 - running_acc: 0.6852 - lr: 0.00040 - epoch_loss: 0.5922 - epoch_reg: 0.000000 - epoch_acc: 0.6847 - valid_loss: 0.6868 - valid_reg: 0.000000 - valid_acc: 0.6322 - epoch_time: 62.8555 s\n",
            "Epoch 79\n",
            "[====================] 200/200: - running_loss: 0.5941 - running_reg: 0.000000 - running_acc: 0.6803 - lr: 0.00040 - epoch_loss: 0.5993 - epoch_reg: 0.000000 - epoch_acc: 0.6744 - valid_loss: 0.6324 - valid_reg: 0.000000 - valid_acc: 0.6566 - epoch_time: 62.8558 s\n",
            "Epoch 80\n",
            "[====================] 200/200: - running_loss: 0.6089 - running_reg: 0.000000 - running_acc: 0.6668 - lr: 0.00039 - epoch_loss: 0.6095 - epoch_reg: 0.000000 - epoch_acc: 0.6678 - valid_loss: 0.6242 - valid_reg: 0.000000 - valid_acc: 0.6546 - epoch_time: 62.8786 s\n",
            "Epoch 81\n",
            "[====================] 200/200: - running_loss: 0.6006 - running_reg: 0.000000 - running_acc: 0.6727 - lr: 0.00039 - epoch_loss: 0.6027 - epoch_reg: 0.000000 - epoch_acc: 0.6725 - valid_loss: 0.6183 - valid_reg: 0.000000 - valid_acc: 0.6633 - epoch_time: 62.8947 s\n",
            "Epoch 82\n",
            "[====================] 200/200: - running_loss: 0.6004 - running_reg: 0.000000 - running_acc: 0.6750 - lr: 0.00039 - epoch_loss: 0.6045 - epoch_reg: 0.000000 - epoch_acc: 0.6691 - valid_loss: 0.6254 - valid_reg: 0.000000 - valid_acc: 0.6589 - epoch_time: 62.8769 s\n",
            "Epoch 83\n",
            "[====================] 200/200: - running_loss: 0.6033 - running_reg: 0.000000 - running_acc: 0.6700 - lr: 0.00039 - epoch_loss: 0.6029 - epoch_reg: 0.000000 - epoch_acc: 0.6692 - valid_loss: 0.6277 - valid_reg: 0.000000 - valid_acc: 0.6535 - epoch_time: 62.7659 s\n",
            "Epoch 84\n",
            "[====================] 200/200: - running_loss: 0.5957 - running_reg: 0.000000 - running_acc: 0.6768 - lr: 0.00038 - epoch_loss: 0.5955 - epoch_reg: 0.000000 - epoch_acc: 0.6775 - valid_loss: 0.6180 - valid_reg: 0.000000 - valid_acc: 0.6619 - epoch_time: 62.8218 s\n",
            "Epoch 85\n",
            "[====================] 200/200: - running_loss: 0.6007 - running_reg: 0.000000 - running_acc: 0.6701 - lr: 0.00038 - epoch_loss: 0.5959 - epoch_reg: 0.000000 - epoch_acc: 0.6767 - valid_loss: 0.6282 - valid_reg: 0.000000 - valid_acc: 0.6537 - epoch_time: 62.9194 s\n",
            "Epoch 86\n",
            "[====================] 200/200: - running_loss: 0.5871 - running_reg: 0.000000 - running_acc: 0.6849 - lr: 0.00038 - epoch_loss: 0.5889 - epoch_reg: 0.000000 - epoch_acc: 0.6847 - valid_loss: 0.6339 - valid_reg: 0.000000 - valid_acc: 0.6590 - epoch_time: 62.9238 s\n",
            "Epoch 87\n",
            "[====================] 200/200: - running_loss: 0.6053 - running_reg: 0.000000 - running_acc: 0.6720 - lr: 0.00038 - epoch_loss: 0.6039 - epoch_reg: 0.000000 - epoch_acc: 0.6708 - valid_loss: 0.6463 - valid_reg: 0.000000 - valid_acc: 0.6296 - epoch_time: 62.8513 s\n",
            "Epoch 88\n",
            "[====================] 200/200: - running_loss: 0.5967 - running_reg: 0.000000 - running_acc: 0.6732 - lr: 0.00037 - epoch_loss: 0.5969 - epoch_reg: 0.000000 - epoch_acc: 0.6733 - valid_loss: 0.6164 - valid_reg: 0.000000 - valid_acc: 0.6634 - epoch_time: 62.8952 s\n",
            "Epoch 89\n",
            "[====================] 200/200: - running_loss: 0.5937 - running_reg: 0.000000 - running_acc: 0.6824 - lr: 0.00037 - epoch_loss: 0.5930 - epoch_reg: 0.000000 - epoch_acc: 0.6823 - valid_loss: 0.6209 - valid_reg: 0.000000 - valid_acc: 0.6606 - epoch_time: 62.8889 s\n",
            "Epoch 90\n",
            "[====================] 200/200: - running_loss: 0.5906 - running_reg: 0.000000 - running_acc: 0.6827 - lr: 0.00037 - epoch_loss: 0.5891 - epoch_reg: 0.000000 - epoch_acc: 0.6853 - valid_loss: 0.6228 - valid_reg: 0.000000 - valid_acc: 0.6603 - epoch_time: 62.8582 s\n",
            "Epoch 91\n",
            "[====================] 200/200: - running_loss: 0.5931 - running_reg: 0.000000 - running_acc: 0.6778 - lr: 0.00037 - epoch_loss: 0.5933 - epoch_reg: 0.000000 - epoch_acc: 0.6773 - valid_loss: 0.6209 - valid_reg: 0.000000 - valid_acc: 0.6589 - epoch_time: 62.7704 s\n",
            "Epoch 92\n",
            "[====================] 200/200: - running_loss: 0.5924 - running_reg: 0.000000 - running_acc: 0.6868 - lr: 0.00037 - epoch_loss: 0.5938 - epoch_reg: 0.000000 - epoch_acc: 0.6836 - valid_loss: 0.6216 - valid_reg: 0.000000 - valid_acc: 0.6619 - epoch_time: 62.8881 s\n",
            "Epoch 93\n",
            "[====================] 200/200: - running_loss: 0.5908 - running_reg: 0.000000 - running_acc: 0.6864 - lr: 0.00036 - epoch_loss: 0.5906 - epoch_reg: 0.000000 - epoch_acc: 0.6852 - valid_loss: 0.6154 - valid_reg: 0.000000 - valid_acc: 0.6653 - epoch_time: 62.9231 s\n",
            "Epoch 94\n",
            "[====================] 200/200: - running_loss: 0.5852 - running_reg: 0.000000 - running_acc: 0.6817 - lr: 0.00036 - epoch_loss: 0.5856 - epoch_reg: 0.000000 - epoch_acc: 0.6842 - valid_loss: 0.6273 - valid_reg: 0.000000 - valid_acc: 0.6572 - epoch_time: 62.8008 s\n",
            "Epoch 95\n",
            "[====================] 200/200: - running_loss: 0.5873 - running_reg: 0.000000 - running_acc: 0.6876 - lr: 0.00036 - epoch_loss: 0.5855 - epoch_reg: 0.000000 - epoch_acc: 0.6877 - valid_loss: 0.6162 - valid_reg: 0.000000 - valid_acc: 0.6632 - epoch_time: 62.8671 s\n",
            "Epoch 96\n",
            "[====================] 200/200: - running_loss: 0.5937 - running_reg: 0.000000 - running_acc: 0.6865 - lr: 0.00036 - epoch_loss: 0.5939 - epoch_reg: 0.000000 - epoch_acc: 0.6875 - valid_loss: 0.6179 - valid_reg: 0.000000 - valid_acc: 0.6627 - epoch_time: 62.8711 s\n",
            "Epoch 97\n",
            "[====================] 200/200: - running_loss: 0.5801 - running_reg: 0.000000 - running_acc: 0.6886 - lr: 0.00036 - epoch_loss: 0.5837 - epoch_reg: 0.000000 - epoch_acc: 0.6870 - valid_loss: 0.6358 - valid_reg: 0.000000 - valid_acc: 0.6551 - epoch_time: 62.0034 s\n",
            "Epoch 98\n",
            "[====================] 200/200: - running_loss: 0.5882 - running_reg: 0.000000 - running_acc: 0.6816 - lr: 0.00036 - epoch_loss: 0.5859 - epoch_reg: 0.000000 - epoch_acc: 0.6845 - valid_loss: 0.6278 - valid_reg: 0.000000 - valid_acc: 0.6612 - epoch_time: 59.7624 s\n",
            "Epoch 99\n",
            "[====================] 200/200: - running_loss: 0.5756 - running_reg: 0.000000 - running_acc: 0.7021 - lr: 0.00035 - epoch_loss: 0.5809 - epoch_reg: 0.000000 - epoch_acc: 0.6941 - valid_loss: 0.6253 - valid_reg: 0.000000 - valid_acc: 0.6584 - epoch_time: 59.8212 s\n",
            " - test_loss: 0.6154 - test_reg: 0.000000 - test_acc: 0.6653 - test_time: 34.0613 s\n",
            "Original model 3464194 params, new model 3464194 params, ratio 1.0\n",
            "Epoch 0\n",
            "[====================] 200/200: - running_loss: 0.6936 - running_reg: 0.000000 - running_acc: 0.5066 - lr: 0.00001 - epoch_loss: 0.6943 - epoch_reg: 0.000000 - epoch_acc: 0.5083 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 39.2064 s\n",
            "Epoch 1\n",
            "[====================] 200/200: - running_loss: 0.6918 - running_reg: 0.000000 - running_acc: 0.5223 - lr: 0.00003 - epoch_loss: 0.6910 - epoch_reg: 0.000000 - epoch_acc: 0.5292 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 28.4104 s\n",
            "Epoch 2\n",
            "[====================] 200/200: - running_loss: 0.6909 - running_reg: 0.000000 - running_acc: 0.5310 - lr: 0.00004 - epoch_loss: 0.6911 - epoch_reg: 0.000000 - epoch_acc: 0.5311 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 28.5284 s\n",
            "Epoch 3\n",
            "[====================] 200/200: - running_loss: 0.6856 - running_reg: 0.000000 - running_acc: 0.5545 - lr: 0.00006 - epoch_loss: 0.6856 - epoch_reg: 0.000000 - epoch_acc: 0.5533 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 28.5233 s\n",
            "Epoch 4\n",
            "[====================] 200/200: - running_loss: 0.6869 - running_reg: 0.000000 - running_acc: 0.5397 - lr: 0.00007 - epoch_loss: 0.6864 - epoch_reg: 0.000000 - epoch_acc: 0.5472 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 28.5368 s\n",
            "Epoch 5\n",
            "[====================] 200/200: - running_loss: 0.6855 - running_reg: 0.000000 - running_acc: 0.5530 - lr: 0.00008 - epoch_loss: 0.6829 - epoch_reg: 0.000000 - epoch_acc: 0.5544 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 28.5442 s\n",
            "Epoch 6\n",
            "[====================] 200/200: - running_loss: 0.6937 - running_reg: 0.000000 - running_acc: 0.5329 - lr: 0.00010 - epoch_loss: 0.6908 - epoch_reg: 0.000000 - epoch_acc: 0.5380 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 28.6253 s\n",
            "Epoch 7\n",
            "[====================] 200/200: - running_loss: 0.6864 - running_reg: 0.000000 - running_acc: 0.5457 - lr: 0.00011 - epoch_loss: 0.6861 - epoch_reg: 0.000000 - epoch_acc: 0.5467 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 28.5082 s\n",
            "Epoch 8\n",
            "[====================] 200/200: - running_loss: 0.6797 - running_reg: 0.000000 - running_acc: 0.5701 - lr: 0.00013 - epoch_loss: 0.6813 - epoch_reg: 0.000000 - epoch_acc: 0.5627 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 28.5940 s\n",
            "Epoch 9\n",
            "[====================] 200/200: - running_loss: 0.6801 - running_reg: 0.000000 - running_acc: 0.5621 - lr: 0.00014 - epoch_loss: 0.6824 - epoch_reg: 0.000000 - epoch_acc: 0.5547 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 28.5793 s\n",
            "Epoch 10\n",
            "[====================] 200/200: - running_loss: 0.6806 - running_reg: 0.000000 - running_acc: 0.5656 - lr: 0.00015 - epoch_loss: 0.6812 - epoch_reg: 0.000000 - epoch_acc: 0.5656 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 28.5971 s\n",
            "Epoch 11\n",
            "[====================] 200/200: - running_loss: 0.6875 - running_reg: 0.000000 - running_acc: 0.5370 - lr: 0.00017 - epoch_loss: 0.6857 - epoch_reg: 0.000000 - epoch_acc: 0.5445 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 28.4929 s\n",
            "Epoch 12\n",
            "[====================] 200/200: - running_loss: 0.6900 - running_reg: 0.000000 - running_acc: 0.5309 - lr: 0.00018 - epoch_loss: 0.6882 - epoch_reg: 0.000000 - epoch_acc: 0.5408 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 28.5783 s\n",
            "Epoch 13\n",
            "[====================] 200/200: - running_loss: 0.6881 - running_reg: 0.000000 - running_acc: 0.5284 - lr: 0.00020 - epoch_loss: 0.6869 - epoch_reg: 0.000000 - epoch_acc: 0.5297 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 28.6162 s\n",
            "Epoch 14\n",
            "[====================] 200/200: - running_loss: 0.6848 - running_reg: 0.000000 - running_acc: 0.5525 - lr: 0.00021 - epoch_loss: 0.6827 - epoch_reg: 0.000000 - epoch_acc: 0.5564 - valid_loss: 0.0000 - valid_reg: 0.000000 - valid_acc: 0.0000 - epoch_time: 28.6673 s\n",
            "Epoch 15\n",
            "[====================] 200/200: - running_loss: 0.6905 - running_reg: 0.000000 - running_acc: 0.5276 - lr: 0.00022 - epoch_loss: 0.6867 - epoch_reg: 0.000000 - epoch_acc: 0.5359 - valid_loss: 0.6884 - valid_reg: 0.000000 - valid_acc: 0.5385 - epoch_time: 62.7038 s\n",
            "Epoch 16\n",
            "[====================] 200/200: - running_loss: 0.6858 - running_reg: 0.000000 - running_acc: 0.5510 - lr: 0.00024 - epoch_loss: 0.6856 - epoch_reg: 0.000000 - epoch_acc: 0.5516 - valid_loss: 0.7093 - valid_reg: 0.000000 - valid_acc: 0.5002 - epoch_time: 62.6258 s\n",
            "Epoch 17\n",
            "[====================] 200/200: - running_loss: 0.6958 - running_reg: 0.000000 - running_acc: 0.5065 - lr: 0.00025 - epoch_loss: 0.6952 - epoch_reg: 0.000000 - epoch_acc: 0.5075 - valid_loss: 0.6944 - valid_reg: 0.000000 - valid_acc: 0.5121 - epoch_time: 62.7207 s\n",
            "Epoch 18\n",
            "[====================] 200/200: - running_loss: 0.6917 - running_reg: 0.000000 - running_acc: 0.5213 - lr: 0.00027 - epoch_loss: 0.6921 - epoch_reg: 0.000000 - epoch_acc: 0.5184 - valid_loss: 0.6887 - valid_reg: 0.000000 - valid_acc: 0.5211 - epoch_time: 62.8039 s\n",
            "Epoch 19\n",
            "[====================] 200/200: - running_loss: 0.6894 - running_reg: 0.000000 - running_acc: 0.5339 - lr: 0.00028 - epoch_loss: 0.6914 - epoch_reg: 0.000000 - epoch_acc: 0.5220 - valid_loss: 0.6838 - valid_reg: 0.000000 - valid_acc: 0.5397 - epoch_time: 62.7798 s\n",
            "Epoch 20\n",
            "[====================] 200/200: - running_loss: 0.6894 - running_reg: 0.000000 - running_acc: 0.5289 - lr: 0.00029 - epoch_loss: 0.6894 - epoch_reg: 0.000000 - epoch_acc: 0.5291 - valid_loss: 0.6909 - valid_reg: 0.000000 - valid_acc: 0.5175 - epoch_time: 62.6796 s\n",
            "Epoch 21\n",
            "[====================] 200/200: - running_loss: 0.6860 - running_reg: 0.000000 - running_acc: 0.5467 - lr: 0.00031 - epoch_loss: 0.6892 - epoch_reg: 0.000000 - epoch_acc: 0.5370 - valid_loss: 0.6725 - valid_reg: 0.000000 - valid_acc: 0.5854 - epoch_time: 62.8417 s\n",
            "Epoch 22\n",
            "[====================] 200/200: - running_loss: 0.6935 - running_reg: 0.000000 - running_acc: 0.5140 - lr: 0.00032 - epoch_loss: 0.6911 - epoch_reg: 0.000000 - epoch_acc: 0.5236 - valid_loss: 0.6882 - valid_reg: 0.000000 - valid_acc: 0.5441 - epoch_time: 62.7858 s\n",
            "Epoch 23\n",
            "[====================] 200/200: - running_loss: 0.6821 - running_reg: 0.000000 - running_acc: 0.5565 - lr: 0.00034 - epoch_loss: 0.6842 - epoch_reg: 0.000000 - epoch_acc: 0.5480 - valid_loss: 0.7334 - valid_reg: 0.000000 - valid_acc: 0.5329 - epoch_time: 62.7559 s\n",
            "Epoch 24\n",
            "[====================] 200/200: - running_loss: 0.6874 - running_reg: 0.000000 - running_acc: 0.5439 - lr: 0.00035 - epoch_loss: 0.6859 - epoch_reg: 0.000000 - epoch_acc: 0.5472 - valid_loss: 0.6908 - valid_reg: 0.000000 - valid_acc: 0.5197 - epoch_time: 62.6685 s\n",
            "Epoch 25\n",
            "[====================] 200/200: - running_loss: 0.6945 - running_reg: 0.000000 - running_acc: 0.5091 - lr: 0.00036 - epoch_loss: 0.6941 - epoch_reg: 0.000000 - epoch_acc: 0.5150 - valid_loss: 0.6940 - valid_reg: 0.000000 - valid_acc: 0.5002 - epoch_time: 62.8416 s\n",
            "Epoch 26\n",
            "[====================] 200/200: - running_loss: 0.6927 - running_reg: 0.000000 - running_acc: 0.5199 - lr: 0.00038 - epoch_loss: 0.6929 - epoch_reg: 0.000000 - epoch_acc: 0.5217 - valid_loss: 0.6995 - valid_reg: 0.000000 - valid_acc: 0.5031 - epoch_time: 62.8521 s\n",
            "Epoch 27\n",
            "[====================] 200/200: - running_loss: 0.6933 - running_reg: 0.000000 - running_acc: 0.5086 - lr: 0.00039 - epoch_loss: 0.6929 - epoch_reg: 0.000000 - epoch_acc: 0.5122 - valid_loss: 0.6928 - valid_reg: 0.000000 - valid_acc: 0.5009 - epoch_time: 62.8304 s\n",
            "Epoch 28\n",
            "[====================] 200/200: - running_loss: 0.6916 - running_reg: 0.000000 - running_acc: 0.5213 - lr: 0.00041 - epoch_loss: 0.6923 - epoch_reg: 0.000000 - epoch_acc: 0.5169 - valid_loss: 0.6960 - valid_reg: 0.000000 - valid_acc: 0.5003 - epoch_time: 62.8490 s\n",
            "Epoch 29\n",
            "[====================] 200/200: - running_loss: 0.6925 - running_reg: 0.000000 - running_acc: 0.5137 - lr: 0.00042 - epoch_loss: 0.6928 - epoch_reg: 0.000000 - epoch_acc: 0.5122 - valid_loss: 0.6960 - valid_reg: 0.000000 - valid_acc: 0.5066 - epoch_time: 62.7875 s\n",
            "Epoch 30\n",
            "[====================] 200/200: - running_loss: 0.6899 - running_reg: 0.000000 - running_acc: 0.5199 - lr: 0.00043 - epoch_loss: 0.6914 - epoch_reg: 0.000000 - epoch_acc: 0.5186 - valid_loss: 0.6899 - valid_reg: 0.000000 - valid_acc: 0.5374 - epoch_time: 62.7421 s\n",
            "Epoch 31\n",
            "[====================] 200/200: - running_loss: 0.6947 - running_reg: 0.000000 - running_acc: 0.5213 - lr: 0.00045 - epoch_loss: 0.6947 - epoch_reg: 0.000000 - epoch_acc: 0.5181 - valid_loss: 0.6913 - valid_reg: 0.000000 - valid_acc: 0.5289 - epoch_time: 62.7677 s\n",
            "Epoch 32\n",
            "[====================] 200/200: - running_loss: 0.6907 - running_reg: 0.000000 - running_acc: 0.5168 - lr: 0.00046 - epoch_loss: 0.6908 - epoch_reg: 0.000000 - epoch_acc: 0.5191 - valid_loss: 0.6883 - valid_reg: 0.000000 - valid_acc: 0.5460 - epoch_time: 62.8437 s\n",
            "Epoch 33\n",
            "[====================] 200/200: - running_loss: 0.6934 - running_reg: 0.000000 - running_acc: 0.5193 - lr: 0.00048 - epoch_loss: 0.6935 - epoch_reg: 0.000000 - epoch_acc: 0.5186 - valid_loss: 0.6942 - valid_reg: 0.000000 - valid_acc: 0.5002 - epoch_time: 63.2057 s\n",
            "Epoch 34\n",
            "[====================] 200/200: - running_loss: 0.6919 - running_reg: 0.000000 - running_acc: 0.5151 - lr: 0.00049 - epoch_loss: 0.6917 - epoch_reg: 0.000000 - epoch_acc: 0.5203 - valid_loss: 0.6908 - valid_reg: 0.000000 - valid_acc: 0.5003 - epoch_time: 63.2735 s\n",
            "Epoch 35\n",
            "[====================] 200/200: - running_loss: 0.6852 - running_reg: 0.000000 - running_acc: 0.5535 - lr: 0.00050 - epoch_loss: 0.6860 - epoch_reg: 0.000000 - epoch_acc: 0.5509 - valid_loss: 0.6865 - valid_reg: 0.000000 - valid_acc: 0.5398 - epoch_time: 63.4636 s\n",
            "Epoch 36\n",
            "[====================] 200/200: - running_loss: 0.6879 - running_reg: 0.000000 - running_acc: 0.5375 - lr: 0.00052 - epoch_loss: 0.6894 - epoch_reg: 0.000000 - epoch_acc: 0.5289 - valid_loss: 0.7304 - valid_reg: 0.000000 - valid_acc: 0.5099 - epoch_time: 63.2952 s\n",
            "Epoch 37\n",
            "[====================] 200/200: - running_loss: 0.6878 - running_reg: 0.000000 - running_acc: 0.5454 - lr: 0.00053 - epoch_loss: 0.6881 - epoch_reg: 0.000000 - epoch_acc: 0.5458 - valid_loss: 0.6837 - valid_reg: 0.000000 - valid_acc: 0.5537 - epoch_time: 63.2354 s\n",
            "Epoch 38\n",
            "[====================] 200/200: - running_loss: 0.6852 - running_reg: 0.000000 - running_acc: 0.5544 - lr: 0.00054 - epoch_loss: 0.6844 - epoch_reg: 0.000000 - epoch_acc: 0.5514 - valid_loss: 0.6908 - valid_reg: 0.000000 - valid_acc: 0.5341 - epoch_time: 63.3467 s\n",
            "Epoch 39\n",
            "[====================] 200/200: - running_loss: 0.6879 - running_reg: 0.000000 - running_acc: 0.5441 - lr: 0.00056 - epoch_loss: 0.6897 - epoch_reg: 0.000000 - epoch_acc: 0.5364 - valid_loss: 0.6898 - valid_reg: 0.000000 - valid_acc: 0.5179 - epoch_time: 63.2070 s\n",
            "Epoch 40\n",
            "[====================] 200/200: - running_loss: 0.6833 - running_reg: 0.000000 - running_acc: 0.5517 - lr: 0.00055 - epoch_loss: 0.6807 - epoch_reg: 0.000000 - epoch_acc: 0.5609 - valid_loss: 0.6850 - valid_reg: 0.000000 - valid_acc: 0.5363 - epoch_time: 63.3068 s\n",
            "Epoch 41\n",
            "[====================] 200/200: - running_loss: 0.6775 - running_reg: 0.000000 - running_acc: 0.5766 - lr: 0.00055 - epoch_loss: 0.6773 - epoch_reg: 0.000000 - epoch_acc: 0.5764 - valid_loss: 0.6787 - valid_reg: 0.000000 - valid_acc: 0.5794 - epoch_time: 63.3017 s\n",
            "Epoch 42\n",
            "[====================] 200/200: - running_loss: 0.6733 - running_reg: 0.000000 - running_acc: 0.5911 - lr: 0.00054 - epoch_loss: 0.6740 - epoch_reg: 0.000000 - epoch_acc: 0.5873 - valid_loss: 0.6627 - valid_reg: 0.000000 - valid_acc: 0.6027 - epoch_time: 63.2846 s\n",
            "Epoch 43\n",
            "[====================] 200/200: - running_loss: 0.6732 - running_reg: 0.000000 - running_acc: 0.5822 - lr: 0.00053 - epoch_loss: 0.6747 - epoch_reg: 0.000000 - epoch_acc: 0.5764 - valid_loss: 0.6585 - valid_reg: 0.000000 - valid_acc: 0.6074 - epoch_time: 63.0420 s\n",
            "Epoch 44\n",
            "[====================] 200/200: - running_loss: 0.6589 - running_reg: 0.000000 - running_acc: 0.6081 - lr: 0.00053 - epoch_loss: 0.6588 - epoch_reg: 0.000000 - epoch_acc: 0.6075 - valid_loss: 0.6637 - valid_reg: 0.000000 - valid_acc: 0.5987 - epoch_time: 62.8785 s\n",
            "Epoch 45\n",
            "[====================] 200/200: - running_loss: 0.6626 - running_reg: 0.000000 - running_acc: 0.5975 - lr: 0.00052 - epoch_loss: 0.6633 - epoch_reg: 0.000000 - epoch_acc: 0.5975 - valid_loss: 0.6690 - valid_reg: 0.000000 - valid_acc: 0.5877 - epoch_time: 62.9876 s\n",
            "Epoch 46\n",
            "[====================] 200/200: - running_loss: 0.6630 - running_reg: 0.000000 - running_acc: 0.6009 - lr: 0.00052 - epoch_loss: 0.6622 - epoch_reg: 0.000000 - epoch_acc: 0.6022 - valid_loss: 0.6568 - valid_reg: 0.000000 - valid_acc: 0.6168 - epoch_time: 62.8521 s\n",
            "Epoch 47\n",
            "[====================] 200/200: - running_loss: 0.6464 - running_reg: 0.000000 - running_acc: 0.6345 - lr: 0.00051 - epoch_loss: 0.6460 - epoch_reg: 0.000000 - epoch_acc: 0.6355 - valid_loss: 0.6509 - valid_reg: 0.000000 - valid_acc: 0.6175 - epoch_time: 62.9317 s\n",
            "Epoch 48\n",
            "[====================] 200/200: - running_loss: 0.6558 - running_reg: 0.000000 - running_acc: 0.6090 - lr: 0.00051 - epoch_loss: 0.6499 - epoch_reg: 0.000000 - epoch_acc: 0.6156 - valid_loss: 0.6587 - valid_reg: 0.000000 - valid_acc: 0.6199 - epoch_time: 62.9228 s\n",
            "Epoch 49\n",
            "[====================] 200/200: - running_loss: 0.6481 - running_reg: 0.000000 - running_acc: 0.6205 - lr: 0.00050 - epoch_loss: 0.6479 - epoch_reg: 0.000000 - epoch_acc: 0.6217 - valid_loss: 0.6461 - valid_reg: 0.000000 - valid_acc: 0.6270 - epoch_time: 63.1734 s\n",
            "Epoch 50\n",
            "[====================] 200/200: - running_loss: 0.6417 - running_reg: 0.000000 - running_acc: 0.6265 - lr: 0.00050 - epoch_loss: 0.6433 - epoch_reg: 0.000000 - epoch_acc: 0.6225 - valid_loss: 0.6542 - valid_reg: 0.000000 - valid_acc: 0.6167 - epoch_time: 63.1393 s\n",
            "Epoch 51\n",
            "[====================] 200/200: - running_loss: 0.6448 - running_reg: 0.000000 - running_acc: 0.6301 - lr: 0.00049 - epoch_loss: 0.6424 - epoch_reg: 0.000000 - epoch_acc: 0.6345 - valid_loss: 0.6507 - valid_reg: 0.000000 - valid_acc: 0.6167 - epoch_time: 63.1025 s\n",
            "Epoch 52\n",
            "[====================] 200/200: - running_loss: 0.6407 - running_reg: 0.000000 - running_acc: 0.6312 - lr: 0.00049 - epoch_loss: 0.6402 - epoch_reg: 0.000000 - epoch_acc: 0.6328 - valid_loss: 0.6458 - valid_reg: 0.000000 - valid_acc: 0.6261 - epoch_time: 63.0930 s\n",
            "Epoch 53\n",
            "[====================] 200/200: - running_loss: 0.6357 - running_reg: 0.000000 - running_acc: 0.6385 - lr: 0.00048 - epoch_loss: 0.6352 - epoch_reg: 0.000000 - epoch_acc: 0.6408 - valid_loss: 0.6464 - valid_reg: 0.000000 - valid_acc: 0.6288 - epoch_time: 63.1950 s\n",
            "Epoch 54\n",
            "[====================] 200/200: - running_loss: 0.6371 - running_reg: 0.000000 - running_acc: 0.6309 - lr: 0.00048 - epoch_loss: 0.6396 - epoch_reg: 0.000000 - epoch_acc: 0.6261 - valid_loss: 0.6548 - valid_reg: 0.000000 - valid_acc: 0.6098 - epoch_time: 63.1258 s\n",
            "Epoch 55\n",
            "[====================] 200/200: - running_loss: 0.6301 - running_reg: 0.000000 - running_acc: 0.6423 - lr: 0.00047 - epoch_loss: 0.6310 - epoch_reg: 0.000000 - epoch_acc: 0.6395 - valid_loss: 0.6446 - valid_reg: 0.000000 - valid_acc: 0.6275 - epoch_time: 63.0689 s\n",
            "Epoch 56\n",
            "[====================] 200/200: - running_loss: 0.6319 - running_reg: 0.000000 - running_acc: 0.6470 - lr: 0.00047 - epoch_loss: 0.6294 - epoch_reg: 0.000000 - epoch_acc: 0.6505 - valid_loss: 0.6417 - valid_reg: 0.000000 - valid_acc: 0.6320 - epoch_time: 63.1636 s\n",
            "Epoch 57\n",
            "[====================] 200/200: - running_loss: 0.6349 - running_reg: 0.000000 - running_acc: 0.6394 - lr: 0.00046 - epoch_loss: 0.6319 - epoch_reg: 0.000000 - epoch_acc: 0.6430 - valid_loss: 0.6479 - valid_reg: 0.000000 - valid_acc: 0.6206 - epoch_time: 63.0361 s\n",
            "Epoch 58\n",
            "[====================] 200/200: - running_loss: 0.6287 - running_reg: 0.000000 - running_acc: 0.6455 - lr: 0.00046 - epoch_loss: 0.6286 - epoch_reg: 0.000000 - epoch_acc: 0.6472 - valid_loss: 0.6620 - valid_reg: 0.000000 - valid_acc: 0.5925 - epoch_time: 63.1081 s\n",
            "Epoch 59\n",
            "[====================] 200/200: - running_loss: 0.6197 - running_reg: 0.000000 - running_acc: 0.6546 - lr: 0.00046 - epoch_loss: 0.6235 - epoch_reg: 0.000000 - epoch_acc: 0.6489 - valid_loss: 0.6347 - valid_reg: 0.000000 - valid_acc: 0.6429 - epoch_time: 63.2067 s\n",
            "Epoch 60\n",
            "[====================] 200/200: - running_loss: 0.6205 - running_reg: 0.000000 - running_acc: 0.6488 - lr: 0.00045 - epoch_loss: 0.6202 - epoch_reg: 0.000000 - epoch_acc: 0.6495 - valid_loss: 0.6360 - valid_reg: 0.000000 - valid_acc: 0.6475 - epoch_time: 63.2242 s\n",
            "Epoch 61\n",
            "[====================] 200/200: - running_loss: 0.6312 - running_reg: 0.000000 - running_acc: 0.6459 - lr: 0.00045 - epoch_loss: 0.6305 - epoch_reg: 0.000000 - epoch_acc: 0.6484 - valid_loss: 0.6329 - valid_reg: 0.000000 - valid_acc: 0.6453 - epoch_time: 63.0868 s\n",
            "Epoch 62\n",
            "[====================] 200/200: - running_loss: 0.6220 - running_reg: 0.000000 - running_acc: 0.6562 - lr: 0.00045 - epoch_loss: 0.6237 - epoch_reg: 0.000000 - epoch_acc: 0.6544 - valid_loss: 0.6492 - valid_reg: 0.000000 - valid_acc: 0.6252 - epoch_time: 63.0627 s\n",
            "Epoch 63\n",
            "[====================] 200/200: - running_loss: 0.6220 - running_reg: 0.000000 - running_acc: 0.6547 - lr: 0.00044 - epoch_loss: 0.6191 - epoch_reg: 0.000000 - epoch_acc: 0.6569 - valid_loss: 0.6309 - valid_reg: 0.000000 - valid_acc: 0.6474 - epoch_time: 63.1146 s\n",
            "Epoch 64\n",
            "[====================] 200/200: - running_loss: 0.6253 - running_reg: 0.000000 - running_acc: 0.6561 - lr: 0.00044 - epoch_loss: 0.6256 - epoch_reg: 0.000000 - epoch_acc: 0.6534 - valid_loss: 0.6291 - valid_reg: 0.000000 - valid_acc: 0.6495 - epoch_time: 63.1911 s\n",
            "Epoch 65\n",
            "[====================] 200/200: - running_loss: 0.6226 - running_reg: 0.000000 - running_acc: 0.6575 - lr: 0.00044 - epoch_loss: 0.6197 - epoch_reg: 0.000000 - epoch_acc: 0.6581 - valid_loss: 0.6287 - valid_reg: 0.000000 - valid_acc: 0.6506 - epoch_time: 63.0783 s\n",
            "Epoch 66\n",
            "[====================] 200/200: - running_loss: 0.6238 - running_reg: 0.000000 - running_acc: 0.6444 - lr: 0.00043 - epoch_loss: 0.6242 - epoch_reg: 0.000000 - epoch_acc: 0.6470 - valid_loss: 0.6470 - valid_reg: 0.000000 - valid_acc: 0.6276 - epoch_time: 63.0711 s\n",
            "Epoch 67\n",
            "[====================] 200/200: - running_loss: 0.6113 - running_reg: 0.000000 - running_acc: 0.6679 - lr: 0.00043 - epoch_loss: 0.6140 - epoch_reg: 0.000000 - epoch_acc: 0.6609 - valid_loss: 0.6282 - valid_reg: 0.000000 - valid_acc: 0.6526 - epoch_time: 63.2385 s\n",
            "Epoch 68\n",
            "[====================] 200/200: - running_loss: 0.6067 - running_reg: 0.000000 - running_acc: 0.6769 - lr: 0.00043 - epoch_loss: 0.6100 - epoch_reg: 0.000000 - epoch_acc: 0.6698 - valid_loss: 0.6296 - valid_reg: 0.000000 - valid_acc: 0.6519 - epoch_time: 62.8379 s\n",
            "Epoch 69\n",
            "[====================] 200/200: - running_loss: 0.6182 - running_reg: 0.000000 - running_acc: 0.6592 - lr: 0.00042 - epoch_loss: 0.6213 - epoch_reg: 0.000000 - epoch_acc: 0.6559 - valid_loss: 0.6484 - valid_reg: 0.000000 - valid_acc: 0.6285 - epoch_time: 63.0218 s\n",
            "Epoch 70\n",
            "[====================] 200/200: - running_loss: 0.6108 - running_reg: 0.000000 - running_acc: 0.6728 - lr: 0.00042 - epoch_loss: 0.6114 - epoch_reg: 0.000000 - epoch_acc: 0.6716 - valid_loss: 0.6353 - valid_reg: 0.000000 - valid_acc: 0.6509 - epoch_time: 63.0574 s\n",
            "Epoch 71\n",
            "[====================] 200/200: - running_loss: 0.6136 - running_reg: 0.000000 - running_acc: 0.6619 - lr: 0.00042 - epoch_loss: 0.6132 - epoch_reg: 0.000000 - epoch_acc: 0.6639 - valid_loss: 0.6282 - valid_reg: 0.000000 - valid_acc: 0.6514 - epoch_time: 62.9490 s\n",
            "Epoch 72\n",
            "[====================] 200/200: - running_loss: 0.6161 - running_reg: 0.000000 - running_acc: 0.6612 - lr: 0.00041 - epoch_loss: 0.6136 - epoch_reg: 0.000000 - epoch_acc: 0.6645 - valid_loss: 0.6253 - valid_reg: 0.000000 - valid_acc: 0.6554 - epoch_time: 63.1518 s\n",
            "Epoch 73\n",
            "[====================] 200/200: - running_loss: 0.6059 - running_reg: 0.000000 - running_acc: 0.6766 - lr: 0.00041 - epoch_loss: 0.6037 - epoch_reg: 0.000000 - epoch_acc: 0.6777 - valid_loss: 0.6327 - valid_reg: 0.000000 - valid_acc: 0.6457 - epoch_time: 62.8961 s\n",
            "Epoch 74\n",
            "[====================] 200/200: - running_loss: 0.6142 - running_reg: 0.000000 - running_acc: 0.6549 - lr: 0.00041 - epoch_loss: 0.6155 - epoch_reg: 0.000000 - epoch_acc: 0.6519 - valid_loss: 0.6243 - valid_reg: 0.000000 - valid_acc: 0.6559 - epoch_time: 63.0984 s\n",
            "Epoch 75\n",
            "[====================] 200/200: - running_loss: 0.6018 - running_reg: 0.000000 - running_acc: 0.6773 - lr: 0.00041 - epoch_loss: 0.6031 - epoch_reg: 0.000000 - epoch_acc: 0.6766 - valid_loss: 0.6259 - valid_reg: 0.000000 - valid_acc: 0.6564 - epoch_time: 63.0348 s\n",
            "Epoch 76\n",
            "[====================] 200/200: - running_loss: 0.6115 - running_reg: 0.000000 - running_acc: 0.6659 - lr: 0.00040 - epoch_loss: 0.6077 - epoch_reg: 0.000000 - epoch_acc: 0.6711 - valid_loss: 0.6242 - valid_reg: 0.000000 - valid_acc: 0.6584 - epoch_time: 63.0588 s\n",
            "Epoch 77\n",
            "[====================] 200/200: - running_loss: 0.6125 - running_reg: 0.000000 - running_acc: 0.6621 - lr: 0.00040 - epoch_loss: 0.6120 - epoch_reg: 0.000000 - epoch_acc: 0.6639 - valid_loss: 0.6376 - valid_reg: 0.000000 - valid_acc: 0.6537 - epoch_time: 62.8782 s\n",
            "Epoch 78\n",
            "[====================] 200/200: - running_loss: 0.6114 - running_reg: 0.000000 - running_acc: 0.6616 - lr: 0.00040 - epoch_loss: 0.6155 - epoch_reg: 0.000000 - epoch_acc: 0.6577 - valid_loss: 0.6220 - valid_reg: 0.000000 - valid_acc: 0.6594 - epoch_time: 62.9609 s\n",
            "Epoch 79\n",
            "[====================] 200/200: - running_loss: 0.6004 - running_reg: 0.000000 - running_acc: 0.6736 - lr: 0.00040 - epoch_loss: 0.5985 - epoch_reg: 0.000000 - epoch_acc: 0.6769 - valid_loss: 0.6242 - valid_reg: 0.000000 - valid_acc: 0.6600 - epoch_time: 62.9639 s\n",
            "Epoch 80\n",
            "[====================] 200/200: - running_loss: 0.5993 - running_reg: 0.000000 - running_acc: 0.6811 - lr: 0.00039 - epoch_loss: 0.6033 - epoch_reg: 0.000000 - epoch_acc: 0.6797 - valid_loss: 0.6378 - valid_reg: 0.000000 - valid_acc: 0.6535 - epoch_time: 63.1771 s\n",
            "Epoch 81\n",
            "[====================] 200/200: - running_loss: 0.5978 - running_reg: 0.000000 - running_acc: 0.6822 - lr: 0.00039 - epoch_loss: 0.6043 - epoch_reg: 0.000000 - epoch_acc: 0.6755 - valid_loss: 0.6317 - valid_reg: 0.000000 - valid_acc: 0.6540 - epoch_time: 63.0279 s\n",
            "Epoch 82\n",
            "[====================] 200/200: - running_loss: 0.6076 - running_reg: 0.000000 - running_acc: 0.6651 - lr: 0.00039 - epoch_loss: 0.6086 - epoch_reg: 0.000000 - epoch_acc: 0.6617 - valid_loss: 0.6307 - valid_reg: 0.000000 - valid_acc: 0.6558 - epoch_time: 63.0906 s\n",
            "Epoch 83\n",
            "[====================] 200/200: - running_loss: 0.6100 - running_reg: 0.000000 - running_acc: 0.6580 - lr: 0.00039 - epoch_loss: 0.6072 - epoch_reg: 0.000000 - epoch_acc: 0.6670 - valid_loss: 0.6247 - valid_reg: 0.000000 - valid_acc: 0.6551 - epoch_time: 63.1149 s\n",
            "Epoch 84\n",
            "[====================] 200/200: - running_loss: 0.5961 - running_reg: 0.000000 - running_acc: 0.6832 - lr: 0.00038 - epoch_loss: 0.5942 - epoch_reg: 0.000000 - epoch_acc: 0.6841 - valid_loss: 0.6247 - valid_reg: 0.000000 - valid_acc: 0.6545 - epoch_time: 63.1526 s\n",
            "Epoch 85\n",
            "[====================] 200/200: - running_loss: 0.6021 - running_reg: 0.000000 - running_acc: 0.6769 - lr: 0.00038 - epoch_loss: 0.5978 - epoch_reg: 0.000000 - epoch_acc: 0.6800 - valid_loss: 0.6246 - valid_reg: 0.000000 - valid_acc: 0.6538 - epoch_time: 63.0909 s\n",
            "Epoch 86\n",
            "[====================] 200/200: - running_loss: 0.5924 - running_reg: 0.000000 - running_acc: 0.6836 - lr: 0.00038 - epoch_loss: 0.5947 - epoch_reg: 0.000000 - epoch_acc: 0.6822 - valid_loss: 0.6364 - valid_reg: 0.000000 - valid_acc: 0.6585 - epoch_time: 63.1872 s\n",
            "Epoch 87\n",
            "[====================] 200/200: - running_loss: 0.6056 - running_reg: 0.000000 - running_acc: 0.6692 - lr: 0.00038 - epoch_loss: 0.6047 - epoch_reg: 0.000000 - epoch_acc: 0.6700 - valid_loss: 0.6201 - valid_reg: 0.000000 - valid_acc: 0.6610 - epoch_time: 63.2474 s\n",
            "Epoch 88\n",
            "[====================] 200/200: - running_loss: 0.5950 - running_reg: 0.000000 - running_acc: 0.6737 - lr: 0.00037 - epoch_loss: 0.5939 - epoch_reg: 0.000000 - epoch_acc: 0.6777 - valid_loss: 0.6336 - valid_reg: 0.000000 - valid_acc: 0.6492 - epoch_time: 62.8828 s\n",
            "Epoch 89\n",
            "[====================] 200/200: - running_loss: 0.5916 - running_reg: 0.000000 - running_acc: 0.6875 - lr: 0.00037 - epoch_loss: 0.5984 - epoch_reg: 0.000000 - epoch_acc: 0.6825 - valid_loss: 0.6313 - valid_reg: 0.000000 - valid_acc: 0.6543 - epoch_time: 63.1334 s\n",
            "Epoch 90\n",
            "[====================] 200/200: - running_loss: 0.6015 - running_reg: 0.000000 - running_acc: 0.6762 - lr: 0.00037 - epoch_loss: 0.5976 - epoch_reg: 0.000000 - epoch_acc: 0.6766 - valid_loss: 0.6281 - valid_reg: 0.000000 - valid_acc: 0.6470 - epoch_time: 63.1451 s\n",
            "Epoch 91\n",
            "[====================] 200/200: - running_loss: 0.5908 - running_reg: 0.000000 - running_acc: 0.6852 - lr: 0.00037 - epoch_loss: 0.5910 - epoch_reg: 0.000000 - epoch_acc: 0.6819 - valid_loss: 0.6216 - valid_reg: 0.000000 - valid_acc: 0.6572 - epoch_time: 63.1551 s\n",
            "Epoch 92\n",
            "[====================] 200/200: - running_loss: 0.5905 - running_reg: 0.000000 - running_acc: 0.6865 - lr: 0.00037 - epoch_loss: 0.5886 - epoch_reg: 0.000000 - epoch_acc: 0.6883 - valid_loss: 0.6311 - valid_reg: 0.000000 - valid_acc: 0.6512 - epoch_time: 63.1097 s\n",
            "Epoch 93\n",
            "[====================] 200/200: - running_loss: 0.5905 - running_reg: 0.000000 - running_acc: 0.6916 - lr: 0.00036 - epoch_loss: 0.5958 - epoch_reg: 0.000000 - epoch_acc: 0.6859 - valid_loss: 0.6444 - valid_reg: 0.000000 - valid_acc: 0.6483 - epoch_time: 63.1639 s\n",
            "Epoch 94\n",
            "[====================] 200/200: - running_loss: 0.5958 - running_reg: 0.000000 - running_acc: 0.6794 - lr: 0.00036 - epoch_loss: 0.5938 - epoch_reg: 0.000000 - epoch_acc: 0.6817 - valid_loss: 0.6290 - valid_reg: 0.000000 - valid_acc: 0.6451 - epoch_time: 63.1734 s\n",
            "Epoch 95\n",
            "[====================] 200/200: - running_loss: 0.5885 - running_reg: 0.000000 - running_acc: 0.6864 - lr: 0.00036 - epoch_loss: 0.5952 - epoch_reg: 0.000000 - epoch_acc: 0.6819 - valid_loss: 0.6226 - valid_reg: 0.000000 - valid_acc: 0.6609 - epoch_time: 63.1519 s\n",
            "Epoch 96\n",
            "[====================] 200/200: - running_loss: 0.5920 - running_reg: 0.000000 - running_acc: 0.6812 - lr: 0.00036 - epoch_loss: 0.5960 - epoch_reg: 0.000000 - epoch_acc: 0.6803 - valid_loss: 0.6619 - valid_reg: 0.000000 - valid_acc: 0.6185 - epoch_time: 63.2249 s\n",
            "Epoch 97\n",
            "[====================] 200/200: - running_loss: 0.6061 - running_reg: 0.000000 - running_acc: 0.6674 - lr: 0.00036 - epoch_loss: 0.6008 - epoch_reg: 0.000000 - epoch_acc: 0.6723 - valid_loss: 0.6184 - valid_reg: 0.000000 - valid_acc: 0.6646 - epoch_time: 62.4822 s\n",
            "Epoch 98\n",
            "[====================] 200/200: - running_loss: 0.5960 - running_reg: 0.000000 - running_acc: 0.6797 - lr: 0.00036 - epoch_loss: 0.5930 - epoch_reg: 0.000000 - epoch_acc: 0.6831 - valid_loss: 0.6310 - valid_reg: 0.000000 - valid_acc: 0.6439 - epoch_time: 59.8346 s\n",
            "Epoch 99\n",
            "[====================] 200/200: - running_loss: 0.5825 - running_reg: 0.000000 - running_acc: 0.6889 - lr: 0.00035 - epoch_loss: 0.5817 - epoch_reg: 0.000000 - epoch_acc: 0.6897 - valid_loss: 0.6230 - valid_reg: 0.000000 - valid_acc: 0.6570 - epoch_time: 59.8162 s\n",
            " - test_loss: 0.6184 - test_reg: 0.000000 - test_acc: 0.6646 - test_time: 34.1201 s\n",
            "\n",
            "Total accuracy: 0.6653\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74SAO5c66OPS"
      },
      "source": [
        "3 x AOGLU l=0.0: 0.6610, 0.6571, 0.6601, 0.6568, 0.6622"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MmO4BN8-gn7T"
      },
      "source": [
        "FFN l=0.005: 0.6585, 0.6530, 0.6589, 0.6601, 0.6594"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vEIV04heCzdU"
      },
      "source": [
        "FFN noorth: 0.6582, 0.6456, 0.6605, 0.6591, 0.5521, | 0.6580, 0.6530"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n7_yUbfTdrCR",
        "outputId": "19835d4c-a559-424b-b274-cb1dea6d1ec3"
      },
      "source": [
        "activation = {}\n",
        "def get_activation(name):\n",
        "    def hook(model, input, output):\n",
        "        activation[name] = output[0].detach()\n",
        "    return hook\n",
        "\n",
        "process_inputs = lambda x: torch.Tensor(x.numpy()).to(torch.int64)\n",
        "\n",
        "list(model.blocks[-1].attention.lka.modules())[0][0].register_forward_hook(get_activation('1'))\n",
        "#list(model.blocks[-1].attention.lka.modules())[0][1].register_forward_hook(get_activation('2'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<torch.utils.hooks.RemovableHandle at 0x7fc6193882d0>"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qhQjEjjecyMN",
        "outputId": "e12c6dfc-e241-4711-c5f2-b54807d891e8"
      },
      "source": [
        "model(process_inputs(next(iter(train_dataset))['inputs']).cuda())\n",
        "activation"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'1': tensor([[[[0.4846, 0.6129, 1.0817,  ..., 0.8458, 0.8182, 0.6113],\n",
              "           [0.6741, 1.0226, 0.9306,  ..., 0.6382, 0.4864, 0.4956],\n",
              "           [0.7144, 0.7925, 0.9776,  ..., 0.8392, 0.8342, 0.6923],\n",
              "           ...,\n",
              "           [0.6351, 0.4956, 0.8323,  ..., 1.1306, 0.8587, 0.7959],\n",
              "           [0.6259, 1.0617, 0.9905,  ..., 0.7249, 0.8088, 0.8153],\n",
              "           [0.7734, 0.8578, 0.5455,  ..., 0.5648, 0.7239, 0.6971]],\n",
              " \n",
              "          [[1.1219, 0.8068, 0.7288,  ..., 0.5271, 1.1042, 0.6128],\n",
              "           [0.8955, 0.9000, 1.3224,  ..., 0.5096, 0.5674, 1.1289],\n",
              "           [0.7283, 0.8557, 0.4822,  ..., 0.8176, 0.3683, 0.6196],\n",
              "           ...,\n",
              "           [0.6962, 0.6160, 0.4413,  ..., 1.0298, 0.8417, 0.8641],\n",
              "           [1.0189, 0.9670, 0.5587,  ..., 0.9078, 0.7291, 0.7901],\n",
              "           [0.6431, 0.5184, 0.5831,  ..., 0.6299, 0.6459, 0.6050]],\n",
              " \n",
              "          [[0.6520, 0.8644, 0.8144,  ..., 0.5583, 0.4932, 0.5764],\n",
              "           [0.3789, 0.7886, 0.5084,  ..., 0.7604, 0.5583, 0.4504],\n",
              "           [0.8306, 0.9530, 0.6154,  ..., 0.7198, 0.6716, 0.8316],\n",
              "           ...,\n",
              "           [0.7072, 0.7885, 0.6645,  ..., 0.5373, 0.8313, 0.7544],\n",
              "           [0.9306, 0.6810, 0.7918,  ..., 0.8676, 0.8027, 0.7283],\n",
              "           [0.6329, 0.6154, 0.7098,  ..., 0.4776, 0.7722, 0.6611]],\n",
              " \n",
              "          [[0.7786, 0.9671, 0.7944,  ..., 0.6249, 0.8022, 0.5712],\n",
              "           [0.5801, 0.6983, 0.5575,  ..., 0.6784, 0.7260, 0.7423],\n",
              "           [0.7165, 0.7591, 0.9826,  ..., 0.4811, 0.4956, 0.8089],\n",
              "           ...,\n",
              "           [0.9886, 0.6557, 0.4287,  ..., 1.0515, 0.5590, 1.2194],\n",
              "           [0.8846, 0.6401, 0.7460,  ..., 0.5371, 0.7053, 0.6377],\n",
              "           [0.9957, 0.2716, 0.6007,  ..., 0.9580, 0.3327, 0.6476]]],\n",
              " \n",
              " \n",
              "         [[[0.8930, 0.6364, 0.6370,  ..., 0.6555, 0.5743, 0.8378],\n",
              "           [0.8831, 0.9794, 0.8412,  ..., 0.5675, 0.6064, 0.7537],\n",
              "           [0.8718, 0.6853, 0.8128,  ..., 0.8434, 0.6892, 0.5453],\n",
              "           ...,\n",
              "           [0.7026, 0.5422, 0.3716,  ..., 0.4875, 0.7247, 0.8934],\n",
              "           [1.2645, 0.7219, 0.6805,  ..., 0.7243, 1.0154, 0.7983],\n",
              "           [0.5913, 0.8728, 0.8055,  ..., 0.9002, 0.7161, 0.8851]],\n",
              " \n",
              "          [[0.4333, 0.6424, 0.4917,  ..., 0.6138, 0.8224, 1.0296],\n",
              "           [0.5902, 1.0217, 0.7909,  ..., 0.6172, 0.5229, 0.6886],\n",
              "           [0.9553, 0.6320, 0.3309,  ..., 1.1288, 0.4941, 0.7927],\n",
              "           ...,\n",
              "           [0.5305, 0.5709, 1.0517,  ..., 0.5401, 0.4635, 1.2611],\n",
              "           [0.5408, 0.6843, 1.0863,  ..., 0.7544, 0.3629, 0.8321],\n",
              "           [0.7382, 0.7683, 0.7612,  ..., 0.7868, 0.5918, 0.5935]],\n",
              " \n",
              "          [[0.7341, 0.9329, 1.0449,  ..., 0.8102, 1.0817, 0.8769],\n",
              "           [0.6250, 0.6260, 0.9342,  ..., 0.7469, 0.5003, 0.5282],\n",
              "           [0.6004, 0.5281, 1.0101,  ..., 0.8523, 0.6920, 0.7553],\n",
              "           ...,\n",
              "           [0.9517, 0.9641, 0.9220,  ..., 0.4221, 0.4426, 0.8048],\n",
              "           [0.4548, 0.5333, 0.5736,  ..., 1.1812, 0.5742, 0.6966],\n",
              "           [0.7830, 0.4640, 0.5448,  ..., 0.6196, 0.5784, 0.7986]],\n",
              " \n",
              "          [[0.7263, 0.8433, 0.6435,  ..., 0.6081, 0.9357, 1.0626],\n",
              "           [0.3649, 0.7983, 0.5728,  ..., 0.4686, 0.9862, 0.7960],\n",
              "           [0.6301, 0.7797, 0.4217,  ..., 0.6837, 0.6317, 0.9112],\n",
              "           ...,\n",
              "           [0.5928, 0.6314, 0.7262,  ..., 0.6301, 0.5901, 1.0862],\n",
              "           [0.5278, 0.4373, 0.4281,  ..., 0.7741, 0.5059, 1.1810],\n",
              "           [0.5299, 0.4793, 0.6697,  ..., 0.5519, 0.4068, 0.5940]]],\n",
              " \n",
              " \n",
              "         [[[0.5173, 0.4609, 0.3646,  ..., 0.7049, 0.8529, 1.0608],\n",
              "           [0.8809, 0.4967, 0.6903,  ..., 0.6765, 0.4130, 0.8474],\n",
              "           [0.4907, 0.7993, 0.4250,  ..., 0.7400, 0.7169, 0.8319],\n",
              "           ...,\n",
              "           [0.7427, 0.6648, 0.7337,  ..., 0.6867, 0.7303, 0.6022],\n",
              "           [0.9273, 0.7987, 1.1405,  ..., 0.6633, 0.4618, 0.4035],\n",
              "           [0.7829, 0.7052, 0.7513,  ..., 0.7967, 0.3077, 0.5792]],\n",
              " \n",
              "          [[0.5744, 0.5170, 0.5111,  ..., 0.6387, 1.4359, 0.6881],\n",
              "           [0.5406, 0.7437, 0.6796,  ..., 0.6085, 0.3997, 0.6651],\n",
              "           [0.9269, 0.7365, 0.6759,  ..., 0.4442, 0.9492, 0.7859],\n",
              "           ...,\n",
              "           [0.5625, 1.0477, 0.6989,  ..., 0.3193, 0.9648, 0.9948],\n",
              "           [0.8559, 0.5630, 0.6819,  ..., 0.7938, 1.0081, 0.7846],\n",
              "           [0.7721, 0.6051, 0.9171,  ..., 0.8199, 0.5124, 0.6793]],\n",
              " \n",
              "          [[0.5939, 0.7135, 0.6993,  ..., 0.8693, 0.7109, 0.4790],\n",
              "           [1.2267, 0.5138, 0.8688,  ..., 0.6919, 0.4456, 1.1771],\n",
              "           [0.8001, 0.5119, 0.4499,  ..., 0.7845, 1.0466, 0.6150],\n",
              "           ...,\n",
              "           [0.9662, 0.5447, 0.5725,  ..., 0.7900, 0.4642, 0.8714],\n",
              "           [0.6128, 1.4435, 1.0496,  ..., 0.4937, 0.8764, 0.7703],\n",
              "           [1.1477, 0.4474, 1.1052,  ..., 0.4511, 0.7057, 1.1471]],\n",
              " \n",
              "          [[0.5751, 0.7363, 0.5543,  ..., 1.2802, 1.0292, 0.9101],\n",
              "           [0.8339, 0.7821, 0.4958,  ..., 0.6689, 0.7094, 1.2240],\n",
              "           [0.6334, 0.6162, 1.0532,  ..., 0.5831, 0.9784, 0.4457],\n",
              "           ...,\n",
              "           [0.9673, 0.5089, 0.5119,  ..., 0.7646, 0.6643, 0.6362],\n",
              "           [0.6229, 0.8128, 0.6808,  ..., 0.8241, 0.7474, 0.5344],\n",
              "           [0.5732, 0.8781, 0.8293,  ..., 0.5295, 0.3167, 0.6894]]],\n",
              " \n",
              " \n",
              "         ...,\n",
              " \n",
              " \n",
              "         [[[0.5139, 0.9585, 0.7048,  ..., 0.4706, 1.1845, 0.8306],\n",
              "           [0.6485, 0.6921, 0.7455,  ..., 0.8412, 0.9490, 0.7054],\n",
              "           [0.8574, 0.5853, 0.8126,  ..., 0.8271, 0.9612, 0.7024],\n",
              "           ...,\n",
              "           [0.8571, 0.9179, 1.1562,  ..., 1.0475, 0.6699, 0.6514],\n",
              "           [0.5395, 0.8354, 0.7111,  ..., 0.8484, 0.9584, 0.6190],\n",
              "           [0.8390, 0.5624, 0.5619,  ..., 0.7656, 0.6843, 0.4733]],\n",
              " \n",
              "          [[1.1947, 0.8580, 0.8976,  ..., 0.7886, 0.7540, 0.7705],\n",
              "           [0.8955, 0.8698, 0.7398,  ..., 0.6024, 0.6055, 0.7405],\n",
              "           [0.5969, 0.4527, 0.7469,  ..., 0.5325, 0.7581, 0.8009],\n",
              "           ...,\n",
              "           [0.6167, 0.6102, 0.5931,  ..., 0.9055, 1.1287, 0.8764],\n",
              "           [0.8777, 0.9808, 0.7232,  ..., 0.9431, 0.2721, 0.6570],\n",
              "           [0.5581, 0.4597, 0.7226,  ..., 0.7046, 0.5034, 0.6325]],\n",
              " \n",
              "          [[0.6878, 0.8854, 0.5886,  ..., 0.6195, 0.8855, 0.6604],\n",
              "           [0.7996, 0.5236, 0.8830,  ..., 0.6311, 0.6650, 0.7740],\n",
              "           [0.5857, 0.6802, 0.6618,  ..., 0.4279, 0.5291, 0.7272],\n",
              "           ...,\n",
              "           [0.6958, 0.6722, 0.4040,  ..., 0.7566, 0.5490, 0.2878],\n",
              "           [0.6755, 0.7320, 0.6415,  ..., 0.5381, 0.8886, 1.0171],\n",
              "           [0.7361, 0.7721, 0.8607,  ..., 0.8243, 0.6769, 0.7289]],\n",
              " \n",
              "          [[0.5134, 1.0933, 0.5616,  ..., 0.5353, 0.6832, 0.6531],\n",
              "           [0.7765, 0.6428, 0.7305,  ..., 0.3585, 0.7072, 0.5546],\n",
              "           [0.8771, 0.5484, 0.7344,  ..., 0.6258, 0.7448, 0.3164],\n",
              "           ...,\n",
              "           [0.6185, 0.7809, 1.2525,  ..., 0.9899, 0.4716, 0.6864],\n",
              "           [0.8044, 0.7140, 0.3929,  ..., 0.5716, 0.2625, 0.5923],\n",
              "           [0.7273, 0.9700, 0.7583,  ..., 0.4546, 0.6063, 0.9399]]],\n",
              " \n",
              " \n",
              "         [[[0.4228, 0.8486, 0.9001,  ..., 0.6750, 0.6145, 0.3459],\n",
              "           [0.9708, 0.7545, 0.9327,  ..., 0.6346, 0.7672, 0.7432],\n",
              "           [0.7026, 0.8030, 0.7853,  ..., 0.5845, 0.6818, 0.5152],\n",
              "           ...,\n",
              "           [0.8206, 0.4788, 1.2461,  ..., 0.7114, 0.4759, 0.6254],\n",
              "           [0.6418, 0.6923, 0.8943,  ..., 0.7203, 0.6995, 0.7107],\n",
              "           [0.6397, 0.5745, 1.0160,  ..., 0.8132, 0.7279, 0.5532]],\n",
              " \n",
              "          [[0.7658, 0.6374, 0.7984,  ..., 0.7095, 0.7758, 0.7840],\n",
              "           [0.7832, 0.4628, 0.3681,  ..., 0.7452, 0.7778, 0.7669],\n",
              "           [0.7413, 0.6951, 0.5761,  ..., 1.0948, 0.4968, 0.4787],\n",
              "           ...,\n",
              "           [0.9395, 1.0545, 0.6344,  ..., 0.7220, 0.4989, 0.7187],\n",
              "           [0.7035, 0.8188, 0.5433,  ..., 0.7804, 0.5496, 0.5223],\n",
              "           [0.8402, 0.7453, 0.3474,  ..., 0.6197, 0.7948, 0.6134]],\n",
              " \n",
              "          [[0.3603, 1.1036, 0.5891,  ..., 0.8252, 0.4258, 0.3955],\n",
              "           [0.7111, 0.9473, 0.4409,  ..., 0.8261, 0.5765, 0.4974],\n",
              "           [0.6300, 0.8217, 1.0144,  ..., 0.9546, 1.1750, 1.1056],\n",
              "           ...,\n",
              "           [0.8961, 0.4505, 0.7297,  ..., 0.8524, 0.4372, 1.0410],\n",
              "           [0.7747, 0.5040, 0.6784,  ..., 0.5566, 1.1498, 0.7962],\n",
              "           [0.6596, 1.1634, 0.4957,  ..., 0.7880, 0.5380, 0.6310]],\n",
              " \n",
              "          [[0.5279, 0.7320, 0.9941,  ..., 0.8691, 1.0518, 0.7016],\n",
              "           [0.7898, 0.6883, 0.5457,  ..., 0.3695, 0.5490, 0.7430],\n",
              "           [0.5119, 0.8078, 0.5855,  ..., 0.5621, 0.9829, 0.7496],\n",
              "           ...,\n",
              "           [0.5684, 1.1142, 0.7967,  ..., 0.5090, 0.4609, 0.7501],\n",
              "           [0.4333, 0.6514, 0.8165,  ..., 0.9520, 0.8970, 1.1366],\n",
              "           [0.4308, 0.3284, 0.8361,  ..., 0.8206, 1.0035, 0.4980]]],\n",
              " \n",
              " \n",
              "         [[[0.7467, 0.6446, 0.5154,  ..., 0.5798, 0.6406, 1.1936],\n",
              "           [0.4474, 0.9590, 0.5592,  ..., 0.7921, 0.7791, 0.6336],\n",
              "           [0.9450, 0.5910, 0.7239,  ..., 0.7865, 0.7471, 0.5572],\n",
              "           ...,\n",
              "           [0.5536, 0.6727, 1.0070,  ..., 0.6674, 1.2189, 0.7011],\n",
              "           [0.4842, 0.5871, 0.6954,  ..., 0.7043, 0.9860, 0.7895],\n",
              "           [0.7020, 0.7433, 0.8664,  ..., 0.5529, 0.7889, 0.8005]],\n",
              " \n",
              "          [[0.7196, 0.3207, 0.9218,  ..., 0.6925, 0.9509, 0.4740],\n",
              "           [0.9858, 1.0358, 0.5459,  ..., 0.9464, 0.4016, 0.5775],\n",
              "           [0.7281, 0.9938, 0.8871,  ..., 0.6001, 0.8402, 1.0374],\n",
              "           ...,\n",
              "           [0.8533, 1.0131, 0.5425,  ..., 0.8832, 0.4754, 0.7555],\n",
              "           [0.7653, 1.0432, 0.6744,  ..., 0.5842, 0.7185, 0.8080],\n",
              "           [0.4853, 0.7610, 0.5558,  ..., 0.8416, 0.6044, 0.9784]],\n",
              " \n",
              "          [[0.6224, 1.0267, 0.9031,  ..., 0.6286, 0.7636, 0.6316],\n",
              "           [0.9949, 0.5400, 0.3923,  ..., 0.7494, 0.4017, 0.5003],\n",
              "           [1.2642, 0.4963, 0.8992,  ..., 0.8460, 1.0868, 1.3053],\n",
              "           ...,\n",
              "           [0.6211, 0.5373, 0.8102,  ..., 0.8368, 0.7892, 1.0315],\n",
              "           [0.5378, 0.6785, 0.8107,  ..., 0.6532, 0.7171, 0.5774],\n",
              "           [0.8034, 0.6444, 0.9854,  ..., 0.5444, 0.5273, 0.7480]],\n",
              " \n",
              "          [[0.6030, 0.6670, 0.5706,  ..., 0.5578, 0.6974, 0.9863],\n",
              "           [0.8439, 0.4847, 0.7106,  ..., 0.5610, 0.7778, 0.4506],\n",
              "           [0.7859, 0.3478, 1.0372,  ..., 0.7525, 1.0591, 0.2940],\n",
              "           ...,\n",
              "           [0.6795, 0.5807, 0.8654,  ..., 0.8432, 0.8197, 0.6898],\n",
              "           [0.9770, 0.5094, 0.6678,  ..., 0.7466, 0.6382, 0.5085],\n",
              "           [0.5039, 0.6764, 0.6206,  ..., 0.7895, 1.3537, 0.8089]]]],\n",
              "        device='cuda:0')}"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wE_DENScSGHL",
        "outputId": "f1b8fb1d-a8a5-45a4-eb9b-07bdd2c8f3c7"
      },
      "source": [
        "model, criterion, _, _, _ = training_setup()\n",
        "tmp = next(iter(train_dataset))\n",
        "outputs, aux_losses = model(process_inputs(tmp['inputs']).cuda())\n",
        "\n",
        "aux_losses = sum(aux_losses)\n",
        "(criterion(outputs, process_inputs(tmp['targets']).cuda()) + aux_losses).backward()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original model 3464194 params, new model 3663874 params, ratio 1.06\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O_XSqOYYTsDT",
        "outputId": "c9a66ce8-a2e3-4f96-f423-7240192ad3e3"
      },
      "source": [
        "list(model.blocks[-1].attention.lka.modules())[0][0].output_bias"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([[[[-2.6187e-03, -5.7948e-04, -3.2907e-04,  6.2941e-04, -7.1119e-04,\n",
              "           -4.3961e-04,  2.6415e-03,  9.3061e-04,  2.9606e-03, -2.6636e-04,\n",
              "           -1.7387e-03,  1.6021e-03,  9.3624e-04,  1.1273e-03, -3.3982e-04,\n",
              "           -4.0739e-04, -2.0149e-03,  9.0991e-04, -1.0498e-03, -2.7959e-03,\n",
              "            2.5112e-04, -9.3633e-04,  6.2485e-04, -1.3612e-03, -2.9829e-04,\n",
              "           -2.0080e-04,  7.8189e-04,  7.8589e-04, -5.3443e-03, -3.7092e-03,\n",
              "           -2.3028e-04,  3.1588e-03, -2.2930e-04,  3.9736e-03,  4.0218e-04,\n",
              "            1.0651e-03,  7.7839e-03,  1.8654e-04, -3.1373e-03,  1.4372e-03,\n",
              "            4.1014e-03, -1.0334e-03, -8.9334e-04, -2.3530e-04, -1.0687e-03,\n",
              "           -2.1504e-03, -1.1092e-03,  1.7172e-05,  1.2050e-03, -1.5283e-03,\n",
              "            1.4103e-04, -2.1979e-03, -5.9292e-05,  2.5531e-03, -4.6291e-03,\n",
              "            8.9993e-04,  5.3557e-05, -1.8944e-03,  6.1842e-04, -3.0434e-03,\n",
              "           -2.6042e-03, -2.7910e-03,  1.9914e-03,  3.3585e-03]],\n",
              "\n",
              "         [[ 2.7830e-03,  6.5157e-04,  1.3587e-03,  2.6377e-03,  2.6050e-03,\n",
              "            1.3299e-04, -1.4192e-03,  4.3132e-04,  1.1622e-03,  1.8455e-03,\n",
              "           -6.5035e-03,  3.9105e-03,  3.6151e-03, -3.1340e-04, -1.1248e-03,\n",
              "           -9.3471e-03, -1.1168e-03, -1.2783e-03,  1.4718e-03, -1.7144e-03,\n",
              "           -1.5742e-03, -2.5472e-03, -3.9025e-04, -7.1958e-03,  2.0620e-03,\n",
              "            1.1221e-03,  4.0981e-03, -3.0099e-03, -2.8092e-03,  1.3933e-03,\n",
              "            1.2233e-03, -2.4611e-03, -7.4985e-03,  2.3598e-03,  1.5210e-04,\n",
              "            2.4839e-03, -1.8429e-03, -2.0700e-05,  2.3278e-03,  1.8919e-03,\n",
              "           -1.5871e-03, -5.1121e-03, -2.0640e-03, -5.9370e-03, -4.9204e-03,\n",
              "           -9.9108e-03, -1.5696e-03, -8.8422e-04, -2.8757e-03, -1.3572e-03,\n",
              "            3.9639e-04,  1.5128e-03, -8.5317e-04, -2.9370e-03, -1.1195e-03,\n",
              "            2.7233e-03, -4.3108e-03, -3.5303e-03,  2.7051e-03,  2.5042e-03,\n",
              "            7.5812e-04,  6.9706e-03, -1.0707e-03,  1.2404e-03]],\n",
              "\n",
              "         [[-3.8659e-03,  1.0261e-02,  2.7044e-03,  1.9696e-03, -1.4937e-04,\n",
              "           -5.3674e-03,  2.8987e-04,  5.6101e-04,  2.4013e-03,  1.1758e-03,\n",
              "           -3.7829e-03, -3.6720e-03,  5.8277e-04, -1.9636e-03,  3.7169e-03,\n",
              "            2.5876e-04, -5.9365e-03, -1.8047e-03,  3.1761e-03, -3.4872e-03,\n",
              "            2.8748e-03, -2.8431e-03,  6.1902e-03,  5.2007e-03,  6.6551e-03,\n",
              "            4.4296e-04,  3.4092e-04,  1.0645e-03,  8.4146e-03,  3.3517e-03,\n",
              "            4.8221e-03,  3.3170e-03, -3.9772e-05, -5.3822e-03, -1.1954e-03,\n",
              "           -2.6741e-05, -3.7010e-03,  4.8294e-03, -1.1520e-04, -1.9767e-04,\n",
              "           -6.4628e-03, -5.5500e-04,  3.8438e-03, -1.9857e-03,  3.4086e-03,\n",
              "            1.0389e-04,  3.8679e-03, -5.5254e-03, -1.2043e-03,  2.0952e-03,\n",
              "           -2.3705e-03,  5.6132e-03,  3.6401e-03, -5.2342e-03, -3.7908e-04,\n",
              "            1.2614e-03, -2.6454e-03, -4.1037e-03,  4.6147e-04,  3.8222e-03,\n",
              "            3.9316e-03,  1.1161e-03, -1.8010e-03, -9.9954e-03]],\n",
              "\n",
              "         [[ 8.1668e-04,  1.3635e-03,  4.4353e-04, -2.8064e-03,  2.3828e-04,\n",
              "            1.7759e-03,  2.0466e-04, -1.8902e-03,  2.3943e-04, -1.3520e-03,\n",
              "            2.3867e-03, -1.2658e-03, -1.5047e-03,  1.0777e-03, -4.2223e-03,\n",
              "            1.9186e-03, -2.0456e-03,  7.1799e-04,  5.4871e-04,  1.6659e-04,\n",
              "           -2.3044e-03, -3.2187e-03,  1.3099e-03, -1.5698e-03, -1.4281e-03,\n",
              "            2.3084e-03, -3.8172e-04, -1.0892e-03,  1.9725e-04, -1.0847e-03,\n",
              "            7.4281e-05, -9.6833e-04, -2.6583e-03, -1.4236e-03, -4.5038e-03,\n",
              "           -4.5519e-06, -1.1274e-03,  4.4086e-04,  8.0243e-05,  2.8444e-03,\n",
              "            1.6535e-03,  5.1128e-04,  1.4300e-03,  4.1248e-03, -3.1418e-03,\n",
              "           -4.7238e-03,  1.9891e-03,  2.6389e-03, -1.0230e-03, -1.4871e-03,\n",
              "            3.6018e-03,  1.1650e-03,  3.4906e-04,  1.1271e-04,  9.0374e-04,\n",
              "           -1.7273e-03,  3.5433e-05, -3.7748e-05, -1.9616e-03, -1.0358e-03,\n",
              "           -2.2366e-03, -3.8164e-04, -2.1304e-04,  7.8375e-04]]]],\n",
              "       device='cuda:0', requires_grad=True)"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJI-xf2xaLqc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}